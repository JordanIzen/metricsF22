---
format:
  revealjs:
    theme: [default, custom.scss]
    logo: "../images/metrics_hex.png"
    height: 900
    width: 1600
    df-print: paged
    slide-number: c
    chalkboard: true
overview: true
execute:
  echo: false
  warning: false
  freeze: auto
---

##  {data-menu-title="Title Slide" background-image="images/metrics_title_slide.png"}

[3.3 --- Omitted Variable Bias]{.custom-title}

[ECON 4470 • Econometrics]{.custom-subtitle}

[Jordan Izenwasser <br> Slides Adapated from Ryan Safner, PhD]{.custom-author}


```{r}
#| label: setup
#| include: false
library(tidyverse)
library(broom)
library(haven)
library(kableExtra)
library(patchwork)
library(fontawesome)
library(gapminder)
library(ggthemes)
library(scales)
library(infer)
library(ggdag)
library(dagitty)
knitr::opts_chunk$set(echo=F,
                      message=F,
                      warning=F)
update_geom_defaults("label", list(family = "Fira Sans Condensed"))
update_geom_defaults("text", list(family = "Fira Sans Condensed"))

ca_school <- read_dta("../files/data/caschool.dta")
scatter <- ggplot(data = ca_school)+
  aes(x = str,
      y = testscr)+
  geom_point(color = "blue")+
  labs(x = "Student to Teacher Ratio",
       y = "Test Score")+
  theme_pander(base_family = "Fira Sans Condensed",
           base_size = 20)
school_reg <- lm(testscr ~ str, data = ca_school)
aug_reg <- school_reg %>% augment()
school_reg_tidy <- tidy(school_reg)

```

## Contents {background-color="#314f4f"}

[Omitted Variables and Bias](#causation-and-correlation)

[The Multivariate Regression Model](#the-multivariate-regression-model)

[Multivariate Regression in R](#multivariate-regression-in-r)

# Omitted Variables and Bias {background-color="#314f4f" .centered}

## The Error Term

$$Y_i=\beta_0+\beta_1X_i+u_i$$

- $u_i$ includes [all other variables that affect $Y$]{.hi-purple}

- Every regression model always has [omitted variables]{.hi} assumed in the error
  - Most are unobservable (hence “u”)
  - [**Examples**:]{.hi-green} innate ability, weather at the time, etc

- Again, we *assume* $u$ is random, with $E[u|X]=0$ and $var(u)=\sigma^2_u$

- *Sometimes*, omission of variables can **bias** OLS estimators $(\hat{\beta_0}$ and $\hat{\beta_1})$

## Omitted Variable Bias I


::: columns
::: {.column width="50%"}
- [Omitted variable bias (OVB)]{.hi} for some omitted variable $\mathbf{Z}$ exists if two conditions are met:

:::
::: {.column width="50%"}
:::
:::

## Omitted Variable Bias I

::: columns
::: {.column width="50%"}
- [Omitted variable bias (OVB)]{.hi} for some omitted variable $\mathbf{Z}$ exists if two conditions are met:

**1. $\mathbf{Z}$ is a determinant of $Y$**

- i.e. $Z$ is in the error term, $u_i$

:::
::: {.column width="50%"}
```{r}
obv_coords <- list(x = c(X = 1, Z = 2, Y = 3),
                        y = c(X = 1, Z = 2, Y = 1))

dagify(Y~X+Z,
       exposure = "X",
       outcome = "Y",
       coords = obv_coords) %>% 
  tidy_dagitty(seed = 2) %>%
  ggdag_status(stylized = FALSE)+theme_dag_blank()+theme(legend.position = "none")
```
:::
:::

## Omitted Variable Bias I

::: columns
::: {.column width="50%"}
- [Omitted variable bias (OVB)]{.hi} for some omitted variable $\mathbf{Z}$ exists if two conditions are met:

**1. $\mathbf{Z}$ is a determinant of $Y$**

- i.e. $Z$ is in the error term, $u_i$

**2. $\mathbf{Z}$ is correlated with the regressor $X$**

- i.e. $cor(X,Z) \neq 0$
- implies $cor(X,u) \neq 0$
- implies [X is endogenous]{.hi-purple}

:::
::: {.column width="50%"}
```{r}
obv_coords <- list(x = c(X = 1, Z = 2, Y = 3),
                        y = c(X = 1, Z = 2, Y = 1))

dagify(Y~X+Z,
       X~Z,
       exposure = "X",
       outcome = "Y",
       coords = obv_coords) %>% 
  tidy_dagitty(seed = 2) %>%
  ggdag_status(stylized = FALSE)+theme_dag_blank()+theme(legend.position = "none")
```
:::
:::

## Omitted Variable Bias II

::: columns
::: {.column width="50%"}
- Omitted variable bias makes $X$ [endogenous]{.hi-purple}

- Violates **zero conditional mean assumption**
$$\mathbb{E}(u_i|X_i)\neq 0 \implies$$ 
  - knowing $X_i$ tells you something about $u_i$ (i.e. something about $Y$ *not* by way of $X$)!

:::
::: {.column width="50%"}
```{r}
obv_coords <- list(x = c(X = 1, Z = 2, Y = 3),
                        y = c(X = 1, Z = 2, Y = 1))

dagify(Y~X+Z,
       X~Z,
       exposure = "X",
       outcome = "Y",
       coords = obv_coords) %>% 
  tidy_dagitty(seed = 2) %>%
  ggdag_status(stylized = FALSE)+theme_dag_blank()+theme(legend.position = "none")
```
:::
:::

## Omitted Variable Bias III

::: columns
::: {.column width="50%"}
- $\hat{\beta_1}$ is [biased]{.hi-purple}: $\mathbb{E}[\hat{\beta_1}] \neq \beta_1$

- $\hat{\beta_1}$ systematically over- or under-estimates the true relationship $(\beta_1)$

- $\hat{\beta_1}$ “picks up” *both* pathways:
  1. $X\rightarrow Y$ 
  2. $X \leftarrow Z\rightarrow Y$ 

:::
::: {.column width="50%"}
```{r}
obv_coords <- list(x = c(X = 1, Z = 2, Y = 3),
                        y = c(X = 1, Z = 2, Y = 1))

dagify(Y~X+Z,
       X~Z,
       exposure = "X",
       outcome = "Y",
       coords = obv_coords) %>% 
  tidy_dagitty(seed = 2) %>%
  ggdag_status(stylized = FALSE)+theme_dag_blank()+theme(legend.position = "none")
```
:::
:::

## Omited Variable Bias: Class Size Example

::: {.callout-tip appearance="simple" icon=false}
## Example

Consider our recurring class size and test score example:
$$\text{Test score}_i = \beta_0 + \beta_1 \text{STR}_i + u_i$$
:::

- Which of the following possible variables would cause a bias if omitted?

. . .

1. $Z_i$: time of day of the test

. . .

2. $Z_i$: parking space per student

. . .

3. $Z_i$: percent of ESL students

## Recall: Endogeneity and Bias

- The true expected value of $\hat{\beta_1}$ is actually: 

$$E[\hat{\beta_1}]=\beta_1+cor(X,u)\frac{\sigma_u}{\sigma_X}$$

. . .

1. If $X$ is exogenous: $cor(X,u)=0$, we're just left with $\beta_1$

. . .

2. The larger $cor(X,u)$ is, larger [bias]{.hi-purple}: $\left(E[\hat{\beta_1}]-\beta_1 \right)$

. . .

3. We can [“sign”]{.hi-turquoise} the direction of the bias based on $cor(X,u)$
  - [Positive]{.hi-turquoise} $cor(X,u)$ overestimates the true $\beta_1$ $(\hat{\beta_1}$ is too high)
  - [Negative]{.hi-turquoise} $cor(X,u)$ underestimates the true $\beta_1$ $(\hat{\beta_1}$ is too low)

## Endogeneity and Bias: Correlations I

- Here is where checking correlations between variables can help us:

```{r}
#| echo: true
#| output-location: fragment
ca_school %>%
  # Select only the three variables we want (there are many)
  select(str, testscr, el_pct) %>%
  # make a correlation table (all variables must be numeric)
  cor()
```

. . .

- `el_pct` is strongly (negatively) correlated with `testscr` (Condition 1)

- `el_pct` is reasonably (positively) correlated with `str` (Condition 2) 	

## Look at Conditional Distributions I

```{r}
#| echo: true
#| output-location: fragment
# make a new variable called EL
# = high (if el_pct is above median) or = low (if below median)
ca_school <- ca_school %>% # next we create a new dummy variable called ESL
  mutate(ESL = ifelse(el_pct > median(el_pct), # test if ESL is above median
                     yes = "High ESL", # if yes, call this variable "High ESL"
                     no = "Low ESL")) # if no, call this variable "Low ESL"

# get average test score by high/low EL
ca_school %>%
  group_by(ESL) %>%
  summarize(Average_test_score = mean(testscr))
```

## Look at Conditional Distributions II

::: {.panel-tabset}

## Plot

```{r}
#| fig-align: "center"
#| fig-width: 14
ggplot(data = ca_school)+
  aes(x = testscr,
      fill = ESL)+
  geom_density(alpha = 0.5)+
  labs(x = "Test Score",
       y = "Density")+
  theme_bw(
    base_family = "Fira Sans Condensed",
    base_size=20
    )+
  theme(legend.position = "bottom")

```

## Code

```{r}
#| echo: true
#| eval: false
ggplot(data = ca_school)+
  aes(x = testscr,
      fill = ESL)+
  geom_density(alpha = 0.5)+
  labs(x = "Test Score",
       y = "Density")+
  theme_bw(
    base_family = "Fira Sans Condensed",
    base_size=20
    )+
  theme(legend.position = "bottom")

```

:::

## Look at Conditional Distributions III

::: {.panel-tabset}

## Plot

```{r}
#| fig-align: "center"
#| fig-width: 14
esl_scatter <- ggplot(data = ca_school)+
  aes(x = str,
      y = testscr,
      color = ESL)+
  geom_point()+
  geom_smooth(method = "lm")+
  labs(x = "STR",
       y = "Test Score")+
  theme_bw(
    base_family = "Fira Sans Condensed",
    base_size=20
    )+
  theme(legend.position = "bottom")

esl_scatter
```

## Code

```{r}
#| echo: true
#| eval: false
esl_scatter <- ggplot(data = ca_school)+
  aes(x = str,
      y = testscr,
      color = ESL)+
  geom_point()+
  geom_smooth(method = "lm")+
  labs(x = "STR",
       y = "Test Score")+
  theme_bw(
    base_family = "Fira Sans Condensed",
    base_size=20
    )+
  theme(legend.position = "bottom")

esl_scatter

```

:::

## Look at Conditional Distributions IV

::: {.panel-tabset}

## Plot

```{r}
#| fig-align: "center"
#| fig-width: 14
esl_scatter+
  facet_grid(~ESL)+
  guides(color = F)
```

## Code

```{r}
#| echo: true
#| eval: false
esl_scatter+
  facet_grid(~ESL)+
  guides(color = F)

```

:::

## Omitted Variable Bias in the Class Size Example

$$\mathbb{E}[\hat{\beta_1}]=\beta_1+bias$$

$\mathbb{E}[\hat{\beta_1}]=$ [$\beta_1$]{.red} $+$ [$cor(X,u)$]{.blue} $\frac{\sigma_u}{\sigma_X}$

. . .

- [$cor(STR,u)$]{.blue} is positive (via $\%EL$)
- $cor(u, \text{Test score})$ is negative (via $\%EL$)
- [$\beta_1$]{.red} is negative (between test score and str)

- [Bias]{.blue} from $\%EL$ is positive
  - Since $\color{red}{\beta_1}$ is negative, it’s made to be a *larger* negative number than it truly is
  - Implies that our $\color{red}{\hat{\beta}_1}$ __*over*states__ the effect of reducing STR on improving Test Scores

## Omitted Variable Bias: Messing with Causality I

- If school districts with higher Test Scores happen to have both lower STR **AND** districts with smaller $STR$ sizes tend to have less $\%EL$ ...

. . .

- How can we say $\hat{\beta_1}$ estimates the [marginal effect]{.hi} of $\Delta STR \rightarrow \Delta \text{Test Score}$?

. . .

- (We can’t.)

## Omitted Variable Bias: Messing with Causality II

::: columns
::: {.column width="50%"}
- Consider an ideal [random controlled trial (RCT)]{.hi-turquoise}

- [Randomly]{.hi-turquoise} assign experimental units (e.g. people, cities, etc) into two (or more) groups:
  - [Treatment group(s)]{.hi}: gets a (certain type or level of) treatment
  - [Control group(s)]{.hi-purple}: gets *no* treatment(s)

- Compare results of two groups to get [average treatment effect]{.hi-slate}

:::
::: {.column width="50%"}
![](images/groupsplit.jpeg){fig-align="center" width=600}
:::
:::

## RCTs Neutralize Omitted Variable Bias I


::: columns
::: {.column width="50%"}

::: {.callout-tip appearance="simple" icon=false}
## Example
Imagine an ideal RCT for measuring the effect of STR on Test Score
:::

- School districts would be [randomly assigned]{.hi-turquoise} a student-teacher ratio 

- With random assignment, all factors in $u$ (%ESL students, family size, parental income, years in the district, day of the week of the test, climate, etc) are distributed *independently* of class size

:::
::: {.column width="50%"}
![](images/classroomdoors.jpg)
:::
:::

## RCTs Neutralize Omitted Variable Bias II


::: columns
::: {.column width="50%"}

::: {.callout-tip appearance="simple" icon=false}
## Example
Imagine an ideal RCT for measuring the effect of STR on Test Score
:::

- Thus, $cor(STR, u)=0$ and $E[u|STR]=0$, i.e. [exogeneity]{.hi-purple}

- Our $\hat{\beta_1}$ would be an [unbiased estimate]{.hi-turquoise} of $\beta_1$, measuring the [true causal effect]{.hi-slate} of STR $\rightarrow$ Test Score 

:::
::: {.column width="50%"}
![](images/classroomdoors.jpg)
:::
:::

## But We Rarely, if Ever, Can Do RCTs

::: columns
::: {.column width="50%"}

- But we **didn't** run an RCT, we have observational data!

- “Treatment” of having a large or small class size is **NOT** randomly assigned!

- $\%EL$: plausibly fits criteria of O.V. bias!
    1. $\%EL$ is a determinant of Test Score
    2. $\%EL$ is correlated with STR

- Thus, “control” group and “treatment” group differ systematically!
  - Small STR also tend to have lower $\%EL$; large STR also tend to have higher $\%EL$
  - [Selection bias]{.hi-orange}: $cor(STR, \%EL) \neq 0$, $E[u_i|STR_i]\neq 0$

:::
::: {.column width="25%"}
![Treatment Group](images/3apples.jpg){height=400}

:::
::: {.column width="25%"}
![Control Group](images/3oranges.jpg){height=400}

:::
:::

## Another Way to Control for Variables I

::: columns
::: {.column width="50%"}

- Pathways connecting str and test score:
  1. str $\rightarrow$ test score
  2. str $\leftarrow$ ESL $\rightarrow$ testscore

:::
::: {.column width="50%"}
```{r}
dagify(test~str+esl,
       str~esl,
       exposure = "str",
       outcome = "test",
       coords = obv_coords) %>% 
  tidy_dagitty(seed = 2) %>%
  ggdag_status(stylized = FALSE)+theme_dag_blank()+theme(legend.position = "none")

```
:::
:::


## Another Way to Control for Variables II

::: columns
::: {.column width="50%"}

- Pathways connecting str and test score:
  1. str $\rightarrow$ test score
  2. str $\leftarrow$ ESL $\rightarrow$ testscore

- DAG rules tell us we need to [control for ESL]{.hi-purple} in order to identify the causal effect of str $\rightarrow$ test score

- So now, [how *do* we control for a variable]{.hi-turquoise}?

:::
::: {.column width="50%"}
```{r}
dagify(test~str+esl,
       str~esl,
       exposure = "str",
       outcome = "test",
       coords = obv_coords) %>% 
  tidy_dagitty(seed = 2) %>%
  ggdag_adjustment_set(stylized = FALSE)+theme_dag_blank()+theme(legend.position = "none")
```
:::
:::

## Controlling for Variables

::: columns
::: {.column width="50%"}
- Look at effect of STR on Test Score by comparing districts with the **same** %EL  	
  -  Eliminates differences in %EL between high and low STR classes
  - “As if” we had a control group! Hold %EL constant 

- The simple fix is just to [not omit %EL]{.hi-purple}!
  - Make it *another* independent variable on the righthand side of the regression

:::
::: {.column width="25%"}
![](images/3apples.jpg){width="400"}
Treatment Group
:::
::: {.column width="25%"}
![](images/3apples.jpg){width="400"}
Control Group
:::
:::

## Controlling for Variables

::: columns
::: {.column width="50%"}
- Look at effect of STR on Test Score by comparing districts with the **same** %EL  	
  -  Eliminates differences in %EL between high and low STR classes
  - “As if” we had a control group! Hold %EL constant 

- The simple fix is just to [not omit %EL]{.hi-purple}!
  - Make it *another* independent variable on the righthand side of the regression

:::
::: {.column width="50%"}
```{r}
library(gganimate)
school<-ca_school %>%
  select("str","testscr","el_pct") %>%
  mutate(ESL = ifelse(el_pct > median(el_pct),
                     "High ESL",
                     "Low ESL")) %>%
  group_by(ESL) %>%
  mutate(mean_str = mean(str),
         mean_testscr = mean(testscr))

before_cor <- paste("1. Raw data: cor(str, testscr) = ",round(cor(school$str,school$testscr),3),sep='')
after_cor <- paste("6. What's left! cor(str, testscr), controlling for ESL = ",round(cor(school$str-school$mean_str,school$testscr-school$mean_testscr),3),sep='')

#Add step 2 in which X is demeaned, and 3 in which both X and Y are, and 4 which just changes label
dffull <- rbind(
  #Step 1: Raw data only
  school %>% mutate(mean_str=NA,mean_testscr=NA,time=before_cor),
  #Step 2: Add x-lines
  school %>% mutate(mean_testscr=NA,time='2. Figure out differences in STR explained by ESL'),
  #Step 3: X de-meaned 
  school %>% mutate(str = str - mean_str,mean_str=0,mean_testscr=NA,time="3. Remove all differences in STR explained by ESL"),
  #Step 4: Remove X lines, add Y
  school %>% mutate(str = str - mean_str,mean_str=NA,time="4. Figure out any differences in Test Scores explained by ESL"),
  #Step 5: Y de-meaned
  school %>% mutate(str = str - mean_str,testscr = testscr - mean_testscr,mean_str=NA,mean_testscr=0,time="5. Remove all differences in Test Scores explained by ESL"),
  #Step 6: Raw demeaned data only
  school %>% mutate(str = str - mean_str,testscr = testscr - mean_testscr,mean_str=NA,mean_testscr=NA,time=after_cor))
```

```{r, fig.retina=3}
p <- ggplot(dffull,aes(y=testscr,x=str,color=as.factor(ESL)))+geom_point()+
  geom_vline(aes(xintercept=mean_str,color=as.factor(ESL)))+
  geom_hline(aes(yintercept=mean_testscr,color=as.factor(ESL)))+
  labs(x = "Student to Teacher Ratio",
       y = "Average District Test Score",
       title = 'Relationship between STR and Test Scores, Controlling for ESL \n{next_state}',
       caption = "Animation inspired by Nick Huntington-Klein’s Causal Animations")+
  theme_bw(base_family = "Fira Sans Condensed", base_size = 14)+
  theme(legend.position = "none")+
  transition_states(time,transition_length=c(12,32,12,32,12,12),state_length=c(160,100,75,100,75,160),wrap=FALSE)+
  ease_aes('sine-in-out')+
  exit_fade()+enter_fade()

animate(p,nframes=200)
```

:::
:::

# The Multivariate Regression Model {background-color="#314f4f" .centered}

## Multivariate Econometric Models Overview

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_k X_k + u$$

. . .

- $Y$ is the [dependent variable]{.hi} of interest
    - AKA “response variable,” “regressand,” “Left-hand side (LHS) variable”

. . .

- $X_1, X_2, \cdots, X_k$ are [independent variables]{.hi}
    - AKA “explanatory variables”, “regressors,” “Right-hand side (RHS) variables”, “covariates”

. . . 

- Our data consists of a spreadsheet of observed values of $(Y_i, X_{1i}, X_{2i}, \cdots, X_{ki})$

## Multivariate Econometric Models: Overview II

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_k X_k + u$$

- To model, we [“regress $Y$ on $X_1, X_2, \cdots, X_k$”]{.hi-turquoise}

. . .

- $\beta_0, \beta_1, \beta_2, \cdots , \beta_k$ are [parameters]{.hi-purple} that describe the population relationships between the variables
    - unknown! to be estimated
    - we estimate $k+1$ parameters (“betas”) on $k$ variables^[Note Bailey defines $k$ to include both the number of variables plus the constant.]

. . .

- $u$ is a random [error term]{.hi}
    - **‘U’nobservable**, we can't measure it, and must model with assumptions about it

## Marginal Effects I

$$Y_i= \beta_0+\beta_1 X_{1i} + \beta_2 X_{2i}+u_i$$
. . .

- Consider changing $X_1$ by $\Delta X_1$ while holding $X_2$ constant:

. . .

\begin{align*}
Y&= \beta_0+\beta_1 X_{1} + \beta_2 X_{2} && \text{Before the change}\\
\end{align*}

## Marginal Effects I

$$Y_i= \beta_0+\beta_1 X_{1i} + \beta_2 X_{2i}+u_i$$

- Consider changing $X_1$ by $\Delta X_1$ while holding $X_2$ constant:

\begin{align*}
Y&= \beta_0+\beta_1 X_{1} + \beta_2 X_{2} && \text{Before the change}\\
Y+\Delta Y&= \beta_0+\beta_1 (X_{1}+\Delta X_1) + \beta_2 X_{2} && \text{After the change}\\
\end{align*}

## Marginal Effects I

$$Y_i= \beta_0+\beta_1 X_{1i} + \beta_2 X_{2i}+u_i$$

- Consider changing $X_1$ by $\Delta X_1$ while holding $X_2$ constant:

\begin{align*}
Y&= \beta_0+\beta_1 X_{1} + \beta_2 X_{2} && \text{Before the change}\\
Y+\Delta Y&= \beta_0+\beta_1 (X_{1}+\Delta X_1) + \beta_2 X_{2} && \text{After the change}\\
\Delta Y&= \beta_1 \Delta X_1 && \text{The difference}\\
\end{align*}

## Marginal Effects I

$$Y_i= \beta_0+\beta_1 X_{1i} + \beta_2 X_{2i}+u_i$$

- Consider changing $X_1$ by $\Delta X_1$ while holding $X_2$ constant:

\begin{align*}
Y&= \beta_0+\beta_1 X_{1} + \beta_2 X_{2} && \text{Before the change}\\
Y+\Delta Y&= \beta_0+\beta_1 (X_{1}+\Delta X_1) + \beta_2 X_{2} && \text{After the change}\\
\Delta Y&= \beta_1 \Delta X_1 && \text{The difference}\\
\frac{\Delta Y}{\Delta X_1} &= \beta_1  && \text{Solving for } \beta_1\\
\end{align*}

## Marginal Effects II

$$\beta_1 =\frac{\Delta Y}{\Delta X_1}\text{ holding } X_2 \text{ constant}$$

. . .

Similarly, for $\beta_2$:

$$\beta_2 =\frac{\Delta Y}{\Delta X_2}\text{ holding }X_1 \text{  constant}$$

. . .

And for the constant, $\beta_0$:

$$\beta_0 =\text{predicted value of Y when } X_1=0, \; X_2=0$$

## You Can Keep Your Intuitions...But They're Wrong Now

::: columns
::: {.column width="50%"}
- We have been envisioning OLS regressions as the equation of a line through a scatterplot of data on two variables, $X$ and $Y$
    - $\beta_0$: “intercept”
    - $\beta_1$: “slope”

- With 3+ variables, OLS regression is no longer a “line” for us to estimate...

:::
::: {.column width="50%"}
:::
:::

## You Can Keep Your Intuitions...But They're Wrong Now

::: columns
::: {.column width="40%"}
- We have been envisioning OLS regressions as the equation of a line through a scatterplot of data on two variables, $X$ and $Y$
    - $\beta_0$: “intercept”
    - $\beta_1$: “slope”

- With 3+ variables, OLS regression is no longer a “line” for us to estimate...

:::
::: {.column width="60%"}
```{r}
library(plotly)
elreg<-lm(testscr~str+el_pct,data=ca_school)
axis_x <- seq(min(ca_school$str), max(ca_school$str), by = 1)
axis_y <- seq(min(ca_school$el_pct), max(ca_school$el_pct), by = 1)


lm_surface <- expand.grid(str = axis_x,el_pct = axis_y,KEEP.OUT.ATTRS = F)
lm_surface$testscr <- predict.lm(elreg, newdata = lm_surface)
lm_surface <- reshape2::acast(lm_surface, el_pct ~ str, value.var = "testscr") #y ~ x



plot_ly(ca_school, x = ~str, y = ~el_pct, z = ~testscr) %>%
  add_markers(alpha=0.75) %>%
  layout(scene = list(xaxis = list(title = 'Class Size'),
                     yaxis = list(title = 'Percent EL'),
                     zaxis = list(title = 'Test Scores'))) %>%
  add_trace(z = lm_surface,
                       x = axis_x,
                       y = axis_y,
                       type = "surface")
```

:::
:::

## The “Constant”

- Alternatively, we can write the population regression equation as:
$$Y_i=\beta_0\color{#e64173}{X_{0i}}+\beta_1X_{1i}+\beta_2X_{2i}+u_i$$

- Here, we added $X_{0i}$ to $\beta_0$

- $X_{0i}$ is a [constant regressor]{.hi}, as we define $X_{0i}=1$ for all $i$ observations

- Likewise, $\beta_0$ is more generally called the [“constant”]{.hi} term in the regression (instead of the “intercept”)

- This may seem silly and trivial, but this will be useful soon!  

## The Population Regression Model: Example I

::: {.callout-tip appearance="simple" icon=false}

## Example

$$\text{Beer Consumption}_i=\beta_0+\beta_1 \, \text{Price}_i+\beta_2 \, \text{Income}_i+\beta_3 \, \text{Nachos Price}_i+\beta_4 \, \text{Wine Price}+u_i$$
:::

. . .

- Let's see what you remember from micro!

. . .

- What measures the **price effect**? What sign should it have?

. . .

- What measures the **income effect**? What sign should it have? What should inferior or normal (necessities & luxury) goods look like?

. . .

- What measures the **cross-price effect(s)**? What sign should substitutes and complements have?

## The Population Regression Model: Example II

::: {.callout-tip appearance="simple" icon=false}

## Example

$$\widehat{\text{Beer Consumption}_i}=20-1.5 \, \text{Price}_i+1.25 \, \text{Income}_i-0.75 \, \text{Nachos Price}_i+1.3 \, \text{Wine Price}_i$$
:::

- Interpret each $\hat{\beta}$

# The Multivariate Regression Model {background-color="#314f4f" .centered}

## Multivariate Regression in R

::: columns
::: {.column width="50%"}
```{r}
#| echo: true

# run regression of testscr on str and el_pct
school_reg_2 <- lm(testscr ~ str + el_pct, 
                 data = ca_school)
```
:::
::: {.column width="50%"}
- Format for regression is
```{r, eval = F, echo = T}
lm(y ~ x1 + x2, data = df)
```

- `y` is dependent variable (listed first!)
- `~` means “is modeled by” or “is explained by”
- `x1` and `x2` are the independent variables
- `df` is the dataframe where the data is stored

:::
:::

## Multivariate Regression in R

```{r}
#| echo: true

# look at reg object
school_reg_2
```

- Stored as an `lm` object called `school_reg_2`, a `list` object

## Multivariate Regression in R

```{r}
#| echo: true
# get full summary
summary(school_reg_2)
```

- Stored as an `lm` object called `school_reg_2`, a `list` object

## Multivariate Regression with Broom

- The `tidy()` function creates a *tidy* `tibble` of regression output

```{r}
#| echo: true
#| eval: true
#| output-location: fragment
# load packages
library(broom)

# tidy regression output
school_reg_2 %>% 
  tidy()
```

## Multivariate Regression Output Table

::: columns
::: {.column width="50%"}
```{r}
#| echo: true
#| eval: false
# load package
library(modelsummary)

modelsummary(models = list("Test Score" = school_reg,
                           "Test Score" = school_reg_2),
             fmt = 2, # round to 2 decimals
             output = "html",
             coef_rename = c("(Intercept)" = "Constant",
                             "str" = "STR"),
             gof_map = list(
               list("raw" = "nobs", "clean" = "n", "fmt" = 0),
               list("raw" = "r.squared", "clean" = "R<sup>2</sup>", "fmt" = 2),
               list("raw" = "rmse", "clean" = "SER", "fmt" = 2)
             ),
             escape = FALSE,
             stars = c('*' = .1, '**' = .05, '***' = 0.01)
)

```
:::
::: {.column width="50%"}
```{r}
#| echo: false
#| eval: true
# load package
library(modelsummary)

modelsummary(models = list("Test Score" = school_reg,
                           "Test Score" = school_reg_2),
             fmt = 2, # round to 2 decimals
             output = "html",
             coef_rename = c("(Intercept)" = "Constant",
                             "str" = "STR"),
             gof_map = list(
               list("raw" = "nobs", "clean" = "n", "fmt" = 0),
               list("raw" = "r.squared", "clean" = "R<sup>2</sup>", "fmt" = 2),
               list("raw" = "rmse", "clean" = "SER", "fmt" = 2)
             ),
             escape = FALSE,
             stars = c('*' = .1, '**' = .05, '***' = 0.01)
)

```

:::
:::
