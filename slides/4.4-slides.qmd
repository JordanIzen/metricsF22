---
format:
  revealjs:
    theme: [default, custom.scss]
    logo: "../images/metrics_hex.png"
    height: 900
    width: 1600
    df-print: paged
    slide-number: c
    chalkboard: true
overview: true
execute:
  echo: false
  warning: false
  freeze: auto
---

##  {data-menu-title="Title Slide" background-image="images/metrics_title_slide.png"}

[4.4 --- Nonlinearity & Transformations]{.custom-title}

[ECON 4470 • Econometrics]{.custom-subtitle}

[Jordan Izenwasser <br> Slides Adapated from Ryan Safner, PhD]{.custom-author}

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(broom)
library(haven)
library(kableExtra)
library(patchwork)
library(fontawesome)
library(gapminder)
library(ggthemes)
library(scales)
library(infer)
library(ggdag)
library(dagitty)
knitr::opts_chunk$set(echo=F,
                      message=F,
                      warning=F)
update_geom_defaults("label", list(family = "Fira Sans Condensed"))
update_geom_defaults("text", list(family = "Fira Sans Condensed"))

library(ggthemes)
library(wooldridge)
wages<-wooldridge::wage1
wages<-wages %>%
  mutate(gender = factor(ifelse(female==0,
                         "Male",
                         "Female")))

```

## Contents {background-color="#314f4f"}

### [Nonlinear Effects](#nonlinear-effects)

### [Polynomial Models](#regression-with-dummy-variables-1)

### [Quadratic Model](#quadratic-model)

### [Logarithmic Models](#logarithmic-models)

### [Linear-Log Model](#linear-log-model)

### [Log-Linear Model](#log-linear-model)

### [Log-Log Model](#log-log-model)

### [Joint Hypothesis Testing](#joint-hypothesis-testing)

# Nonlinear Effects {.centered background-color="#314f4f"}

## *Linear* Regression

- Often, data and relationships between variables may *not* be linear

## *Linear* Regression {.smaller}

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 14

library(gapminder)
p1 <- ggplot(data = gapminder %>% filter(gdpPercap < 60000))+
  aes(x = gdpPercap,
      y = lifeExp)+
  geom_point(color="blue", alpha=0.5)+
  scale_x_continuous(labels=scales::dollar,
                     breaks=seq(0,120000,20000))+
  scale_y_continuous(breaks=seq(0,100,10),
                     limits=c(0,100))+
  labs(x = "GDP per Capita",
       y = "Life Expectancy (Years)")+
  coord_cartesian(clip = "off")+
  theme_bw(base_family = "Fira Sans Condensed",
           base_size=16)
p1
```

## *Linear* Regression {.smaller}

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 14
p1 + geom_smooth(method = "lm", color = "red")
```

$$\color{red}{\widehat{\text{Life Expectancy}_i}=\hat{\beta_0}+\hat{\beta_1}\text{GDP}_i}$$

## *Linear* Regression {.smaller}

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 14
p1 + geom_smooth(method = "lm", color = "red")+
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), color = "green")
```

$$\color{red}{\widehat{\text{Life Expectancy}_i}=\hat{\beta_0}+\hat{\beta_1}\text{GDP}_i}$$

$$\color{green}{\widehat{\text{Life Expectancy}_i}=\hat{\beta_0}+\hat{\beta_1}\text{GDP}_i+\hat{\beta_2}\text{GDP}_i^2}$$

## *Linear* Regression {.smaller}

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 14
p1 + geom_smooth(method="lm", color="red")+
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), color = "green")+
  stat_smooth(method = "lm", formula=y~log(x), color="orange", size=2)
```

$$\color{red}{\widehat{\text{Life Expectancy}_i}=\hat{\beta_0}+\hat{\beta_1}\text{GDP}_i}$$

$$\color{green}{\widehat{\text{Life Expectancy}_i}=\hat{\beta_0}+\hat{\beta_1}\text{GDP}_i+\hat{\beta_2}\text{GDP}_i^2}$$

$$\color{orange}{\widehat{\text{Life Expectancy}_i}=\hat{\beta_0}+\hat{\beta_1}\ln \text{GDP}_i}$$

## Sources of Nonlinearities

- Effect of $X_1 \rightarrow Y$ might be nonlinear if:

. . .

1. $X_1 \rightarrow Y$ is different for different levels of $X_1$
    - e.g. **diminishing returns**: $\uparrow X_1$ increases $Y$ at a *decreasing* rate
    - e.g. **increasing returns**: $\uparrow X_1$ increases $Y$ at an *increasing* rate

. . .

2. $X_1 \rightarrow Y$ is different for different levels of $X_2$
    - e.g. interaction effects (last lesson)

## Nonlinearities Alter Marginal Effects

::: columns
::: {.column width="50%"}
- **Linear**:
$$Y=\hat{\beta_0}+\hat{\beta_1}X$$

- marginal effect (slope), $(\hat{\beta_1}) = \frac{\Delta Y}{\Delta X}$ is constant for all $X$
:::

::: {.column width="50%"}
```{r}
line=function(x){10-x}

ggplot(data.frame(x=c(0,10)), aes(x=x))+
  stat_function(fun=line, geom="line", size=2, color="blue")+
  geom_segment(aes(x=5, xend=5, y=line(5), yend=line(6)), linetype="dashed", color="red", size=1)+
  geom_segment(aes(x=5, xend=6, y=line(6), yend=line(6)), linetype="dashed", color="red", size=1)+
  scale_x_continuous(breaks=seq(0,10,1),
                     limits=c(0,10),
                     expand=expand_scale(mult=c(0,0.1)))+
  scale_y_continuous(breaks=seq(0,10,1),
                     limits=c(0,10),
                     expand=expand_scale(mult=c(0,0.1)))+
  labs(x = "X",
       y = "Y")+
  theme_bw(base_family = "Fira Sans Condensed", base_size=16)

```
:::
:::

## Nonlinearities Alter Marginal Effects

::: columns
::: {.column width="50%"}
- **Polynomial**:
$$Y=\hat{\beta_0}+\hat{\beta_1}X+\hat{\beta_2}X^2$$

- Marginal effect, “slope” $\left(\neq \hat{\beta_1}\right)$ *depends on the value of* $X$!
:::

::: {.column width="50%"}
```{r}
curve=function(x){10/x}

ggplot(data.frame(x=c(0,10)), aes(x=x))+
  stat_function(fun=curve, geom="line", size=2, color="blue")+
  geom_segment(aes(x=1, xend=1, y=5, yend=9), linetype="dashed", color="red", size=1)+
  geom_segment(aes(x=1, xend=2, y=5, yend=5), linetype="dashed", color="red", size=1)+
  geom_segment(aes(x=4, xend=4, y=2.5, yend=2), linetype="dashed", color="red", size=1)+
  geom_segment(aes(x=4, xend=5, y=2, yend=2), linetype="dashed", color="red", size=1)+
  geom_segment(aes(x=8, xend=8, y=curve(8), yend=curve(9)), linetype="dashed", color="red", size=1)+
  geom_segment(aes(x=8, xend=9, y=curve(9), yend=curve(9)), linetype="dashed", color="red", size=1)+
  scale_x_continuous(breaks=seq(0,10,1),
                     limits=c(0,10),
                     expand=expand_scale(mult=c(0,0.1)))+
  scale_y_continuous(breaks=seq(0,10,1),
                     limits=c(0,10),
                     expand=expand_scale(mult=c(0,0.1)))+
  labs(x = "X",
       y = "Y")+
  theme_bw(base_family = "Fira Sans Condensed", base_size=16)

```
:::
:::


## Nonlinearities Alter Marginal Effects

::: columns
::: {.column width="50%"}
- **Interaction Effect**:
$$\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X_1+\hat{\beta_2}X_2+\hat{\beta_3}X_1 \times X_2$$

- Marginal effect, “slope” *depends on the value of* $X_2$!

- Easy example: if $X_2$ is a dummy variable:
  - [$X_2=0$ (control)]{.blue} vs. [$X_2=1$ (treatment)]{.pink}
:::

::: {.column width="50%"}
```{r}
treat=function(x){4+x}
control=function(x){2+0.5*x}

ggplot(data.frame(x=c(0,10)), aes(x=x))+
  stat_function(fun=treat, geom="line", size=2, color="#e64173")+
  geom_label(x=3, y=treat(3), color="#e64173", label=expression(paste("When ",x[2]==1)))+
  geom_segment(aes(x=5, xend=6, y=treat(5), yend=treat(5)), linetype="dashed", color="black", size=1)+
  geom_segment(aes(x=6, xend=6, y=treat(5), yend=treat(6)), linetype="dashed", color="black", size=1)+
  stat_function(fun=control, geom="line", size=2, color="#0047AB")+
  geom_label(x=3, y=control(3), color="#0047AB", label=expression(paste("When ",x[2]==0)))+
  geom_segment(aes(x=5, xend=6, y=control(5), yend=control(5)), linetype="dashed", color="black", size=1)+
  geom_segment(aes(x=6, xend=6, y=control(5), yend=control(6)), linetype="dashed", color="black", size=1)+
  
  scale_x_continuous(breaks=seq(0,10,1),
                     limits=c(0,10),
                     expand=expand_scale(mult=c(0,0.1)))+
  scale_y_continuous(breaks=seq(0,10,1),
                     limits=c(0,10),
                     expand=expand_scale(mult=c(0,0.1)))+
  labs(x = expression(X[1]),
       y = "Y")+
  theme_bw(base_family = "Fira Sans Condensed", base_size=16)

```
:::
:::

# Polynomial Models {.centered background-color="#314f4f"}

## Polynomial Functions of $X$ I {.smaller}

::: columns
::: {.column width="50%"}
- [Linear]{.blue}

$$\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X$$

:::

::: {.column width="50%"}
```{r}
#| fig-height: 9
linear=function(x){x}
quadratic=function(x){x^2}
cubic=function(x){x^3}
quartic=function(x){x^4}

poly<-ggplot(data.frame(x=c(-10,10)), aes(x=x))+
  stat_function(fun=linear, geom="line", size=2, color = "blue")+
  geom_hline(yintercept=0, size = 1)+
  geom_vline(xintercept=0, size = 1)+
    scale_x_continuous(breaks=seq(-1,1,1),
                     limits=c(-1,1),
                     expand=expand_scale(mult=c(0,0.1)))+
  scale_y_continuous(breaks=seq(-1,1,1),
                     limits=c(-1,1))+
  labs(x = "X",
       y = "Y")+
  theme_classic(base_family = "Fira Sans Condensed", base_size=16)
poly
```
:::
:::

## Polynomial Functions of $X$ I {.smaller}

::: columns
::: {.column width="50%"}
- [Linear]{.blue}

$$\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X$$

- [Quadratic]{.green}

$$\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X+\hat{\beta_2}X^2$$

:::

::: {.column width="50%"}
```{r}
#| fig-height: 9
poly+stat_function(fun=quadratic, geom="line", size=2, color = "green")
```
:::
:::

## Polynomial Functions of $X$ I {.smaller}

::: columns
::: {.column width="50%"}
- [Linear]{.blue}

$$\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X$$

- [Quadratic]{.green}

$$\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X+\hat{\beta_2}X^2$$

- [Cubic]{.orange}

$$\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X+\hat{\beta_2}X^2+\hat{\beta_3}X^3$$

:::

::: {.column width="50%"}
```{r}
#| fig-height: 9
poly+stat_function(fun=quadratic, geom="line", size=2, color = "green")+
  stat_function(fun=cubic, geom="line", size=2, color = "orange")
```
:::
:::

## Polynomial Functions of $X$ I {.smaller}

::: columns
::: {.column width="50%"}
- [Linear]{.blue}

$$\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X$$

- [Quadratic]{.green}

$$\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X+\hat{\beta_2}X^2$$

- [Cubic]{.orange}

$$\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X+\hat{\beta_2}X^2+\hat{\beta_3}X^3$$
- [Quartic]{.purple}

$$\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X+\hat{\beta_2}X^2+\hat{\beta_3}X^3+\hat{\beta_4}X^4$$

:::

::: {.column width="50%"}
```{r}
#| fig-height: 9
poly+stat_function(fun=quadratic, geom="line", size=2, color = "green")+
  stat_function(fun=cubic, geom="line", size=2, color = "orange")+
  stat_function(fun=quartic, geom="line", size=2, color = "purple")

```
:::
:::

## Polynomial Functions of $X$ II

$$\hat{Y_i} = \hat{\beta_0} + \hat{\beta_1} X_i + \hat{\beta_2} X_i^2 + \cdots + \hat{\beta_{\color{#e64173}{r}}} X_i^{\color{#e64173}{r}} + u_i$$

. . .

- Where $\color{e64173}{r}$ is the highest power $X_i$ is raised to
  - quadratic $\color{e64173}{r=2}$
  - cubic $\color{e64173}{r=3}$

. . .

- The graph of an $r$^th^-degree polynomial function has $(r-1)$ bends 

. . .

- Just another multivariate OLS regression model!

# Quadratic Model {.centered background-color="#314f4f"}

## Quadratic Model

$$\hat{Y_i} = \hat{\beta_0} + \hat{\beta_1} X_i + \hat{\beta_2} X_i^2$$

- [Quadratic model]{.hi} has $X$ and $X^2$ variables in it (yes, need both!)

. . .

- How to interpret coefficients (betas)?
  - $\beta_0$ as “intercept” and $\beta_1$ as “slope” makes no sense  
  - $\beta_1$ as effect $X_i \rightarrow Y_i$ holding $X_i^2$ constant??^[Note: this is *not* a perfect multicollinearity problem! Correlation only measures *linear* relationships!]

. . .

- **Estimate marginal effects** by calculating predicted $\hat{Y_i}$ for different levels of $X_i$

## Quadratic Model: Calculating Marginal Effects

$$\hat{Y_i} = \hat{\beta_0} + \hat{\beta_1} X_i + \hat{\beta_2} X_i^2$$

- What is the [marginal effect]{.hi} of $\Delta X_i \rightarrow \Delta Y_i$?

. . .

- Take the **derivative** of $Y_i$ with respect to $X_i$:
$$\frac{\partial \, Y_i}{\partial \, X_i} = \hat{\beta_1}+2\hat{\beta_2} X_i$$

. . .

- [Marginal effect]{.hi} of a 1 unit change in $X_i$ is a $\color{#6A5ACD}{\left(\hat{\beta_1}+2\hat{\beta_2} X_i \right)}$ unit change in $Y$

## Quadratic Model: Example I

::: callout-tip
## Example

$$\widehat{\text{Life Expectancy}_i} = \hat{\beta_0}+\hat{\beta_1} \, \text{GDP per capita}_i+\hat{\beta_2}\, \text{GDP per capita}^2_i$$
:::

. . .

- Use `gapminder` package and data

```{r}
#| echo: true
library(gapminder)
```

## Quadratic Model: Example II

- These coefficients will be very large, so let's transform `gdpPercap` to be in $1,000's

```{r}
#| echo: true
#| output-location: fragment
gapminder <- gapminder %>%
  mutate(GDP_t = gdpPercap/1000)

gapminder %>% head() # look at it
```

## Quadratic Model: Example II

- Let’s also create a squared term, `gdp_sq`

```{r}
#| echo: true
#| output-location: fragment
gapminder <- gapminder %>%
  mutate(GDP_sq = GDP_t^2)

gapminder %>% head() # look at it
```

## Quadratic Model: Example IV

- Can “manually” run a multivariate regression with `GDP_t` and `GDP_sq`

```{r}
#| echo: true
#| output-location: fragment
library(broom)
reg1 <- lm(lifeExp ~ GDP_t + GDP_sq, data = gapminder)

reg1 %>% tidy()
```

## Quadratic Model: Example IV

- OR use `gdp_t` and add the `I()` operator to transform the variable in the regression, `I(gdp_t^2)`^[[Here](https://stackoverflow.com/questions/8055508/in-r-formulas-why-do-i-have-to-use-the-i-function-on-power-terms-like-y-i) is a decent explanation of what `I()` does. An alternative is to use `poly(GDP_t, 2)` to make the squared term, but this [has some issues](https://stats.stackexchange.com/questions/25975/how-to-add-second-order-terms-into-the-model-in-r).]

```{r}
#| echo: true
#| output-location: fragment
reg1_alt <- lm(lifeExp ~ GDP_t + I(GDP_t^2), data = gapminder)

reg1_alt %>% tidy()
```

## Quadratic Model: Example V

```{r}
reg1 %>% tidy()
```

. . .

$$\widehat{\text{Life Expectancy}_i} = 50.52+1.55 \, \text{GDP}_i - 0.02\, \text{GDP}^2_i$$

. . .

- Positive effect $(\hat{\beta_1}>0)$, with diminishing returns $(\hat{\beta_2}<0)$

- Marginal effect of GDP on Life Expectancy **depends on initial value of GDP!**

## Quadratic Model: Example VI

```{r}
reg1 %>% tidy()
```

- [Marginal effect]{.hi} of GDP on Life Expectancy:

. . .

\begin{align*}
\frac{\partial \, Y}{\partial \; X} &= \hat{\beta_1}+2\hat{\beta_2} X_i\\
\frac{\partial \, \text{Life Expectancy}}{\partial \, \text{GDP}} &\approx 1.55+2(-0.02) \, \text{GDP}\\
 &\approx \color{#e64173}{1.55-0.04 \, \text{GDP}}\\
\end{align*}

::: footer
:::

## Quadratic Model: Example VII

$$\frac{\partial \, \text{Life Expectancy}}{\partial \, \text{GDP}} = 1.55-0.04 \, \text{GDP}$$

. . .

Marginal effect of GDP if GDP $=5$ ($ thousand): 

. . .

\begin{align*}
\frac{\partial \, \text{Life Expectancy}}{\partial \, \text{GDP}} &= 1.55-0.04\text{GDP}\\
&= 1.55-0.04(5)\\
&= 1.55-0.20\\
&=1.35\\
\end{align*}

. . .

- i.e. for every addition $1 (thousand) in GDP per capita, average life expectancy increases by 1.35 years

## Quadratic Model: Example VIII

$$\frac{\partial \, \text{Life Expectancy}}{\partial \, \text{GDP}} = 1.55-0.04 \, \text{GDP}$$

Marginal effect of GDP if GDP $=25$ ($ thousand): 

. . .

\begin{align*}
\frac{\partial \, \text{Life Expectancy}}{\partial \, \text{GDP}} &= 1.55-0.04\text{GDP}\\
&= 1.55-0.04(25)\\
&= 1.55-1.00\\
&=0.55\\
\end{align*}

. . .

- i.e. for every addition $1 (thousand) in GDP per capita, average life expectancy increases by 0.55 years

## Quadratic Model: Example X

$$\frac{\partial \, \text{Life Expectancy}}{\partial \, \text{GDP}} = 1.55-0.04 \, \text{GDP}$$

Marginal effect of GDP if GDP $=50$ ($ thousand): 

. . .

\begin{align*}
\frac{\partial \, \text{Life Expectancy}}{\partial \, \text{GDP}} &= 1.55-0.04\text{GDP}\\
&= 1.55-0.04(50)\\
&= 1.55-2.00\\
&=-0.45\\
\end{align*}

. . .

- i.e. for every addition $1 (thousand) in GDP per capita, average life expectancy *decreases* by 0.45 years

## Quadratic Model: Example XI

\begin{align*}\widehat{\text{Life Expectancy}_i} &= 50.52+1.55 \, \text{GDP per capita}_i - 0.02\, \text{GDP per capita}^2_i \\
\frac{\partial \, \text{Life Expectancy}}{d \, \text{GDP}} &= 1.55-0.04\text{GDP} \\ \end{align*}

| *Initial* GDP per capita | Marginal Effect^[Of +$1,000 GDP/capita on Life Expectancy.] |
|----------------|-------------------:|
| $5,000 | $1.35$ years |
| $25,000 | $0.55$ years |
| $50,000 | $-0.45$ years |

## Quadratic Model: Example XII

```{r}
#| fig-align: center
#| fig-width: 14
#| echo: true
#| code-fold: true

ggplot(data = gapminder)+
  aes(x = GDP_t,
      y = lifeExp)+
  geom_point(color = "blue", alpha=0.5)+
  stat_smooth(method = "lm",
              formula = y ~ x + I(x^2),
              color = "green")+ 
  geom_vline(xintercept = c(5,25,50),
             linetype = "dashed",
             color = "red", size = 1)+
  scale_x_continuous(labels = scales::dollar,
                     breaks = seq(0,120,10))+
  scale_y_continuous(breaks = seq(0,100,10),
                     limits = c(0,100))+
  labs(x = "GDP per Capita (in Thousands)",
       y = "Life Expectancy (Years)")+
  theme_bw(base_family = "Fira Sans Condensed",
           base_size=16)
```


## Determining If Polynomials Are Necessary I

```{r}
tidy(reg1)
```

- Is the quadratic term necessary?

. . .

- Determine if $\hat{\beta_2}$ (on $X_i^2)$ is statistically significant:
  - $H_0: \hat{\beta_2}=0$
  - $H_a: \hat{\beta_2} \neq 0$
  
. . .

- Statistically significant $\implies$ we should keep the quadratic model
  - If we only ran a linear model, it would be incorrect!

::: footer
:::

## Determining Polynomials are Necessary II

- Should we keep going up in polynomials?

. . .

```{R}
#| fig-align: center
#| fig-width: 14

p1+geom_smooth(method="lm", color="red")+
  stat_smooth(method="lm", formula=y~x+I(x^2), color = "green")
```

## Determining Polynomials are Necessary II

- Should we keep going up in polynomials?

```{R}
#| fig-align: center
#| fig-width: 14

p1+geom_smooth(method="lm", color="red")+
  stat_smooth(method="lm", formula=y~x+I(x^2), color = "green")+
  stat_smooth(method="lm", formula=y~x+I(x^2)+I(x^3), color = "purple")
```

$$\color{#6A5ACD}{\widehat{\text{Life Expectancy}_i} = \hat{\beta_0}+\hat{\beta_1} GDP_i+\hat{\beta_2}GDP^2_i+\hat{\beta_3}GDP_i^3}$$

## Determining Polynomials are Necessary III

- In general, you should have a [compelling theoretical reason]{.hi-purple} why data or relationships should [“change direction”]{.hi-purple} multiple times

- Or clear data patterns that have multiple “bends”

- Recall, we care more about accurately measuring the causal effect of $X \rightarrow Y$, rather than getting the most accurate prediction possible for $\hat{Y}$

```{r}
#| fig-align: center
#| fig-width: 14

# make obviously cubic data
df_3<-tibble(x = seq(0,20,0.05),
           y = (500 + 0.4 * (x-10)^3)+rnorm(length(x), 10, 50)) # real function + random noise

ggplot(data = df_3)+
  aes(x = x,
      y = y)+
  geom_point(color="blue")+
  stat_smooth(method = "lm", formula = y~x+I(x^2)+I(x^3), color="red")+
  labs(x = "X",
       y = "Y")+
  theme_bw(base_family = "Fira Sans Condensed",
           base_size=16)

```

## Determining Polynomials are Necessary IV

```{r}
cub_reg <- lm(lifeExp ~ GDP_t + I(GDP_t^2) + I(GDP_t^3), data = gapminder)

cub_reg %>% tidy()
```

. . .

- $\hat{\beta_3}$ is statistically significant...
- ...but can we really think of a good reason to complicate the model?

## If You Kept Going... {.smaller}

```{r}
lm(lifeExp ~ GDP_t + I(GDP_t^2) + I(GDP_t^3) + I(GDP_t^4) + I(GDP_t^5) + I(GDP_t^6) + I(GDP_t^7) + I(GDP_t^8) + I(GDP_t^9), data = gapminder) %>% tidy()
```

. . . 

- It takes until a 9^th^-degree polynomial for one of the terms to become insignificant...

- ...but does this make the model *better*? *more interpretable*? 

- A famous problem of [overfitting]{.hi}

## If You Kept Going...Visually

```{r}
p1+geom_smooth(method="lm", color="red")+
  stat_smooth(method="lm", formula=y~x+I(x^2), color = "green")+
  stat_smooth(method="lm", formula=y~x+I(x^2)+I(x^3), color = "purple")
```

## If You Kept Going...Visually

```{r}
p1+stat_smooth(method="lm", formula=y~x+I(x^2)+I(x^3)+I(x^4), color = "black")
```

A 4^th^-degree polynomial


## If You Kept Going...Visually

```{r}
p1+stat_smooth(method="lm", formula=y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8)+I(x^9), color = "black")
```

A 9^th^-degree polynomial

## If You Kept Going...Visually

```{r}
p1+stat_smooth(method="lm", formula=y ~ poly(x,14), color = "black")
```
A 14^th^-degree polynomial


## Strategy for Polynomial Model Specification

1. Are there good theoretical reasons for relationships changing (e.g. increasing/decreasing returns)?

. . .

2. Plot your data: does a straight line fit well enough?

. . .

3. Specify a polynomial function of a higher power (start with 2) and estimate OLS regression

. . .

4. Use $t$-test to determine if higher-power term is significant

. . .

5. Interpret effect of change in $X$ on $Y$

. . .

6. Repeat steps 3-5 as necessary (if there are good theoretical reasons)

# Logarithmic Models {.centered background-color="#314f4f"}

## *Linear* Regression {.smaller}

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 14
p1 + geom_smooth(method="lm", color="red")+
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), color = "green")+
  stat_smooth(method = "lm", formula=y~log(x), color="orange", size=2)
```

$$\color{red}{\widehat{\text{Life Expectancy}_i}=\hat{\beta_0}+\hat{\beta_1}\text{GDP}_i}$$

$$\color{green}{\widehat{\text{Life Expectancy}_i}=\hat{\beta_0}+\hat{\beta_1}\text{GDP}_i+\hat{\beta_2}\text{GDP}_i^2}$$

$$\color{orange}{\widehat{\text{Life Expectancy}_i}=\hat{\beta_0}+\hat{\beta_1}\ln \text{GDP}_i}$$

## Logarithmic Models

::: columns
::: {.column width="50%"}
- Another useful model for nonlinear data is the [logarithmic model]{.hi}^[Don’t confuse this with a [logistic (logit) model]{.hi} for *dependent* dummy variables.]
  - We transform either $X$, $Y$, or *both* by taking the [(natural) logarithm]{.hi-purple}

- Logarithmic model has two additional advantages
  1. We can easily interpret coefficients as **percentage changes** or **elasticities**
  2. Useful economic shape: diminishing returns (production functions, utility functions, etc)
:::

::: {.column width="50%"}
```{r}
ggplot(data.frame(x=0:10),aes(x=x))+
    stat_function(fun=log, color="blue",size =2)+
  geom_label(x=9,y=log(9), label="y=log x", color="blue", size=5)+
  scale_x_continuous(breaks=seq(0,10,1),
                     limits=c(0,10),
                     expand=c(0,0))+
  scale_y_continuous(limits=c(0,2.5),
                     expand=c(0,0))+
  labs(x = "X",
       y = "Y")+
  theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)

```
:::
:::


## The Natural Logarithm: Example {.smaller}

- Most useful property: for small change in $x$, $\Delta x$:

$$\underbrace{\ln(x+\Delta x) - \ln(x)}_{\text{Difference in logs}} \approx \underbrace{\frac{\Delta x}{x}}_{\text{Relative change}}$$

. . .

::: callout-tip
## Example

Let $x=100$ and $\Delta x =1$, relative change is:

$$\frac{\Delta x}{x} = \frac{(101-100)}{100} = 0.01 \text{ or }1\%$$
:::

. . .


- The logged difference:
$$\ln(101)-\ln(100) = 0.00995 \approx 1\%$$

. . .

- This allows us to very easily interpret coefficients as **percent changes** or [elasticities]{.hi-purple}

## Elasticity

- An [elasticity]{.hi} between any two variables, $\epsilon_{Y,X}$ describes the [responsiveness]{.hi-purple} (in %) of one variable $(Y)$ to a change in another $(X)$

. . .

$$\epsilon_{Y,X}=\frac{\% \Delta Y}{\% \Delta X} =\cfrac{\left(\frac{\Delta Y}{Y}\right)}{\left( \frac{\Delta X}{X}\right)}$$

. . .

- Numerator is relative change in $Y$, Denominator is relative change in $X$

. . .

- [Interpretation]{.hi-purple}: a 1% change in $X$ will cause a $\epsilon_{Y,X}$% change in $Y$


## Math FYI: Cobb Douglas Functions and Logs

- One of the (many) reasons why economists love Cobb-Douglas functions:
$$Y=AL^{\alpha}K^{\beta}$$

. . .

- Taking logs, relationship becomes linear:

$$\ln(Y)=\ln(A)+\alpha \ln(L)+ \beta \ln(K)$$

. . .

- With data on $(Y, L, K)$ and linear regression, can estimate $\alpha$ and $\beta$
  - $\alpha$: elasticity of $Y$ with respect to $L$
      - A 1% change in $L$ will lead to an $\alpha$% change in $Y$
  - $\beta$: elasticity of $Y$ with respect to $K$
      - A 1% change in $K$ will lead to a $\beta$% change in $Y$

## Math FYI: Cobb Douglas Functions and Logs

::: callout-tip
## Example

$$Y=2L^{0.75}K^{0.25}$$

:::

. . .

- Taking logs:

$$\ln Y=\ln 2+0.75 \ln L + 0.25 \ln K$$

. . .

- A 1% change in $L$ will yield a 0.75% change in output $Y$

- A 1% change in $K$ will yield a 0.25% change in output $Y$

## Logarithms in `R` I

- The `log()` function can easily take the logarithm

```{r}
#| echo: true
#| output-location: fragment

gapminder <- gapminder %>%
  mutate(loggdp = log(gdpPercap)) # log GDP per capita

gapminder %>% head() # look at it
```


## Logarithms in `R` II

- Note, `log()` by default is the **natural logarithm $ln()$**, i.e. base `e`
  - Can change base with e.g. `log(x, base = 5)`
  - Some common built-in logs: `log10`, `log2` 

```{r}
#| echo: true

log10(100)
log2(16)
log(19683, base=3)
```

## Logarithms in `R` III

- Note when running a regression, you can pre-transform the data into logs (as I did above), or just add `log()` around a variable in the regression

```{r}
lm(lifeExp ~ loggdp,
   data = gapminder) %>%
  tidy()
```

## Types of Logarithmic Models

- Three types of log regression models, depending on which variables we log

. . .

1.  [Linear-log model:]{.hi-purple} $Y_i=\beta_0+\beta_1 \color{#e64173}{\ln X_i}$

. . .

2. [Log-linear model:]{.hi-purple} $\color{#e64173}{\ln Y_i}=\beta_0+\beta_1X_i$

. . .

3. [Log-log model:]{.hi-purple} $\color{#e64173}{\ln Y_i}=\beta_0+\beta_1 \color{#e64173}{\ln X_i}$

# Linear-Log Model {.centered background-color="#314f4f"}

## Linear-Log Model: Interpretation

- [Linear-log model]{.hi-purple} has an independent variable $(X)$ that is logged

\begin{align*}
Y&=\beta_0+\beta_1 \color{#e64173}{\ln X_i}\\
\beta_1&=\cfrac{\Delta Y}{\big(\frac{\Delta X}{X}\big)}\\
\end{align*}

. . .

- [**Marginal effect of** $\mathbf{X \rightarrow Y}$: a **1%** change in $X \rightarrow$ a $\frac{\beta_1}{100}$ **unit** change in $Y$]{.hi-purple}

## Linear-Log Model in `R` {.smaller}

```{r}
lin_log_reg <- lm(lifeExp ~ loggdp,
                  data = gapminder)

library(broom)

lin_log_reg %>% tidy()
```

. . .

$$\widehat{\text{Life Expectancy}}_i=-9.10+8.41 \, \text{ln GDP}_i$$

. . .

- A **1% change in GDP** $\rightarrow$ a $\frac{9.41}{100}=$ **0.0841 year increase** in Life Expectancy

. . .

- A **25% fall in GDP** $\rightarrow$ a $(-25 \times 0.0841)=$ **2.1025 year _decrease_** in Life Expectancy

. . .

- A **100% rise in GDP** $\rightarrow$ a $(100 \times 0.0841)=$ **8.4100 year increase** in Life Expectancy 

## Linear-Log Model Graph (Linear X-Axis)

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
#| fig-width: 14

ggplot(data = gapminder)+
  aes(x = gdpPercap,
      y = lifeExp)+
  geom_point(color = "blue", alpha = 0.5)+
  geom_smooth(method = "lm",
              formula = y ~ log(x),
              color = "orange")+ 
  scale_x_continuous(labels = scales::dollar,
                     breaks = seq(0,120000,20000))+
  scale_y_continuous(breaks = seq(0,100,10),
                     limits = c(0,100))+
  labs(x = "GDP per Capita",
       y = "Life Expectancy (Years)")+
  theme_bw(base_family = "Fira Sans Condensed",
           base_size = 16)
```


## Linear-Log Model Graph (Log X-Axis)

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
#| fig-width: 14

ggplot(data = gapminder)+
  aes(x = loggdp,
      y = lifeExp)+
  geom_point(color = "blue", alpha = 0.5)+
  geom_smooth(method = "lm",
              formula = y ~ log(x),
              color = "orange")+ 
  scale_y_continuous(breaks = seq(0,100,10),
                     limits = c(0,100))+
  labs(x = "Log GDP per Capita",
       y = "Life Expectancy (Years)")+
  theme_bw(base_family = "Fira Sans Condensed",
           base_size = 16)
```

# Log-Linear Model {.centered background-color="#314f4f"}

## Log-Linear Model: Interpretation

- [Log-linear model]{.hi-purple} has the dependent variable $(Y)$ logged

. . .

\begin{align*}
\color{#e64173}{\ln Y_i}&=\beta_0+\beta_1 X\\
\beta_1&=\cfrac{\big(\frac{\Delta Y}{Y}\big)}{\Delta X}\\
\end{align*}

. . .

- [**Marginal effect of** $\mathbf{X \rightarrow Y}$: a **1 unit** change in $X \rightarrow$ a $\beta_1 \times 100$ **%** change in $Y$]{.hi-purple}

## Log-Linear Model in `R` (Preliminaries)

- We will again have very large/small coefficients if we deal with GDP directly, again let's transform `gdpPercap` into $1,000s, call it `gdp_t`

- Then log LifeExp

```{r}
#| echo: true
#| output-location: fragment
gapminder <- gapminder %>%
  mutate(gdp_t = gdpPercap/1000, # first make GDP/capita in $1000s
         loglife = log(lifeExp)) # take the log of LifeExp
gapminder %>% head() # look at it
```

## Log-Linear Model in `R` {.smaller}

```{R}
log_lin_reg <- lm(loglife~gdp_t,
                data = gapminder)

log_lin_reg %>% tidy()
```

. . .

$$\widehat{\ln\text{Life Expectancy}}_i=3.967+0.013 \, \text{GDP}_i$$

. . .

- A **$1 (thousand) change in GDP** $\rightarrow$ a $0.013 \times 100\%=$ **1.3% increase** in Life Expectancy

. . .

- A **$25 (thousand) fall in GDP** $\rightarrow$ a $(-25 \times 1.3\%)=$ **32.5% decrease** in Life Expectancy 

. . .

- A **$100 (thousand) rise in GDP** $\rightarrow$ a $(100 \times 1.3\%)=$ **130% increase** in Life Expectancy 

## Linear-Log Model Graph

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
#| fig-width: 14
ggplot(data = gapminder)+
  aes(x = gdp_t,
      y = loglife)+ 
  geom_point(color = "blue", alpha = 0.5)+
  geom_smooth(method = "lm", color = "orange")+
  scale_x_continuous(labels = scales::dollar,
                     breaks = seq(0,120,20))+
  labs(x = "GDP per Capita ($ Thousands)",
       y = "Log Life Expectancy")+
  theme_bw(base_family = "Fira Sans Condensed",
           base_size = 16)
```

# Log-Log Model {.centered background-color="#314f4f"}

## Log-Log Model

- [Log-log model]{.hi-purple} has both variables $(X \text{ and } Y)$ logged

. . .

\begin{align*}
\color{#e64173}{\ln Y_i}&=\beta_0+\beta_1 \color{#e64173}{\ln X_i}\\
\beta_1&=\cfrac{\big(\frac{\Delta Y}{Y}\big)}{\big(\frac{\Delta X}{X}\big)}\\
\end{align*}

. . .

- [**Marginal effect of** $\mathbf{X \rightarrow Y}$: a **1%** change in $X \rightarrow$ a $\beta_1$ **%** change in $Y$]{.hi-purple}

- $\beta_1$ is the [elasticity]{.hi-turquoise} of $Y$ with respect to $X$! 

## Log-Log Model in `R` {.smaller}

```{r}
log_log_reg <- lm(loglife ~ loggdp,
                  data = gapminder)

log_log_reg %>% tidy()
```

. . .

$$\widehat{\text{ln Life Expectancy}}_i=2.864+0.147 \, \text{ln GDP}_i$$

. . .

- A **1% change in GDP** $\rightarrow$ a **0.147% increase** in Life Expectancy

. . .

- A **25% fall in GDP** $\rightarrow$ a $(-25 \times 0.147\%)=$ **3.675% decrease** in Life Expectancy 

. . .

- A **100% rise in GDP** $\rightarrow$ a $(100 \times 0.147\%)=$ **14.7% increase** in Life Expectancy 

## Log-Log Model Graph

```{r}
#| echo: true
#| code-fold: true
#| fig-align: center
#| fig-width: 14
ggplot(data = gapminder)+
  aes(x = loggdp,
      y = loglife)+ 
  geom_point(color = "blue", alpha = 0.5)+
  geom_smooth(method = "lm", color = "orange")+
  labs(x = "Log GDP per Capita",
       y = "Log Life Expectancy")+
  theme_bw(base_family = "Fira Sans Condensed",
           base_size = 16)
```

## Comparing Log Models I

| Model | Equation | Interpretation |
|-------|----------|----------------|
| Linear-[Log]{.hi} | $Y=\beta_0+\beta_1 \color{#e64173}{\ln X}$ | 1[%]{.hi} change in $X \rightarrow \frac{\hat{\beta_1}}{100}$ **unit** change in $Y$ |
| [Log]{.hi}-Linear | $\color{#e64173}{\ln Y}=\beta_0+\beta_1X$ | 1 **unit** change in $X \rightarrow \hat{\beta_1}\times 100$[%]{.hi} change in $Y$ |
| [Log]{.hi}-[Log]{.hi} | $\color{#e64173}{\ln Y}=\beta_0+\beta_1\color{#e64173}{\ln X}$ | 1[%]{.hi} change in $X \rightarrow \hat{\beta_1}$[%]{.hi} change in $Y$ |

- Hint: the variable that gets [logged]{.hi} changes in [percent]{.hi} terms, the **linear** variable (not logged) changes in **unit** terms
  - Going from units $\rightarrow$ percent: multiply by 100
  - Going from percent $\rightarrow$ units: divide by 100

## Comparing Models II {.smaller}

```{r}
#| echo: true
#| code-fold: true
library(modelsummary)
modelsummary(models = list("Life Exp." = lin_log_reg,
                           "Log Life Exp." = log_lin_reg,
                           "Log Life Exp." = log_log_reg),
             fmt = 2, # round to 2 decimals
             output = "html",
             coef_rename = c("(Intercept)" = "Constant",
                             "gdp_t" = "GDP per capita ($1,000s)",
                             "loggdp" = "Log GDP per Capita"),
             gof_map = list(
               list("raw" = "nobs", "clean" = "n", "fmt" = 0),
               #list("raw" = "r.squared", "clean" = "R<sup>2</sup>", "fmt" = 2),
               list("raw" = "adj.r.squared", "clean" = "Adj. R<sup>2</sup>", "fmt" = 2),
               list("raw" = "rmse", "clean" = "SER", "fmt" = 2)
             ),
             escape = FALSE,
             stars = c('*' = .1, '**' = .05, '***' = 0.01)
)
```

- Models are very different units, how to choose? 
  1. Compare intuition
  2. Compare $R^2$’s
  3. Compare graphs

## Comparing Models III

| Linear-[Log]{.hi} | [Log]{.hi}-Linear | [Log]{.hi}-[Log]{.hi} |
|:----------:|:----------:|:-------:|
| ![](4.4-slides_files/figure-revealjs/unnamed-chunk-36-1.png) | ![](4.4-slides_files/figure-revealjs/unnamed-chunk-39-1.png) | ![](4.4-slides_files/figure-revealjs/unnamed-chunk-41-1.png) |
| $\hat{Y_i}=\hat{\beta_0}+\hat{\beta_1}\color{#e64173}{\ln X_i}$ | $\color{#e64173}{\ln Y_i}=\hat{\beta_0}+\hat{\beta_1}X_i$ | $\color{#e64173}{\ln Y_i}=\hat{\beta_0}+\hat{\beta_1}\color{#e64173}{\ln X_i}$ |
| $R^2=0.65$ | $R^2=0.30$ | $R^2=0.61$ |

## When to Log? 

- In practice, the following types of variables are usually logged:
  - Variables that must always be **positive** (prices, sales, market values)
  - **Very large** numbers (population, GDP)
  - Variables we want to talk about as **percentage changes or growth rates** (money supply, population, GDP)
  - Variables that have **diminishing returns** (output, utility)
  - Variables that have nonlinear scatterplots

. . .

- *Avoid* logs for:
  - Variables that are less than one, decimals, 0, or negative
  - Categorical variables (season, gender, political party)
  - Time variables (year, week, day)


# Joint Hypothesis Testing {.centered background-color="#314f4f"}

## Joint Hypothesis Testing I

::: callout-tip
## Example

Return again to:

$$\widehat{\text{Wage}}_i=\hat{\beta_0}+\hat{\beta_1} \, \text{Male}_i+\hat{\beta_2}\text{Northeast}_i+\hat{\beta_3}\,\text{Midwest}_i+\hat{\beta_4}\,\text{South}_i$$
:::

. . .

- Maybe region doesn't affect wages *at all*?

. . .

- $H_0: \beta_2=0, \, \beta_3=0, \, \beta_4=0$

. . .

- This is a [joint hypothesis]{.hi} (of multiple parameters) to test

## Joint Hypothesis Testing II

- A [joint hypothesis]{.hi} tests against the null hypothesis of a value for **multiple** parameters:
$$\mathbf{H_0: \beta_1= \beta_2=0}$$
the hypotheses that **multiple** regressors are equal to zero (have no causal effect on the outcome) 

. . .

- Our [alternative hypothesis]{.hi-purple} is that:
$$H_1: \text{ either } \beta_1\neq0\text{ or } \beta_2\neq0\text{ or both}$$

or simply, that $H_0$ is not true 

## Types of Joint Hypothesis Tests

1. $H_0$: $\beta_1=\beta_2=0$
    - Testing against the claim that multiple variables don't matter
    - Useful under high multicollinearity between variables
    - $H_a$: at least one parameter $\neq$ 0

. . .

2. $H_0$: $\beta_1=\beta_2$
    - Testing whether two variables matter the same
    - Variables must be the same units
    - $H_a: \beta_1 (\neq, <, \text{ or }>) \beta_2$

. . .

3. $H_0:$ ALL $\beta$'s $=0$
    - The "**Overall F-test"**
    - Testing against claim that regression model explains *NO* variation in $Y$

## Joint Hypothesis Tests: F-statistic

- The [F-statistic]{.hi-turquoise} is the test-statistic used to test joint hypotheses about regression coefficients with an [F-test]{.hi-turquoise}

. . .

- This involves comparing two models:
  1. [Unrestricted model]{.hi}: regression with all coefficients
  2. [Restricted model]{.hi-purple}: regression under null hypothesis (coefficients equal hypothesized values)

. . .

- $F$ is an [analysis of variance (ANOVA)]{.hi-turquoise}
  - essentially tests whether $R^2$ increases statistically significantly as we go from the restricted model$\rightarrow$unrestricted model

. . .

- $F$ has its own distribution, with *two* sets of degrees of freedom 

## Joint Hypothesis F-test: Example I

::: callout-tip
## Example
$$\widehat{\text{Wage}}_i=\hat{\beta_0}+\hat{\beta_1} \, \text{Male}_i+\hat{\beta_2}\text{Northeast}_i+\hat{\beta_3}\,\text{Midwest}_i+\hat{\beta_4}\,\text{South}_i$$

:::

. . .

- $H_0: \beta_2=\beta_3=\beta_4=0$

. . .

- $H_a$: $H_0$ is not true (at least one $\beta_i \neq 0$)

## Joint Hypothesis F-test: Example II

::: callout-tip
## Example
$$\widehat{\text{Wage}}_i=\hat{\beta_0}+\hat{\beta_1} \, \text{Male}_i+\hat{\beta_2}\text{Northeast}_i+\hat{\beta_3}\,\text{Midwest}_i+\hat{\beta_4}\,\text{South}_i$$

:::

- [Unrestricted model]{.hi}:

$$\widehat{\text{Wage}}_i=\hat{\beta_0}+\hat{\beta_1} \, \text{Male}_i+\hat{\beta_2}\text{Northeast}_i+\hat{\beta_3}\,\text{Midwest}_i+\hat{\beta_4}\,\text{South}_i$$


- [Restricted model]{.hi-purple}:

$$\widehat{\text{Wage}}_i=\hat{\beta_0}+\hat{\beta_1} \, \text{Male}_i$$

. . .

- $F$-test: **does going from [restricted]{.hi-purple} to [unrestricted]{.hi} model statistically significantly improve $R^2$?**

## Calculating the F-statistic

::: columns
::: {.column width="50%"}
$$F_{q,(n-k-1)}=\cfrac{\left(\displaystyle\frac{(R^2_u-R^2_r)}{q}\right)}{\left(\displaystyle\frac{(1-R^2_u)}{(n-k-1)}\right)}$$
:::

::: {.column width="50%"}

:::
:::

## Calculating the F-statistic

::: columns
::: {.column width="50%"}
$$F_{q,(n-k-1)}=\cfrac{\left(\displaystyle\frac{(\color{#e64173}{R^2_u}-R^2_r)}{q}\right)}{\left(\displaystyle\frac{(1-\color{#e64173}{R^2_u})}{(n-k-1)}\right)}$$
:::

::: {.column width="50%"}
- $\color{#e64173}{R^2_u}$: the $R^2$ from the [unrestricted model]{.hi} (all variables)

:::
:::

## Calculating the F-statistic

::: columns
::: {.column width="50%"}
$$F_{q,(n-k-1)}=\cfrac{\left(\displaystyle\frac{(\color{#e64173}{R^2_u}-\color{#6A5ACD}{R^2_r})}{q}\right)}{\left(\displaystyle\frac{(1-\color{#e64173}{R^2_u})}{(n-k-1)}\right)}$$
:::

::: {.column width="50%"}
- $\color{#e64173}{R^2_u}$: the $R^2$ from the [unrestricted model]{.hi} (all variables)


- $\color{#6A5ACD}{R^2_r}$: the $R^2$ from the [restricted model]{.hi-purple} (null hypothesis)
:::
:::

## Calculating the F-statistic

::: columns
::: {.column width="50%"}
$$F_{q,(n-k-1)}=\cfrac{\left(\displaystyle\frac{(\color{#e64173}{R^2_u}-\color{#6A5ACD}{R^2_r})}{q}\right)}{\left(\displaystyle\frac{(1-\color{#e64173}{R^2_u})}{(n-k-1)}\right)}$$
:::

::: {.column width="50%"}
- $\color{#e64173}{R^2_u}$: the $R^2$ from the [unrestricted model]{.hi} (all variables)

- $\color{#6A5ACD}{R^2_r}$: the $R^2$ from the [restricted model]{.hi-purple} (null hypothesis)

- $q$: number of restrictions (number of $\beta's=0$ under null hypothesis)

- $k$: number of $X$ variables in [unrestricted model]{.hi} (all variables)

- $F$ has two sets of degrees of freedom:
  - $q$ for the numerator, $(n-k-1)$ for the denominator

:::
:::

## Calculating the F-statistic

::: columns
::: {.column width="50%"}
$$F_{q,(n-k-1)}=\cfrac{\left(\displaystyle\frac{(R^2_u-R^2_r)}{q}\right)}{\left(\displaystyle\frac{(1-R^2_u)}{(n-k-1)}\right)}$$
:::

::: {.column width="50%"}
- [Key takeaway]{.hi-purple}: The bigger the difference between $(R^2_u-R^2_r)$, the greater the improvement in fit by adding variables, the larger the $F$!



:::
:::

## F-test Example I

- We'll use the `wooldridge` package's `wage1` data again

```{r}
#| echo: true

# load in data from wooldridge package
library(wooldridge)
wages <- wage1

# run regressions
unrestricted_reg <- lm(wage ~ female + northcen + west + south, data = wages)
restricted_reg <- lm(wage ~ female, data = wages)
```

## F-test Example II


- [Unrestricted model]{.hi}:

$$\widehat{\text{Wage}}_i=\hat{\beta_0}+\hat{\beta_1} \, \text{Male}_i+\hat{\beta_2}\text{Northeast}_i+\hat{\beta_3}\,\text{Midwest}_i+\hat{\beta_4}\,\text{South}_i$$


- [Restricted model]{.hi-purple}:

$$\widehat{\text{Wage}}_i=\hat{\beta_0}+\hat{\beta_1} \, \text{Male}_i$$

- $H_0: \beta_2 = \beta_3 = \beta_4 =0$

- $q = 3$ restrictions (F numerator df)

- $n-k-1 = 526-4-1=521$ (F denominator df)

## F-test Example III

- We can use the `car` package's `linearHypothesis()` command to run an $F$-test:
  - first argument: name of the (unrestricted) regression
  - second argument: vector of variable names (in quotes) you are testing

. . .

```{r}
#| echo: true
# load car package for additional regression tools
library(car) 
# F-test
linearHypothesis(unrestricted_reg, c("northcen", "west", "south")) 
```

. . .

- $p$-value on $F$-test $<0.05$, so we can reject $H_0$

## All F-test I {.smaller}

::: columns
::: {.column width="50%"}
```{r}
summary(unrestricted_reg)
```
:::

::: {.column width="50%"}
- Last line of regression output from `summary()` is an **All F-test**
  - $H_0:$ all $\beta's=0$ 
      - the regression explains no variation in $Y$
  - Calculates an `F-statistic` that, if high enough, is significant (`p-value` $<0.05)$ enough to reject $H_0$

:::
:::

## All F-test II {.smaller}

- Alternatively, if you use `broom` instead of `summary()`:
  - `glance()` command makes table of regression summary statistics
  - `tidy()` only shows coefficients

```{r}
#| echo: true
glance(unrestricted_reg)
```

- `statistic` is the All F-test, `p.value` next to it is the p-value from the F test