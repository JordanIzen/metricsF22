{
  "hash": "6daf3a738cf89da3d220f07a8862b63e",
  "result": {
    "markdown": "---\ntitle: \"2.3 — Simple Linear Regression — Appendix\"\nexecute:\n  freeze: auto\npage-layout: full\n---\n\n\n\n\n## Variance\n\nRecall the variance of a *discrete* random variable $X$, denoted $var(X)$ or $\\sigma^2$, is the expected value (probability-weighted average) of the squared deviations of $X_i$ from it's mean (or expected value) $\\bar{X}$ or $E(X)$.^[Note there will be a different in notation depending on whether we refer to a population (e.g. $\\mu_{X}$) or to a sample (e.g. $\\bar{X}$). As the overwhelming majority of cases we will deal with samples, I will use sample notation for means).]\n\n\\begin{align*}\n\\sigma_X^2&=E(X-E(X))\\\\\n&=\\sum^n_{i=1} (X_i-\\bar{X})^2 p_i\n\\end{align*}\n\nFpr *continuous* data (if all possible values of $X_i$ are equally likely or we don't know the probabilities), we can write variance as a simple average of squared deviations from the mean:\n\n\\begin{align*}\n\\sigma_X^2&=\\frac{1}{n}\\sum^n_{i=1}(X_i-\\bar{X})^2 \t\n\\end{align*}\n\nVariance has some useful properties:\n\n**Property 1**: The variance of a constant is 0\n\n\n$$var(c)=0 \\text{ iff } P(X=c)=1$$\n\n\nIf a random variable takes the same value (e.g. 2) with probability 1.00, $E(2)=2$, so the average squared deviation from the mean is 0, because there are never any values other than 2.\n\n**Property 2**: The variance is unchanged for a random variable plus/minus a constant\n\n\n$$var(X\\pm c)$$\n\n\nSince the variance of a constant is 0. \n\n**Property 3**: The variance of a scaled random variable is scaled by the square of the coefficient\n\n\n$$var(aX)=a^2var(X)$$\n\n\n**Property 4**: The variance of a linear transformation of a random variable is scaled by the square of the coefficient\n\n\n$$var(aX+b)=a^2var(X)$$\n\n\n## Covariance\n\nFor two random variables, $X$ and $Y$, we can measure their **covariance** (denoted $cov(X,Y)$ or $\\sigma_{X,Y}$)^[Again, to be technically correct, $\\sigma_{X,Y}$ refers to populations, $s_{X,Y}$ refers to samples, in line with population vs. sample variance and standard deviation. Recall also that sample estimates of variance and standard deviation divide by $n-1$, rather than $n$. In large sample sizes, this difference is negligible.] to quantify how they vary *together*. A good way to think about this is: when $X$ is above its mean, would we expect $Y$ to also be above its mean (and covary positively), or below its mean (and covary negatively). Remember, this is describing the *joint* probability distribution for two random variables. \n\n\\begin{align*}\n\\sigma_{X,Y}&=E\\big[(X-\\bar{X})(Y-\\bar{Y})\\big]\n\\end{align*}\n\nAgain, in the case of equally probable values for both $X$ and $Y$, covariance is sometimes written:\n\n\\begin{align*}\n\\sigma_{X,Y}&=\\frac{1}{N}\\sum_{i=1}^n(X-\\bar{X})(Y-\\bar{Y})\n\\end{align*}\n\nCovariance also has a number of useful properties:\n\n**Property 1**: The covariance of a random variable $X$ and a constant $c$ is 0\n\n\n$$cov(X,c)=0$$\n\n\n**Property 2**: The covariance of a random variable and itself is the variable's variance\n\n\\begin{align*}\n\tcov(X,X)&=var(X)\\\\\n\t\\sigma_{X,X}&=\\sigma^2_X\\\\\n\t\\end{align*}\n\n**Property 3**: The covariance of a two random variables $X$ and $Y$ each scaled by a constant $a$ and $b$ is the product of the covariance and the constants\n\n\n$$cov(aX,bY)=a\\times b \\times cov(X,Y)$$\n\n\n\n**Property 4**: If two random variables are independent, their covariance is 0\n\n\n$$cov(X,Y)=0 \\text{ iff } X \\text{ and } Y \\text{ are independent:}  E(XY)=E(X)\\times E(Y)$$\n\n\n## Correlation\n\nCovariance, like variance, is often cumbersome, and the numerical value of the covariance of two random variables does not really mean much. It is often convenient to normalize the covariance to a decimal between $-1$ and 1. We do this by dividing by the product of the standard deviations of $X$ and $Y$. This is known as the **correlation coefficient** between $X$ and $Y$, denoted $corr(X,Y)$ or $\\rho_{X,Y}$ (for populations) or $r_{X,Y}$ (for samples): \n\n\\begin{align*}\t\nr_{X,Y}&=\\frac{cov(X,Y)}{sd(X)sd(Y)}\\\\\n&=\\frac{E\\big[(X-\\bar{X})(Y-\\bar{Y})\\big]}{\\sqrt{E\\big[X-\\bar{X}\\big]}\\sqrt{E\\big[Y-\\bar{Y}\\big]}}\\\\\n&=\\frac{\\sigma_{X,Y}}{\\sigma_X \\sigma_Y}\\\\\n\\end{align*}\n\nNote this also means that covariance is the product of the standard deviation of $X$ and $Y$ and their correlation coefficient: \n\n\\begin{align*}\n\\sigma_{X,Y}&=r_{X,Y}\\sigma_X \\sigma_Y\t\\\\\ncov(X,Y)&=corr(X,Y)\\times sd(X) \\times sd(Y)\t\\\\\n\\end{align*}\n\nAnother way to reach the (sample) correlation coefficient is by finding the average joint $Z$-score of each pair of $(X_i,Y_i)$: \n\n\\begin{align*}\nr_{X,Y}&=\\frac{1}{n}\\frac{\\displaystyle\\sum^n_{i=1}(X_i-\\bar{X})(Y_i-\\bar{Y}))}{s_X s_Y} && \\text{Definition of sample correlation}\\\\\n&=\\frac{1}{n}\\displaystyle\\sum^n_{i=1}\\bigg(\\frac{X_i-\\bar{X}}{s_X}\\bigg)\\bigg(\\frac{Y_i-\\bar{Y}}{s_Y}\\bigg) && \\text{Breaking into separate sums} \\\\\n&=\\frac{1}{n}\\displaystyle\\sum^n_{i=1}(Z_X)(Z_Y) && \\text{Recognize each sum is the z-score for that r.v.} \\\\\n\\end{align*}\n\nCorrelation has some useful properties that should be familiar to you: \n\n- Correlation is between $-1$ and 1\n- A correlation of -1 is a downward sloping straight line\n- A correlation of 1 is an upward sloping straight line\n- A correlation of 0 implies no relationship\n\n\n### Calculating Correlation Example\n\nWe can calculate the correlation of a simple data set (of 4 observations) using `R` to show how correlation is calculated. We will use the $Z$-score method. Begin with a simple set of data in $(X_i, Y_i)$ points: \n\n\n$$ (1,1), (2,2), (3,4), (4,9) $$\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ncorr_example <- tibble(x = c(1,2,3,4),\n                       y = c(1,2,4,9))\n\nggplot(data = corr_example)+\n  aes(x = x,\n      y = y)+\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](2.3-appendix_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncorr_example %>%\n  summarize(mean_x = mean(x), #find mean of x, its 2.5\n            sd_x = sd(x), #find sd of x, its 1.291\n            mean_y = mean(y), #find mean of y, its 4\n            sd_y = sd(y)) #find sd of y, its 3.559\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  mean_x  sd_x mean_y  sd_y\n   <dbl> <dbl>  <dbl> <dbl>\n1    2.5  1.29      4  3.56\n```\n:::\n\n```{.r .cell-code}\n#take z score of x,y for each pair and multiply them\n\ncorr_example <- corr_example %>%\n  mutate(z_product = ((x - mean(x))/sd(x)) * ((y - mean(y))/sd(y)))\n\ncorr_example %>%\n  summarize(avg_z_product = sum(z_product)/(n() - 1), # average z products over n-1\n            actual_corr = cor(x,y), #compare our answer to actual cor() command!\n            covariance = cov(x,y)) # just for kicks, what's the covariance? \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  avg_z_product actual_corr covariance\n          <dbl>       <dbl>      <dbl>\n1         0.943       0.943       4.33\n```\n:::\n:::\n",
    "supporting": [
      "2.3-appendix_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}