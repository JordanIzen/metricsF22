{
  "hash": "2e5f5879f743569ca96f57666ce737ed",
  "result": {
    "markdown": "---\ntitle: \"2.4 — Goodness of Fit and Bias — Appendix\"\nexecute:\n  freeze: auto\npage-layout: full\n---\n\n\n\n\n## Deriving the OLS Estimators\n\nThe population linear regression model is: \n\n\n$$Y_i=\\beta_0+\\beta_1 X_i + u _i$$\n\n\nThe errors $(u_i)$ are unobserved, but for candidate values of $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$, we can obtain an estimate of the residual. Algebraically, the error is: \n\n\n$$\\hat{u_i}=    Y_i-\\hat{\\beta_0}-\\hat{\\beta_1}X_i$$\n\n\nRecall our goal is to find $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ that *minimizes* the sum of squared errors (SSE): \n\n\n$$SSE= \\sum^n_{i=1} \\hat{u_i}^2$$\n\n\nSo our minimization problem is:\n\n\n$$\\min_{\\hat{\\beta_0}, \\hat{\\beta_1}} \\sum^n_{i=1} (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1}X_i)^2$$\n\n\nUsing calculus, we take the partial derivatives and set it equal to 0 to find a minimum. The first order conditions are:\n\n\n$$\\begin{align*}\n\\frac{\\partial SSE}{\\partial \\hat{\\beta_0}}&=-2\\displaystyle\\sum^n_{i=1} (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1} X_i)=0\\\\\n\\frac{\\partial SSE}{\\partial \\hat{\\beta_1}}&=-2\\displaystyle\\sum^n_{i=1} (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1} X_i)X_i=0\\\\\n\\end{align*}$$\n\n### Finding $\\hat{\\beta_0}$\n\nWorking with the first FOC, divide both sides by $-2$: \n\n\n$$\\displaystyle\\sum^n_{i=1} (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1} X_i)=0$$\n\n\nThen expand the summation across all terms and divide by $n$: \n\n\n$$\\underbrace{\\frac{1}{n}\\sum^n_{i=1} Y_i}_{\\bar{Y}}-\\underbrace{\\frac{1}{n}\\sum^n_{i=1}\\hat{\\beta_0}}_{\\hat{\\beta_0}}-\\underbrace{\\frac{1}{n}\\sum^n_{i=1} \\hat{\\beta_1} X_i}_{\\hat{\\beta_1}\\bar{X}}=0$$\n\n\nNote the first term is $\\bar{Y}$, the second is $\\hat{\\beta_0}$, the third is $\\hat{\\beta_1}\\bar{X}$.^[From the [rules about summation operators](/resources/appendices/2.1-appendix/#the-summation-operator), we define the mean of a random variable $X$ as $\\bar{X}=\\frac{1}{n}\\displaystyle\\sum_{i=1}^n X_i$. The mean of a constant, like $\\beta_0$ or $\\beta_1$ is itself.]\n\nSo we can rewrite as: \n\n$$\\bar{Y}-\\hat{\\beta_0}-\\beta_1=0$$\n\n\nRearranging:\n\n\n$$\\hat{\\beta_0}=\\bar{Y}-\\bar{X}\\beta_1$$\n\n\n### Finding $\\hat{\\beta_1}$\n\nTo find $\\hat{\\beta_1}$, take the second FOC and divide by $-2$: \n\n\n$$\\displaystyle\\sum^n_{i=1} (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1} X_i)X_i=0$$\n\n\nFrom the formula for $\\hat{\\beta_0}$, substitute in for $\\hat{\\beta_0}$:\n\n\n$$\\displaystyle\\sum^n_{i=1} \\bigg(Y_i-[\\bar{Y}-\\hat{\\beta_1}\\bar{X}]-\\hat{\\beta_1} X_i\\bigg)X_i=0$$\n\n\nCombining similar terms:\n\n\n$$\\displaystyle\\sum^n_{i=1} \\bigg([Y_i-\\bar{Y}]-[X_i-\\bar{X}]\\hat{\\beta_1}\\bigg)X_i=0$$\n\n\nDistribute $X_i$ and expand terms into the subtraction of two sums (and pull out $\\hat{\\beta_1}$ as a constant in the second sum:\n\n\n$$\\displaystyle\\sum^n_{i=1} [Y_i-\\bar{Y}]X_i-\\hat{\\beta_1}\\displaystyle\\sum^n_{i=1}[X_i-\\bar{X}]X_i=0$$\n\n\nMove the second term to the righthand side: \n\n\n$$\\displaystyle\\sum^n_{i=1} [Y_i-\\bar{Y}]X_i=\\hat{\\beta_1}\\displaystyle\\sum^n_{i=1}[X_i-\\bar{X}]X_i$$\n\n\nDivide to keep just $\\hat{\\beta_1}$ on the right:\n\n\n$$\\frac{\\displaystyle\\sum^n_{i=1} [Y_i-\\bar{Y}]X_i}{\\displaystyle\\sum^n_{i=1}[X_i-\\bar{X}]X_i}=\\hat{\\beta_1}$$\n\n\nNote that from the [rules about summation operators](https://metricsf19.classes.ryansafner.com/class/07-class/#the-summation-operatorproperties): \n\n\n$$\\displaystyle\\sum^n_{i=1} [Y_i-\\bar{Y}]X_i=\\displaystyle\\sum^n_{i=1} (Y_i-\\bar{Y})(X_i-\\bar{X})$$\n\n\nand: \n\n\n$$\\displaystyle\\sum^n_{i=1} [X_i-\\bar{X}]X_i=\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})(X_i-\\bar{X})=\\displaystyle\\sum^n_{i=1}(X_i-\\bar{X})^2$$\n\n\nPlug in these two facts: \n\n\n$$\\frac{\\displaystyle\\sum^n_{i=1} (Y_i-\\bar{Y})(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1}(X_i-\\bar{X})^2}=\\hat{\\beta_1}$$\n\n\n## Algebraic Properties of OLS Estimators\n\nThe OLS residuals $\\hat{u}$ and predicted values $\\hat{Y}$ are chosen by the minimization problem to satisfy: \n\n1. The expected value (average) error is 0: \n\n$$E(u_i)=\\frac{1}{n}\\displaystyle \\sum_{i=1}^n \\hat{u_i}=0$$\n\n\n2. The covariance between $X$ and the errors is 0:\n\n$$\\hat{\\sigma}_{X,u}=0$$\n\n\nNote the first two properties imply strict **exogeneity**. That is, this is only a valid model if $X$ and $u$ are not correlated.\n\n3. The expected predicted value of $Y$ is equal to the expected value of $Y$:\n\n$$\\bar{\\hat{Y}}=\\frac{1}{n} \\displaystyle\\sum_{i=1}^n \\hat{Y_i} = \\bar{Y}$$\n\n\n4. Total sum of squares is equal to the explained sum of squares plus sum of squared errors:\n\n\\begin{align*}TSS&=ESS+SSE\\\\\n\\sum_{i=1}^n (Y_i-\\bar{Y})^2&=\\sum_{i=1}^n (\\hat{Y_i}-\\bar{Y})^2 + \\sum_{i=1}^n {u}^2\\\\\n\\end{align*}\n\nRecall $R^2$ is $\\frac{ESS}{TSS}$ or $1-SSE$\n\n5. The regression line passes through the point $(\\bar{X},\\bar{Y})$, i.e. the mean of $X$ and the mean of $Y$.\n\n## Bias in $\\hat{\\beta_1}$\n\nBegin with the formula we derived for $\\hat{\\beta_1}$:\n\n\n$$\\hat{\\beta_1}=\\frac{\\displaystyle \\sum^n_{i=1} (Y_i-\\bar{Y})(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}$$\n\n\nRecall from **Rule 6** of summations, we can rewrite the numerator as \n\n\\begin{align*}\n\t=&\\displaystyle \\sum^n_{i=1} (Y_i-\\bar{Y})(X_i-\\bar{X})\\\\\n\t=& \\displaystyle \\sum^n_{i=1} Y_i(X_i-\\bar{X})\\\\\n\\end{align*}\n\n\n$$\\hat{\\beta_1}=\\frac{\\displaystyle \\sum^n_{i=1} Y_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}$$\n\n\nWe know the true population relationship is expressed as:\n\n\n$$Y_i=\\beta_0+\\beta_1 X_i+u_i$$\n\n\nSubstituting this in for $Y_i$ in equation 2:\n\n\n$$\\hat{\\beta_1}=\\frac{\\displaystyle \\sum^n_{i=1} (\\beta_0+\\beta_1X_i+u_i)(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}$$\n\nBreaking apart the sums in the numerator:\n\n\n$$\\hat{\\beta_1}=\\frac{\\displaystyle \\sum^n_{i=1} \\beta_0(X_i-\\bar{X})+\\displaystyle \\sum^n_{i=1} \\beta_1X_i(X_i-\\bar{X})+\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}$$\n\n\nWe can simplify equation 4 using **Rules 4 and 5** of summations\n\n1. The first term in the numerator $\\left[\\displaystyle \\sum^n_{i=1} \\beta_0(X_i-\\bar{X})\\right]$ has the constant $\\beta_0$, which can be pulled out of the summation. This gives us the summation of deviations, which add up to 0 as per **Rule 4**:\n\n\\begin{align*}\n\\displaystyle \\sum^n_{i=1} \\beta_0(X_i-\\bar{X})&= \\beta_0 \\displaystyle \\sum^n_{i=1} (X_i-\\bar{X})\\\\\n&=\\beta_0 (0)\\\\\n&=0\\\\\n\\end{align*}\n\n2. The second term in the numerator  $\\left[\\displaystyle \\sum^n_{i=1} \\beta_1X_i(X_i-\\bar{X})\\right]$ has the constant $\\beta_1$, which can be pulled out of the summation. Additionally, **Rule 5** tells us $\\displaystyle \\sum^n_{i=1} X_i(X_i-\\bar{X})=\\displaystyle \\sum^n_{i=1}(X_i-\\bar{X})^2$:\n\n\\begin{align*}\n\\displaystyle \\sum^n_{i=1} \\beta_1X_1(X_i-\\bar{X})&= \\beta_1 \\displaystyle \\sum^n_{i=1} X_i(X_i-\\bar{X})\\\\\n&=\\beta_1\\displaystyle \\sum^n_{i=1}(X_i-\\bar{X})^2\\\\\n\\end{align*}\n\nWhen placed back in the context of being the numerator of a fraction, we can see this term simplifies to just $\\beta_1$:\n\n\\begin{align*}\n\t\\frac{\\beta_1\\displaystyle \\sum^n_{i=1}(X_i-\\bar{X})^2}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} &=\\frac{\\beta_1}{1} \\times \\frac{\\displaystyle \\sum^n_{i=1}(X_i-\\bar{X})^2}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}\\\\\n\t&=\\beta_1\t\\\\\n\\end{align*}\n\nThus, we are left with: \n\n\n$$\\hat{\\beta_1}=\\beta_1+\\frac{\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}$$\n\n\nNow, take the expectation of both sides:\n\n\n$$E[\\hat{\\beta_1}]=E\\left[\\beta_1+\\frac{\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} \\right]$$\n\n\nWe can break this up, using properties of expectations. First, recall $E[a+b]=E[a]+E[b]$, so we can break apart the two terms. \n\n\n$$E[\\hat{\\beta_1}]=E[\\beta_1]+E\\left[\\frac{\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} \\right]$$\n\n\nSecond, the true population value of $\\beta_1$ is a constant, so $E[\\beta_1]=\\beta_1$.\n\nThird, since we assume $X$ is also \"fixed\" and not random, the variance of $X$, $\\displaystyle\\sum_{i=1}^n (X_i-\\bar{X})$, in the denominator, is just a constant, and can be brought outside the expectation. \n\n\n$$E[\\hat{\\beta_1}]=\\beta_1+\\frac{E\\left[\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})\\right]\t}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}$$\n\n\nThus, the properties of the equation are primarily driven by the expectation $E\\bigg[\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})\\bigg]$. We now turn to this term. \n\nUse the property of summation operators to expand the numerator term: \n\n\\begin{align*}\n\t\\hat{\\beta_1}&=\\beta_1+\\frac{\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} \\\\\n\t\t\\hat{\\beta_1}&=\\beta_1+\\frac{\\displaystyle \\sum^n_{i=1} (u_i-\\bar{u})(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} \\\\\n\\end{align*}\n\nNow divide the numerator and denominator of the second term by $\\frac{1}{n}$. Realize this gives us the covariance between $X$ and $u$ in the numerator and variance of $X$ in the denominator, based on their respective definitions.\n\n\\begin{align*}\n\t\\hat{\\beta_1}&=\\beta_1+\\cfrac{\\frac{1}{n}\\displaystyle \\sum^n_{i=1} (u_i-\\bar{u})(X_i-\\bar{X})}{\\frac{1}{n}\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} \\\\\n\t\\hat{\\beta_1}&=\\beta_1+\\cfrac{cov(X,u)}{var(X)} \\\\\n\t\\hat{\\beta_1}&=\\beta_1+\\cfrac{s_{X,u}}{s_X^2} \\\\\n\\end{align*}\n\nBy the Zero Conditional Mean assumption of OLS, $s_{X,u}=0$. \n\nAlternatively, we can express the bias in terms of correlation instead of covariance:\n\n\n$$E[\\hat{\\beta_1}]=\\beta_1+\\cfrac{cov(X,u)}{var(X)}$$\n\n\nFrom the definition of correlation:\n\n\\begin{align*}\n\t cor(X,u)&=\\frac{cov(X,u)}{s_X s_u}\\\\\n\t cor(X,u)s_Xs_u &=cov(X,u)\\\\\n\\end{align*}\n\nPlugging this in: \n\n\\begin{align*}\nE[\\hat{\\beta_1}]&=\\beta_1+\\frac{cov(X,u)}{var(X)} \\\\\nE[\\hat{\\beta_1}]&=\\beta_1+\\frac{\\big[cor(X,u)s_xs_u\\big]}{s^2_X} \\\\\nE[\\hat{\\beta_1}]&=\\beta_1+\\frac{cor(X,u)s_u}{s_X} \\\\\nE[\\hat{\\beta_1}]&=\\beta_1+cor(X,u)\\frac{s_u}{s_X} \\\\\n\\end{align*}\n\n## Proof of the Unbiasedness of $\\hat{\\beta_1}$\n\nBegin with equation:^[Admittedly, this is a simplified version where $\\hat{\\beta_0}=0$, but there is no loss of generality in the results.]\n\n\n$$\\hat{\\beta_1}=\\frac{\\sum Y_iX_i}{\\sum X_i^2}$$\n\n\nSubstitute for $Y_i$:\n\n\n$$\\hat{\\beta_1}=\\frac{\\sum (\\beta_1 X_i+u_i)X_i}{\\sum X_i^2}$$\n\n\nDistribute $X_i$ in the numerator:\n\n\n$$\\hat{\\beta_1}=\\frac{\\sum \\beta_1 X_i^2+u_iX_i}{\\sum X_i^2}$$\n\n\nSeparate the sum into additive pieces:\n\n\n$$\\hat{\\beta_1}=\\frac{\\sum \\beta_1 X_i^2}{\\sum X_i^2}+\\frac{u_i X_i}{\\sum X_i^2}$$\n\n\n$\\beta_1$ is constant, so we can pull it out of the first sum:\n\n\n$$\\hat{\\beta_1}=\\beta_1 \\frac{\\sum X_i^2}{\\sum X_i^2}+\\frac{u_i X_i}{\\sum X_i^2}$$\n\n\nSimplifying the first term, we are left with:\n\n\n$$\\hat{\\beta_1}=\\beta_1 +\\frac{u_i X_i}{\\sum X_i^2}$$\n\n\nNow if we take expectations of both sides: \n\n\n$$E[\\hat{\\beta_1}]=E[\\beta_1] +E\\left[\\frac{u_i X_i}{\\sum X_i^2}\\right]$$\n\n\n$\\beta_1$ is a constant, so the expectation of $\\beta_1$ is itself. \n\n\n$$E[\\hat{\\beta_1}]=\\beta_1 +E\\bigg[\\frac{u_i X_i}{\\sum X_i^2}\\bigg]$$\n\n\nUsing the properties of expectations, we can pull out $\\frac{1}{\\sum X_i^2}$ as a constant:\n\n\n$$E[\\hat{\\beta_1}]=\\beta_1 +\\frac{1}{\\sum X_i^2} E\\bigg[\\sum u_i X_i\\bigg]$$\n\n\nAgain using the properties of expectations, we can put the expectation inside the summation operator (the expectation of a sum is the sum of expectations):\n\n\n$$E[\\hat{\\beta_1}]=\\beta_1 +\\frac{1}{\\sum X_i^2}\\sum E[u_i X_i]$$\n\n\nUnder the exogeneity condition, the correlation between $X_i$ and $u_i$ is 0.\n\n\n$$E[\\hat{\\beta_1}]=\\beta_1$$\n",
    "supporting": [
      "2.4-appendix_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}