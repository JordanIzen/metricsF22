[
  {
    "objectID": "assignments/00-preliminary-survey.html",
    "href": "assignments/00-preliminary-survey.html",
    "title": "Preliminary Survey on Statistics and Software",
    "section": "",
    "text": "This is an ungraded and anonymous survey for me to evaluate the distribution of your math and statistics backgrounds. Please complete all problems to the best of your ability. Your responses will help me craft the course to see which material we need to focus on at greater length, especially review material.\n\n\n Download PDF  Answer Key\n\n\nQuestion 1\nUsing the following sample data:\n\\[8, 12, 9, 10, 11, 5, 15\\]\n\nFind the median.\nCalculate the sample mean, \\(\\bar{x}\\)\nCalculate the sample standard deviation, \\(s\\)\n\n\n\nQuestion 2\nFor a fair, six-sided die:\n\nWhat is the probability of rolling a 5?\nWhat is the probability of rolling an even number?\nWhat is the probability of rolling an even number or a 3?\nIf you have two fair dice, what is the probability of rolling a 6 on both dice?\n\n\n\nQuestion 3\nHedge fund A earns an average rate of return of 2.5% per year with a standard deviation of 0.5%, while hedge fund B earns an average rate of return of 3.0% per year with a standard deviation of 2.0%. Which is more unusual, Hedge fund A earning a 4.0% return or hedge fund B earning a return -1.0% return? Why?1\n\n\nQuestion 4\nA discrete random variable \\(X\\) has the following pdf:\n\n\n\n \n  \n    x \n    p(x) \n  \n \n\n  \n    10 \n    0.1 \n  \n  \n    20 \n    0.2 \n  \n  \n    30 \n    0.3 \n  \n  \n    40 \n    0.4 \n  \n\n\n\n\nCalculate the sample standard deviation, \\(s\\) of \\(X\\).\n\n\nQuestion 5\nThe random variable \\(Y\\) is normally distributed with a mean of 50 and standard deviation of 12\n\\[Y \\sim N (50,12)\\]\n\nWhat is the \\(Z\\)-score for \\(Y=74\\)?\nIn your own words, what does this \\(Z\\)-score mean?\nWhat is the probability that \\(Y\\) takes on a value greater than 74?\n\n\n\nQuestion 6\nOn a scale of 1 (least) to 10 (most), how anxious are you about this class? Feel free to share any specific anxieties (they have a better chance to be specifically addressed if you do!).\n\n\nQuestion 7\nOn a scale of 1 (least) to 10 (most), how familiar would you say you are with computer programming and/or statistical software?\n\n\nQuestion 8\nList any statistical software packages (e.g. R, Microsoft Excel, Stata, SAS, SPSS, Minitab, etc.) and any programming languages (e.g. html, php, C/++, Python, LaTeX, etc.) you have had any experience with, and rate your proficiency between 1 (least) and 5 (most), if applicable.\n\n\n\n\n\nFootnotes\n\n\nHint: Standardize the two hedge funds.↩︎"
  },
  {
    "objectID": "assignments/01-exam.html",
    "href": "assignments/01-exam.html",
    "title": "Midterm Exam",
    "section": "",
    "text": "Warning\n\n\n\nThe midterm exam will be in class on Monday October 17."
  },
  {
    "objectID": "assignments/01-exam.html#exam-information",
    "href": "assignments/01-exam.html#exam-information",
    "title": "Midterm Exam",
    "section": "Exam Information",
    "text": "Exam Information\nYou may not have anything with you for the exam (no notes, etc) except a calculator. I write questions such that you can get a perfect score without a calculator, and answers should typically be simple whole numbers. However, I understand that if nothing else, calculators are moral support and you can use them. I will provide simple calculators if you need to borrow one, as well as extra paper.\nI write the exam so that most students can complete it in less than the required time, but you will have the whole class period, plus a few minutes between classes. Questions draw from concepts in the slides and whatever we discuss in class, no other outside knowledge is needed.\nYou must show your work for all problems. On all exam questions, I give points for partial credit. The more of your thought process you show (if you are unsure), the more points I am able to give. Both correct answers with no work shown, or blank answers will not receive full points.\nIf you have any approved testing accommodations, or know in advance you must be absent, please confirm with me ASAP and we will make arrangements\n\nStudy Tools\n\n Midterm Concepts\n Midterm Review Questions\n Old Midterm Exam (Answer Key)\n Formula Sheet"
  },
  {
    "objectID": "assignments/01-exam.html#my-advice",
    "href": "assignments/01-exam.html#my-advice",
    "title": "Midterm Exam",
    "section": "My Advice",
    "text": "My Advice\nMake sure you do all of the homework problems and learn from the answer keys to the homeworks, as well as the in-class practice problems. While some of the questions should be novel applications, conceptual questions on homeworks will get you in the right headspace to think about answering a question on an exam.\n\nThings Worth Knowing/Memorizing\n\nThe difference between exogenous and endogenous variables/models\nHow OLS estimators are chosen (minimize SSR)\nThe four assumptions made about the error term, and which one is most important, and why\nWhat \\(R^2\\) means, in English, and the methods of calculating it\nWhat \\(SER\\) means, in English\nWhat homoskedasticity and heteroskedasticity mean, in English\nHow to read a regression table and various forms of regression outputs from R\nInterpreting what \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are in terms of a graph and in terms of a question"
  },
  {
    "objectID": "assignments/01-problem-set.html",
    "href": "assignments/01-problem-set.html",
    "title": "Problem Set 1",
    "section": "",
    "text": "Warning\n\n\n\nThis assignment is due by class on Wednesday September 21.\nPlease read the instructions for completing and submitting homeworks.\nPDF  R Project  R Studio Cloud\nThe  PDF is useful if you want to print out the problem set and write on it. The  R Project is a zipped .zip file which contains a  .qmd file to write answers in, and the data, all in a logical working directory. (See this resource for help unzipping files). You can also just write an .R file in the project if you don’t want to use markdown. If you use the  cloud project, I have already installed tidyverse and tinytex (to produce pdfs)."
  },
  {
    "objectID": "assignments/01-problem-set.html#answers",
    "href": "assignments/01-problem-set.html#answers",
    "title": "Problem Set 1",
    "section": "Answers",
    "text": "Answers\n\n html   R Project  R Studio Cloud"
  },
  {
    "objectID": "assignments/01-problem-set.html#question-1",
    "href": "assignments/01-problem-set.html#question-1",
    "title": "Problem Set 1",
    "section": "Question 1",
    "text": "Question 1\n\nPart A\nWhat are the top 5 boys names for 2017, and what percent (note not the proportion!) of overall names is each?\n\n\nPart B\nWhat are the top 5 girls names for 2017, and what percent of overall names is each?"
  },
  {
    "objectID": "assignments/01-problem-set.html#question-2",
    "href": "assignments/01-problem-set.html#question-2",
    "title": "Problem Set 1",
    "section": "Question 2",
    "text": "Question 2\nMake two barplots of these top 5 names, one for each sex. Map aesthetics x to name and y to prop [or percent, if you made that variable, as I did.] and use geom_col (since you are declaring a specific y, otherwise you could just use geom_bar() and just an x.)"
  },
  {
    "objectID": "assignments/01-problem-set.html#question-3",
    "href": "assignments/01-problem-set.html#question-3",
    "title": "Problem Set 1",
    "section": "Question 3",
    "text": "Question 3\nFind your name. [If your name isn’t in there 😟, pick a random name.] count by sex how many babies since 1880 were named your name. [Hint: if you do only this, you’ll get the number of rows (years) there are in the data. You want to add the number of babies in each row (n), so inside count, add , wt = n to weight the count by n.] Also create a variable for the percent of each sex."
  },
  {
    "objectID": "assignments/01-problem-set.html#question-4",
    "href": "assignments/01-problem-set.html#question-4",
    "title": "Problem Set 1",
    "section": "Question 4",
    "text": "Question 4\nMake a line graph of the number of babies with your name over time, colored by sex."
  },
  {
    "objectID": "assignments/01-problem-set.html#question-5",
    "href": "assignments/01-problem-set.html#question-5",
    "title": "Problem Set 1",
    "section": "Question 5",
    "text": "Question 5\n\nPart A\nFind the most common name for boys by year between 1980-2017. [Hint: you’ll want to first group_by(year). Once you’ve got all the right conditions, you’ll get a table with a lot of data. You only want to slice(1) to keep just the 1st row of each grouped-year’s data.]\n\n\nPart B\nNow do the same for girls."
  },
  {
    "objectID": "assignments/01-problem-set.html#question-6",
    "href": "assignments/01-problem-set.html#question-6",
    "title": "Problem Set 1",
    "section": "Question 6",
    "text": "Question 6\nNow let’s graph the evolution of the most common names since 1880.\n\nPart A\nFirst, find out what are the top 10 overall most popular names for boys and for girls in the data. [Hint: first group_by(name).] You may want to create two objects, each with these top 5 names as character elements.\n\n\nPart B\nNow make two linegraphs of these 5 names over time, one for boys, and one for girls. [Hint: you’ll first want to subset the data to use for your data layer in the plot. First group_by(year) and also make sure you only use the names you found in Part A. Try using the %in% command to do this.]"
  },
  {
    "objectID": "assignments/01-problem-set.html#question-7",
    "href": "assignments/01-problem-set.html#question-7",
    "title": "Problem Set 1",
    "section": "Question 7",
    "text": "Question 7\nDownload these two datasets that I’ve cleaned up a bit: [If you want a challenge, try downloading them from the websites and cleaning them up yourself!]\n\n econ_freedom.csv\n pol_freedom.csv\n\nBelow is a brief description of the variables I’ve put in each dataset:\n\nEcon Freedom\n\n\n\nVariable\nDescription\n\n\n\n\nyear\nYear\n\n\nISO\nThree-letter country code\n\n\ncountry\nName of the country\n\n\nef_index\nTotal economic freedom index (0 - least to 100 - most)\n\n\nrank\nRank of the country in terms of economic freedom\n\n\ncontinent\nContinent the country is in\n\n\n\n\n\nPol Freedom\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ncountry\nName of the country\n\n\nC/T\nWhether the location is a country (C) or territory (T)\n\n\nyear\nYear\n\n\nstatus\nWhether the location is Free (F), Partly Free (F) or Not Free (NF)\n\n\nfh_score\nTotal political freedom index (0 - least to 100 - most)\n\n\n\nImport and save them each as an object using my_df_name <- read_csv(\"name_of_the_file.csv\"). I suggest one as econ and the other as pol, but it’s up to you. Look at each object you’ve created."
  },
  {
    "objectID": "assignments/01-problem-set.html#question-8",
    "href": "assignments/01-problem-set.html#question-8",
    "title": "Problem Set 1",
    "section": "Question 8",
    "text": "Question 8\nNow let’s join them together so that we can have a single dataset to work with. You can learn more about this in the 1.4 slides. Since both datasets have both country and year variables (spelled exactly the same in both!), we can use these two variables as a key to combine observations. Run the following code (substituting whatever you are naming your objects):\n\nfreedom <- left_join(econ, pol, # join pol tibble to econ tibble\n                     by = c(\"country\", \"year\")) # keys to match variables between two tibbles!\n\nTake a look at freedom to make sure it appears to have worked."
  },
  {
    "objectID": "assignments/01-problem-set.html#question-9",
    "href": "assignments/01-problem-set.html#question-9",
    "title": "Problem Set 1",
    "section": "Question 9",
    "text": "Question 9\n\nPart A\nMake a barplot of the 10 countries with the highest Economic Freedom index score in 2018. You may want to find this first and save it as an object to use for your plot’s data layer. Use geom_col() since we will map ef_index to y. If you want to order the bars, set x = fct_reorder(ISO, desc(ef_index)) to reorder ISO (or country, if you prefer) by EF score in descending order.\n\n\nPart B\nMake a barplot of the 10 countries with the highest Freedom House index score in 2018, similar to what you did for Part A."
  },
  {
    "objectID": "assignments/01-problem-set.html#question-10",
    "href": "assignments/01-problem-set.html#question-10",
    "title": "Problem Set 1",
    "section": "Question 10",
    "text": "Question 10\nNow make a scatterplot of Political freedom (fh_score as y) on Economic Freedom (ef_index as x) in the year 2018, and color by continent."
  },
  {
    "objectID": "assignments/01-problem-set.html#question-11",
    "href": "assignments/01-problem-set.html#question-11",
    "title": "Problem Set 1",
    "section": "Question 11",
    "text": "Question 11\nSave your plot from Question 10 as an object, and add a new layer where we will highlight a few countries. Pick a few countries (I suggest using the ISO code) and create a new object filtering the data to only include these countries (again the %in% command will be most helpful here).\n\nAdditionally, install and load a package called \"ggrepel\", which will adjust labels so they do not overlap on a plot.\n\n\nThen, add the following layer to your plot:\n\n\ngeom_label_repel(data = countries, # or whatever object name you created\n                     aes(x = ef_index,\n                         y = fh_score,\n                         label = ISO, # show ISO as label (you could do country instead)\n                         color = continent),\n                     alpha = 0.5, # make it a bit transparent\n                     box.padding = 0.75, # control how far labels are from points\n                     show.legend = F) # don't want this to add to the legend\n\nThis should highlight these countries on your plot."
  },
  {
    "objectID": "assignments/01-problem-set.html#question-12",
    "href": "assignments/01-problem-set.html#question-12",
    "title": "Problem Set 1",
    "section": "Question 12",
    "text": "Question 12\nLet’s just look only at the United States and see how it has fared in both measures of freedom over time. filter() the data to look only at the United States (its ISO is \"USA\"). Use both a geom_point() layer and a geom_path() layer, which will connect the dots over time. Let’s also see this by labeling the years with an additional layer geom_text_repel(aes(label = year))."
  },
  {
    "objectID": "assignments/02-problem-set.html",
    "href": "assignments/02-problem-set.html",
    "title": "Problem Set 2",
    "section": "",
    "text": "Warning\n\n\n\nThis assignment is due by class on Wednesday September 28.\nPlease read the instructions for completing and submitting homeworks.\nPDF  R Project  R Studio Cloud\nThe  PDF is useful if you want to print out the problem set and write on it. The  R Project is a zipped .zip file which contains a  .qmd file to write answers in, and the data, all in a logical working directory. (See this resource for help unzipping files). You can also just write an .R file in the project if you don’t want to use markdown. If you use the  cloud project, I have already installed tidyverse and tinytex (to produce pdfs)."
  },
  {
    "objectID": "assignments/02-problem-set.html#answers",
    "href": "assignments/02-problem-set.html#answers",
    "title": "Problem Set 2",
    "section": "Answers",
    "text": "Answers\n\n html   R Project  R Studio Cloud"
  },
  {
    "objectID": "assignments/02-problem-set.html#question-1",
    "href": "assignments/02-problem-set.html#question-1",
    "title": "Problem Set 2",
    "section": "Question 1",
    "text": "Question 1\nIn your own words, explain the difference between endogeneity and exogeneity."
  },
  {
    "objectID": "assignments/02-problem-set.html#question-2",
    "href": "assignments/02-problem-set.html#question-2",
    "title": "Problem Set 2",
    "section": "Question 2",
    "text": "Question 2\n\nPart A\nIn your own words, explain what (sample) standard deviation means.\n\n\nPart B\nIn your own words, explain how (sample) standard deviation is calculated. You may also write the formula, but it is not necessary."
  },
  {
    "objectID": "assignments/02-problem-set.html#question-3",
    "href": "assignments/02-problem-set.html#question-3",
    "title": "Problem Set 2",
    "section": "Question 3",
    "text": "Question 3\nSuppose you have a very small class of four students that all take a quiz. Their scores are reported as follows:\n\\[\\{83, 92, 72, 81\\}\\]\n\nPart A\nCalculate the median.\n\n\nPart B\nCalculate the sample mean, \\(\\bar{x}\\).\n\n\nPart C\nCalculate the sample standard deviation, \\(s\\).\n\n\nPart D\nMake or sketch a rough histogram of this data, with the size of each bin being 10 (i.e. 70’s, 80’s, 90’s, 100’s). You can draw this by hand or use R.\nIf you are using ggplot, you want to use + geom_histogram(breaks = seq(start,end,by)) and add another layer + scale_x_continuous(breaks=seq(start,end,by)) . The first layer creates bins in the histogram, and the second layer creates ticks on the x axis; both by creating a sequence starting at some starting value, some ending value, by a certain interval (e.g. by 2, or by 10).\n\nIs this distribution roughly symmetric or skewed? What would we expect about the mean and the median?\n\n\nPart E\nSuppose instead the person who got the 72 did not show up that day to class, and got a 0 instead. Recalculate the mean and median. What happened and why?"
  },
  {
    "objectID": "assignments/02-problem-set.html#question-4",
    "href": "assignments/02-problem-set.html#question-4",
    "title": "Problem Set 2",
    "section": "Question 4",
    "text": "Question 4\nSuppose the probabilities of a visitor to Amazon’s website buying 0, 1, or 2 books are 0.2, 0.4, and 0.4 respectively.\n\nPart A\nCalculate the expected number of books a visitor will purchase.\n\n\nPart B\nCalculate the standard deviation of book purchases.\n\n\nPart C\nBonus: try doing this in R by making an initial dataframe of the data, and then making new columns to the “table” like we did in class."
  },
  {
    "objectID": "assignments/02-problem-set.html#question-5",
    "href": "assignments/02-problem-set.html#question-5",
    "title": "Problem Set 2",
    "section": "Question 5",
    "text": "Question 5\nScores on the SAT (out of 1600) are approximately normally distributed with a mean of 500 and standard deviation of 100.\n\nPart A\nWhat is the probability of getting a score between a 400 and a 600?\n\n\nPart B\nWhat is the probability of getting a score between a 300 and a 700?\n\n\nPart C\nWhat is the probability of getting at least a 700?\n\n\nPart D\nWhat is the probability of getting at most a 700?\n\n\nPart E\nWhat is the probability of getting exactly a 500?"
  },
  {
    "objectID": "assignments/02-problem-set.html#question-6",
    "href": "assignments/02-problem-set.html#question-6",
    "title": "Problem Set 2",
    "section": "Question 6",
    "text": "Question 6\nRedo problem 5 by using the pnorm() command in R.\nHint: This function has four arguments:\n\nthe value of the random variable\nthe mean of the distribution\nthe sd of the distribution\nlower.tail TRUE or FALSE."
  },
  {
    "objectID": "assignments/03-problem-set.html",
    "href": "assignments/03-problem-set.html",
    "title": "Problem Set 3",
    "section": "",
    "text": "Warning\n\n\n\nThis assignment is due by Friday October 14 on Blackboard Assignments.\nPlease read the instructions for completing and submitting homeworks.\nPDF  R Project  R Studio Cloud\nThe  PDF is useful if you want to print out the problem set and write on it. The  R Project is a zipped .zip file which contains a  .qmd file to write answers in, and the data, all in a logical working directory. (See this resource for help unzipping files). You can also just write an .R file in the project if you don’t want to use markdown. If you use the  cloud project, I have already installed tidyverse and tinytex (to produce pdfs)."
  },
  {
    "objectID": "assignments/03-problem-set.html#question-1",
    "href": "assignments/03-problem-set.html#question-1",
    "title": "Problem Set 3",
    "section": "Question 1",
    "text": "Question 1\nIn your own words, describe what exogeneity and endogeneity mean, and how they are related to bias in our regression. What things can we learn about the bias if we know \\(X\\) is endogenous?"
  },
  {
    "objectID": "assignments/03-problem-set.html#question-2",
    "href": "assignments/03-problem-set.html#question-2",
    "title": "Problem Set 3",
    "section": "Question 2",
    "text": "Question 2\nIn your own words, describe what \\(R^2\\) means. How do we calculate it, what does it tell us, and how do we interpret it?"
  },
  {
    "objectID": "assignments/03-problem-set.html#question-3",
    "href": "assignments/03-problem-set.html#question-3",
    "title": "Problem Set 3",
    "section": "Question 3",
    "text": "Question 3\nIn your own words, describe what the standard error of the regression (\\(SER\\)) means. How do we calculate it, what does it tell us, and how do we interpret it?"
  },
  {
    "objectID": "assignments/03-problem-set.html#question-4",
    "href": "assignments/03-problem-set.html#question-4",
    "title": "Problem Set 3",
    "section": "Question 4",
    "text": "Question 4\nIn your own words, describe what homoskedasticity and heteroskedasticity mean: both in ordinary English, and in terms of the graph of the OLS regression line."
  },
  {
    "objectID": "assignments/03-problem-set.html#question-5",
    "href": "assignments/03-problem-set.html#question-5",
    "title": "Problem Set 3",
    "section": "Question 5",
    "text": "Question 5\nIn your own words, describe what the variation in \\(\\hat{\\beta_1}\\) (either variance or standard error) means, or is measuring. What three things determine the variation, and in what way?"
  },
  {
    "objectID": "assignments/03-problem-set.html#question-6",
    "href": "assignments/03-problem-set.html#question-6",
    "title": "Problem Set 3",
    "section": "Question 6",
    "text": "Question 6\nIn your own words, describe what a p-value means, and how it is used to establish statistical significance."
  },
  {
    "objectID": "assignments/03-problem-set.html#question-7",
    "href": "assignments/03-problem-set.html#question-7",
    "title": "Problem Set 3",
    "section": "Question 7",
    "text": "Question 7\nA researcher is interested in examining the impact of illegal music downloads on commercial music sales. The author collects data on commercial sales of the top 500 singles from 2017 (Y) and the number of downloads from a web site that allows ‘file sharing’ (X). The author estimates the following model:\n\\[\n\\text{music sales}_i = \\beta_0+\\beta_1 \\text{illegal downloads}_i + u_i\n\\]\nThe author finds a large, positive, and statistically significant estimate of \\(\\hat{\\beta_1}\\). The author concludes these results demonstrate that illegal downloads actually boost music sales. Is this an unbiased estimate of the impact of illegal music on sales? Why or why not? Do you expect the estimate to overstate or understate the true relationship between illegal downloads and sales?"
  },
  {
    "objectID": "assignments/03-problem-set.html#question-8",
    "href": "assignments/03-problem-set.html#question-8",
    "title": "Problem Set 3",
    "section": "Question 8",
    "text": "Question 8\nA researcher wants to estimate the relationship between average weekly earnings (\\(AWE\\), measured in dollars) and \\(Age\\) (measured in years) using a simple OLS model. Using a random sample of college-educated full-time workers aged 25-65 yields the following:\n\\[\n\\widehat{AWE} = 696.70+9.60 \\, Age\n\\]\n\nPart A\nInterpret what \\(\\hat{\\beta_0}\\) means in this context.\n\n\nPart B\nInterpret what \\(\\hat{\\beta_1}\\) means in this context.\n\n\nPart C\nThe \\(R^2=0.023\\) for this regression. What are the units of the \\(R^2\\), and what does this mean?\n\n\nPart D\nThe \\(SER, \\, \\hat{\\sigma_u}=624.1\\) for this regression. What are the units of the SER in this context, and what does it mean? Is the SER large in the context of this regression?\n\n\nPart E\nSuppose Maria is 20 years old. What is her predicted \\(\\widehat{AWE}\\)?\n\n\nPart F\nSuppose the data shows her actual \\(AWE\\) is $430. What is her residual? Is this a relatively good or a bad prediction? Hint: compare your answer here to your answer in Part D.\n\n\nPart G\nWhat does the error term, \\(u_i\\) represent in this case? What might individuals have different values of \\(\\hat{u}_i\\)?\n\n\nPart H\nDo you think that \\(Age\\) is exogenous? Why or why not? Would we expect \\(\\hat{\\beta_1}\\) to be too large or too small?"
  },
  {
    "objectID": "assignments/03-problem-set.html#question-9",
    "href": "assignments/03-problem-set.html#question-9",
    "title": "Problem Set 3",
    "section": "Question 9",
    "text": "Question 9\nSuppose a researcher is interested in estimating a simple linear regression model:\n\\[\nY_i=\\beta_0+\\beta_1X_i+u_i\n\\]\nIn a sample of 48 observations, she generates the following descriptive statistics:\n\n\\(\\bar{X}=30\\)\n\\(\\bar{Y}=63\\)\n\\(\\displaystyle\\sum^n_{i=1}(X_i-\\bar{X})^2= 6900\\)\n\\(\\displaystyle\\sum^n_{i=1}(Y_i-\\bar{Y})^2= 29000\\)\n\\(\\displaystyle\\sum^n_{i=1}(X_i-\\bar{X})(Y_i-\\bar{Y})=13800\\)\n\\(\\displaystyle\\sum^n_{i=1}\\hat{u}^2=1656\\)\n\n\nPart A\nWhat is the OLS estimate of \\(\\hat{\\beta_1}\\)?\n\n\nPart B\nWhat is the OLS estimate of \\(\\hat{\\beta_0}\\)?\n\n\nPart C\nSuppose the OLS estimate of \\(\\hat{\\beta_1}\\) has a standard error of \\(0.072\\). Could we probably reject a null hypothesis of \\(H_0: \\beta_1=0\\) at the 5% level?\n\n\nPart D\nCalculate the \\(R^2\\) for this model. How much variation in \\(Y\\) is explained by our model?"
  },
  {
    "objectID": "assignments/04-problem-set.html",
    "href": "assignments/04-problem-set.html",
    "title": "Problem Set 4",
    "section": "",
    "text": "Warning\n\n\n\nThis assignment is due by Friday november 11 by email.\nPlease read the instructions for completing and submitting homeworks.\nPDF  R Project  R Studio Cloud\nThe  PDF is useful if you want to print out the problem set and write on it. The  R Project is a zipped .zip file which contains a  .qmd file to write answers in, and the data, all in a logical working directory. (See this resource for help unzipping files). You can also just write an .R file in the project if you don’t want to use markdown. If you use the  cloud project, I have already installed tidyverse and tinytex (to produce pdfs)."
  },
  {
    "objectID": "assignments/04-problem-set.html#question-1",
    "href": "assignments/04-problem-set.html#question-1",
    "title": "Problem Set 4",
    "section": "Question 1",
    "text": "Question 1\nIn your own words, explain the fundamental problem of causal inference."
  },
  {
    "objectID": "assignments/04-problem-set.html#question-2",
    "href": "assignments/04-problem-set.html#question-2",
    "title": "Problem Set 4",
    "section": "Question 2",
    "text": "Question 2\nIn your own words, explain how properly conducting a randomized controlled trial helps to identify the causal effect of one variable on another."
  },
  {
    "objectID": "assignments/04-problem-set.html#question-3",
    "href": "assignments/04-problem-set.html#question-3",
    "title": "Problem Set 4",
    "section": "Question 3",
    "text": "Question 3\nIn your own words, describe what omitted variable bias means. What are the two conditions for a variable to bias OLS estimates if omitted?"
  },
  {
    "objectID": "assignments/04-problem-set.html#question-4",
    "href": "assignments/04-problem-set.html#question-4",
    "title": "Problem Set 4",
    "section": "Question 4",
    "text": "Question 4\nIn your own words, describe what multicollinearity means. What is the cause, and what are the consequences of multicollinearity? How can we measure multicollinearity and its effects? What happens if multicollinearity is perfect?"
  },
  {
    "objectID": "assignments/04-problem-set.html#question-5",
    "href": "assignments/04-problem-set.html#question-5",
    "title": "Problem Set 4",
    "section": "Question 5",
    "text": "Question 5\nExplain how we use Directed Acyclic Graphs (DAGs) to depict a causal model: what are the two criteria that must hold for identifying a causal effect of \\(X\\) on \\(Y\\)? When should we control a variable, and when should we not control for a variable?"
  },
  {
    "objectID": "assignments/04-problem-set.html#question-6",
    "href": "assignments/04-problem-set.html#question-6",
    "title": "Problem Set 4",
    "section": "Question 6",
    "text": "Question 6\nA pharmaceutical company is interested in estimating the impact of a new drug on cholesterol levels. They enroll 200 people in a clinical trial. People are randomly assigned the treatment group or into the control group. Half of the people are given the new drug and half the people are given a sugar pill with no active ingredient. To examine the impact of dosage on reductions in cholesterol levels, the authors of the study regress the following model:\n\\[\\text{cholesterol level}_i = \\beta_0+\\beta_1 \\text{dosage level}_i + u_i\\]\nFor people in the control group, dosage level\\(_i=0\\) and for people in the treatment group, dosage level\\(_i\\) measures milligrams of the active ingredient. In this case, the authors find a large, negative, statistically significant estimate of \\(\\hat{\\beta_1}\\). Is this an unbiased estimate of the impact of dosage on change in cholesterol level? Why or why not? Do you expect the estimate to overstate or understate the true relationship between dosage and cholesterol level?"
  },
  {
    "objectID": "assignments/04-problem-set.html#question-7",
    "href": "assignments/04-problem-set.html#question-7",
    "title": "Problem Set 4",
    "section": "Question 7",
    "text": "Question 7\nData were collected from a random sample of 220 home sales from a community in 2017.\n\\[\\widehat{Price}=119.2+0.485 \\, BDR+23.4 \\, Bath+0.156 \\, Hsize+0.002 \\, Lsize+0.090 \\, Age\\]\n\n\n\nVariable\nDescription\n\n\n\n\n\\(Price\\)\nselling price (in $1,000s)\n\n\n\\(BDR\\)\nnumber of bedrooms\n\n\n\\(Bath\\)\nnumber of bathrooms\n\n\n\\(Hsize\\)\nsize of the house (in ft\\(^2)\\)\n\n\n\\(Lsize\\)\nlot size (in ft\\(^2)\\)\n\n\n\\(Age\\)\nage of the house (in years)\n\n\n\n\nPart A\nSuppose that a homeowner converts part of an existing living space in her house to a new bathroom. What is the expected increase in the value of the house?\n\n\nPart B\nSuppose a homeowner adds a new bathroom to her house, which also increases the size of the house by 100 square feet. What is the expected increase in the value of the house?\n\n\nPart C\nSuppose the \\(R^2\\) of this regression is 0.727. Calculate the adjusted \\(\\bar{R}^2\\).\n\n\nPart D\nSuppose the following auxiliary regression for \\(BDR\\) has an \\(R^2\\) of 0.841.\n\\[\\widehat{BDR}=\\delta_0+\\delta_1Bath+\\delta_2Hsize+\\delta_3Lsize+\\delta_4Age\\]\nCalculate the Variance Inflation Factor for \\(BDR\\) and explain what it means."
  },
  {
    "objectID": "assignments/04-problem-set.html#question-8",
    "href": "assignments/04-problem-set.html#question-8",
    "title": "Problem Set 4",
    "section": "Question 8",
    "text": "Question 8\nA researcher wants to investigate the effect of education on average hourly wages. Wage, education, and experience in the dataset have the following correlations:\n\n\n\n\nWage\nEducation\nExperience\n\n\n\n\nWage\n1.0000\n\n\n\n\nEducation\n0.4059\n1.0000\n\n\n\nExperience\n0.1129\n-0.2995\n1.0000\n\n\n\nShe runs a simple regression first, and gets the results:\n\\[\\widehat{\\text{Wage}} = -0.9049 +  0.5414 \\, Education\\]\nShe runs another regression, and gets the results:\n\\[\\widehat{\\text{Experience}} = 35.4615 - 1.4681 \\, Education\\]\n\nPart A\nIf the true marginal effect of experience on wages (holding education constant) is 0.0701, calculate the omitted variable bias in the first regression caused by omitting experience. Does the estimate of \\(\\hat{\\beta_1}\\) in the first regression overstate or understate the effect of education on wages?\n\n\nPart B\nKnowing this, what would be the true effect of education on wages, holding experience constant?\n\n\nPart C\nThe \\(R^2\\) for the second regression is 0.0897. If she were to run a better regression including both education and experience, how much would the variance of the coefficients on education and experience increase? Why?"
  },
  {
    "objectID": "assignments/04-problem-set.html#question-9",
    "href": "assignments/04-problem-set.html#question-9",
    "title": "Problem Set 4",
    "section": "Question 9",
    "text": "Question 9\n\n heightwages.csv\n\nDownload and read in heightwages.csv dataset. If you don’t want to download/upload it, you can read it in directly from the url by running this chunk:\n\n# run or edit this chunk\nmlb <- read_csv(\"http://metricsf22.classes.ryansafner.com/files/data/heightwages.csv\")\n\nThis data is a part of a larger dataset from the National Longitudinal Survey of Youth (NLSY) 1979 cohort: a nationally representative sample of 12,686 men and women aged 14-22 years old when they were first surveyed in 1979. They were subsequently interviewed every year through 1994 and then every other year afterwards. There are many included variables, but for now we will just focus on:\n\n\n\nVariable\nDescription\n\n\n\n\nwage96\nAdult hourly wages ($/hr) reported in 1996\n\n\nheight85\nAdult height (inches) reported in 1985\n\n\nheight81\nAdolescent height (inches) reported in 1981\n\n\n\n\nWe want to figure out what is the effect of height on wages (e.g. do taller people earn more on average than shorter people?)\n\n\nPart A\nCreate a quick scatterplot between height85 (as \\(X)\\) amd wage96 (as \\(Y)\\).\n\n\nPart B\nRegress wages on adult height. Write the equation of the estimated OLS regression. Interpret the coefficient on height85.\n\n\nPart C\nHow much would someone who is 5’10” (70 in) be predicted to earn per hour, according to the model?\n\n\nPart D\nWould adolescent height cause an omitted variable bias if it were left out? Explain using both your intuition, and some statistical evidence with R.\n\n\nPart E\nNow add adolescent height to the regression, and write the new regression equation below, as before. Interpret the coefficient on height85.\n\n\nPart F\nHow much would someone who is 5’10” in 1985 and 4’8” in 1981 be predicted to earn, according to the model?\n\n\nPart G\nWhat happened to the estimate on height85 and its standard error?\n\n\nPart H\nIs there multicollinearity between height85 and height81? Explore with a scatterplot. Hint: to avoid overplotting, use geom_jitter() instead of geom_point() to get a better view of the data.\n\n\nPart I\nQuantify how much multicollinearity affects the variance of the OLS estimates on both heights. Hint: You’ll need the vif command from the car package.\n\n\nPart J\nReach the same number as in part I by running an auxiliary regression.\nHint: There’s some missing wage96 data that may give you a different answer, so take the data and filter(!is.na(wage96)) before running this regression — this will include only observations for wage96 that are not NA’s.\n\n\nPart K\nMake a regression table from part B and D using modelsummary."
  },
  {
    "objectID": "assignments/04-problem-set.html#question-10",
    "href": "assignments/04-problem-set.html#question-10",
    "title": "Problem Set 4",
    "section": "Question 10",
    "text": "Question 10\nInstall and load the wooldridge package. This package contains datasets used in Jeffrey Wooldridge’s Introductory Econometrics: A Modern Approach (the textbook that I used in my econometrics classes years ago!).\nWe will use the bwght data from wooldridge, which comes from The 1988 National Health Interview Survey., used in J. Mullahy (1997), “Instrumental-Variable Estimation of Count Data Models: Applications to Models of Cigarette Smoking Behavior,” Review of Economics and Statistics 79: 596-593.\nLet’s just look at the following variables:\n\n\n\nVariable\nDescription\n\n\n\n\nbwght\nBirth Weight (ounces)\n\n\ncigs\nCigarettes smoked per day while pregnant (1988)\n\n\nmotheduc\nMother’s education (number of years)\n\n\ncigprice\nPrice of cigarette pack (1988)\n\n\nfaminc\nFamily’s income in $1,000s (1988)\n\n\n\n\nWe want to explore how a mother smoking during pregnancy affects the baby’s birthweight (which may have strong effects on outcomes over the child’s life).\n\nJust to be explicit about it, assign this as some dataframe (feel free to change the name), i.e.:\n\n# run or edit this chunk\nbirths <- bwght # feel free to rename whatever you want for the dataframe\n\n\nPart A\nMake a correlation table for our variables listed above.\nHint: select() these variables and then pipe this into cor(., use = \"pairwise.complete.obs\") to use only observations for which there are data on each variable (to avoid NA’s).\n\n\nPart B\nConsider the following causal model:\n\ndagify(bwght ~ cigs + inc,\n       cigs ~ price + educ + inc,\n       inc ~ educ,\n       exposure = \"cigs\",\n       outcome = \"bwght\") %>%\n  tidy_dagitty(seed = 256) %>%\n  ggdag_status()+\n  theme_dag_blank()+\n  theme(legend.position = \"none\")\n\n\n\n\nNote what we are hypothesizing:\n\nbwght is caused by cigs and inc\ncigs are caused by price, educ, and inc\ninc is caused by educ\n\nSee also how this is written into the notation in R to draw (plot) the DAG.\nCreate this model on dagitty.net. What does dagitty tell us the testable implications of this causal model?\nYou can answer this using dagitty.net, and/or R.\n\n\nPart C\nTest each implication given to you by dagitty.\n\nFor independencies, e.g. \\((x \\perp y)\\): run a regression of \\(y\\) on \\(x\\).\nFor conditional independencies, e.g. \\((x \\perp y | z, a)\\): run a regression of \\(y\\) on \\(x, z, a\\).\n\nFor each, test against the null hypothesis that the relevant coefficient \\((\\beta_1) =0\\) (i.e. \\(x\\) and \\(y\\) are indeed independent).\nDoes this causal model hold up well?\n\n\nPart D\nList all of the possible pathways from cigs to bwght. Which are “front-doors” and which are “back-doors?” Are any blocked by colliders?\nYou can answer this using dagitty.net, and/or R.\n\n\nPart E\nWhat is the minimal sufficient set of variables we need to control for in order to causally identify the effect of cigs on bwght?\nYou can answer this using dagitty.net, and/or R.\n\n\nPart F\nEstimate the causal effect by running the appropriate regression in R.\nFYI, on dagitty.net, you can change a variable on the diagram to be “adjusted” (controlled for) by clicking it and then hitting the A key.\n\n\nPart G\nWe saw some effect between faminc and cigprice. Perhaps there are unobserved factors (such as the economy’s performance) that affect both. Add an unobserved factor u1 to your dagitty model.\nFYI, on dagitty.net, you can make a variable be “unobserved” by double-clicking it and then hitting the U key.\n\n\nPart H\nPerhaps our model is poorly specified. Maybe motheduc actually has a causal effect on bwght? Tweak your model on dagitty.net to add this potential relationship. What testable implications does this new model imply?\n\n\nPart I\nTest these implications appropriately in R, like you did in Part C. Does this model hold up well?\n\n\nPart J\nUnder this new causal model, list all of the possible pathways from cigs to bwght. Which are “front-doors” and which are “back-doors?” Are any blocked by colliders?\nYou can answer this using dagitty.net, and/or R.\n\n\nPart K\nUnder this new causal model, what is the minimal sufficient set of variables we need to control in order to causally identify the effect of cigs on bwght?\nYou can answer this using dagitty.net, and/or R.\n\n\nPart L\nEstimate the causal effect in this new model by running the appropriate regression in R. Compare your answers to those in part F.\n\n\nPart M\nTry out drawing this model using the ggdag package in R. See my DAG in question 3 for an example."
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignment Details",
    "section": "",
    "text": "On this page, you can find more information about each individual assignment, as well as the assignments themselves.\nUse this tool to help calculate your overall course grade using existing assignment grades, as well as forecast “what if” scenarios.\nPlease refer to the syllabus for more information about grades."
  },
  {
    "objectID": "assignments/index.html#homeworks",
    "href": "assignments/index.html#homeworks",
    "title": "Assignment Details",
    "section": "Homeworks",
    "text": "Homeworks\nThere will be several problem sets (one at the end of each lesson). Problem sets will be a combination of math/statistical theory & application problems and problems that require use of R with real data. You may collaborate with other students to work on problem sets, but each person must turn in an individual problem set. Problem sets are due one week from the class period where we finish a lesson, and must emailed to me by the start of class (so please type or, if you must, hand write and scan them)."
  },
  {
    "objectID": "assignments/index.html#exams",
    "href": "assignments/index.html#exams",
    "title": "Assignment Details",
    "section": "Exams",
    "text": "Exams\nThis class will have two (2) exams."
  },
  {
    "objectID": "assignments/index.html#midterm-exam",
    "href": "assignments/index.html#midterm-exam",
    "title": "Assignment Details",
    "section": "Midterm Exam",
    "text": "Midterm Exam\nAfter we have finished linear regression, there will be a midterm constituting a combination of multiple choice, problem, and short answer questions. This will cover the content we discuss in class, my lectures, and the readings. The midterm provides feedback both to you and to me that ensures everyone is progressing on schedule and comprehending the material. This is critical, as the rest of the course will build off of this foundation."
  },
  {
    "objectID": "assignments/index.html#final-exam",
    "href": "assignments/index.html#final-exam",
    "title": "Assignment Details",
    "section": "Final Exam",
    "text": "Final Exam\nOn the college-determined date, we will have a comprehensive, closed-book, in-class final exam."
  },
  {
    "objectID": "assignments/problem-sets.html",
    "href": "assignments/problem-sets.html",
    "title": "Problem Sets",
    "section": "",
    "text": "There will be several problem sets (typically 5-6) spaced throughout the semester. Typically, after we cover a major concept or set of tools, I will assign a problem set to practice this material.\nProblem sets will be a combination of math/statistical theory & application problems, as well as problems that require the use of R with real data.\nProblem sets are typically due one week from the class period it is assigned (although the due date announced on the problem set is the final authority on this), and must handed in or emailed to me by the start of class (so please type or, if you must, hand write and scan them)."
  },
  {
    "objectID": "assignments/problem-sets.html#instructions",
    "href": "assignments/problem-sets.html#instructions",
    "title": "Problem Sets",
    "section": "Instructions",
    "text": "Instructions\nDue to the combination of traditional and R problems, there are several ways you can complete and turn each assignment:\n\nType up any applicable answers (saving any plots as images and including them) in a (e.g. Word) document and save it as a PDF and turn in a (commented!) .R file of commands for each relevant question.\nIf you wish to write out answers by hand, you may either print the  pdf above or write your answers (all I need is your work and answers) on your own paper and then please scan/photograph & convert them to a single PDF, if they are easily readable, but this is not preferred. See my guide to making a PDF\nYou can do your homework in Quarto, rendering it to a single html or pdf file, which you will turn in (make sure it shows all code input and outputs). You do not need to turn in the .qmd file. I often provide an R Project for you to download and unzip (as well as one in Rstudio Cloud), which contains a .qmd file for you to work in (and sometimes data files).\n\nTo minimize confusion, I suggest creating a new R Project (e.g. hw1) and storing any data and plots in that folder on your computer. See my example workflow.\nYou may work together (and I highly encourage that) but you must turn in your own answers."
  },
  {
    "objectID": "assignments/problem-sets.html#grading",
    "href": "assignments/problem-sets.html#grading",
    "title": "Problem Sets",
    "section": "Grading",
    "text": "Grading\nI grade homeworks 70% for completion, and for the remaining 30%, pick one question to grade for accuracy — so it is best that you try every problem, even if you are unsure how to complete it accurately."
  },
  {
    "objectID": "assignments/research-paper.html",
    "href": "assignments/research-paper.html",
    "title": "Research Paper Assignment",
    "section": "",
    "text": "Warning\n\n\n\nDue by the end of the day via email Monday December 6. See below for what the email should contain.\nBelow I have the video/lecture and slides that I give all ECON 306 courses on how to write a good paper in general — geared towards writing an Op-Ed, but the principles apply to any paper you wish to write well:"
  },
  {
    "objectID": "assignments/research-paper.html#overview",
    "href": "assignments/research-paper.html#overview",
    "title": "Research Paper Assignment",
    "section": "Overview",
    "text": "Overview\nYour task is to write a research paper on any topic in political economy of your choosing so long as it has an empirical component. The purpose of this paper is threefold: (1) to demonstrate to me that you have mastered the material of this course, (2) to develop your writing, communication, and data analysis skills, and (3) to help you develop, strengthen, and/or modify your own views by grappling with them in writing. As a reminder, this paper constitutes 25% of your final course grade.\nI will spend a significant portion of one class discussing more about the writing process, and provide a detailed guide to help you choose a topic, craft arguments, and write a good paper.\nI am NOT looking for a survey of existing research, a series of block quotes showing what some economist said about X, a regurgitation of my lectures, a list of pros and cons with a last minute conclusion, a book review, and so on. I also do NOT want you to compress everything about econometrics you learned from this class into a single paper. You should only use those insights that are relevant to your topic and your argument (it may only be one or two things!).\nI am looking for a paper that attempts to answer a specific research question of interest to economists by using data analysis. You should be able to summarize your paper in one or two sentences – the specific research question your paper addresses, your method for answering it, and your results. It should be a reasonably original paper (it is difficult with limited knowledge, time, and data to offer something truly original), and it should be your own take on the topic. I expect you to, at the very least, have one multivariate regression used to reach your conclusions. I do not expect or require you to find statistically significant results.\nPlease note that, while you are not required to, I highly recommend discussing your ideas with me over email or in person. I will also read any drafts you would like to submit to me early, and provide you with helpful comments, subject to my own availability. I will stop accepting drafts to provide comments by November 30."
  },
  {
    "objectID": "assignments/research-paper.html#length-references-mechanics",
    "href": "assignments/research-paper.html#length-references-mechanics",
    "title": "Research Paper Assignment",
    "section": "Length, References, & Mechanics",
    "text": "Length, References, & Mechanics\nI hesitate to give formal length requirements, because most students will write the bare minimum, and also because different papers have different optimal lengths. What truly matters is that your paper is long enough to say what you need to say, to say it well, and to say it briefly. Ceterus paribus, if papers \\(A\\) and \\(B\\) say the same thing, but paper \\(B\\) says it in half the length (without sacrificing key arguments), paper \\(B\\) is a better paper. You will also find that simply by including the necessary components of an empirical paper (plots, tables, etc.), your page count will by necessity increase on its own.\nSince you of course still want to know what a good length will be, my rule of thumb is that your paper should be about \\(\\mathbf{10\\pm 5}\\) pages (size 12 font, double spaced, 1” margins). This will depend on the topic you have chosen to written on, your data, and your own personal writing style.\nI would also like you to use scholarly references, that is, articles from economics journals and cite them properly. I do not have a minimum requirement of the amount of references, but I expect you to have at least 2-5 scholarly references, depending on your paper topic and thesis. I am not particularly picky about exactly how you format your citations or bibliography, just please be consistent, and do not use footnotes or endnotes (only because they annoy me). I suggest the APA author-year-page in-text citation format that is fairly standard in economics journals, i.e.: “The division of labor is limited by the extent of the market,” (Smith 1776: 27). Look at my slides or my handouts for a suggested bibliography style. If you use .bib files, the default formatting is fine."
  },
  {
    "objectID": "assignments/research-paper.html#sources-for-inspiration",
    "href": "assignments/research-paper.html#sources-for-inspiration",
    "title": "Research Paper Assignment",
    "section": "Sources for Inspiration",
    "text": "Sources for Inspiration\nHere is a list of a few places you might consider to dig up some information on your topic, or to help you find a topic. Just be sure to read and cite the actual sources that these secondary sources cite.\n\nMajor news outlets (e.g. CNN, Wall Street Journal, New York Times, USA Today, Huffington Post, the Guardian) and wherever you may find your news (e.g. reddit, Buzzfeed, etc) both for current events, and also their Opinion/Commentary sections for other op-eds to learn from and/or critique\nWikipedia – no seriously, it is the first place I go to learn about a new topic. Just be sure to actually investigate the underlying research and use that for your references!\nEconomics Podcasts\n\nEcontalk – a fantastic podcast series by Russ Roberts that interviews famous economists, philosophers, businesspeople, and other figures who have an impact on the world of ideas\nFreakonomics (Radio) – another great podcast by one of the authors of the famed Freakonomics books about intermediate-level economic topics, often an in-depth series of interviews on a major issue\nNPR Planet Money – another good podcast series on economics topics and current events, much shorter and more 10,000-foot level approach than Econtalk or Freakonomics\n\nPopular Economists’ Blogs/Blogs on Economics:\n\nMarginal Revolution – Tyler Cowen & Alex Tabarrok (head and shoulders above the rest!)\nCafe Hayek – Don Boudreaux (libertarian-leaning, mostly just Don on trade and micro-policy)\nEconLog – Bryan Caplan, David Henderson (libertarian-leaning, good analysis)\nThe Conscience of a Liberal – Paul Krugman at New York Times (strong left-wing politics)\nThe Grumpy Economist – John Cochrane (Chicago School approach)\nGreg Mankiw’s Blog – Greg Mankiw (moderate conservative, New Keynesian approach)\nUndercover Economist – Tim Harford (British, non-political, very easy to understand)\nChris Blattman – Chris Blattman (great on economic development, poverty, and conflict in poor countries)\nSlate Star Codex – “Scott Alexander” (a pseudonym, apparently a Medical Student, but one of the most lucid social science blogs ever written)\nFivethirtyeight – Nate Silver & co. (a journalist, but a leader on using data and statistics in social science and journalism)\nAndrew Gelman – Andrew Gelman (a statistician, but another leader on using data and statistics for social science)"
  },
  {
    "objectID": "assignments/research-paper.html#data-sources",
    "href": "assignments/research-paper.html#data-sources",
    "title": "Research Paper Assignment",
    "section": "Data Sources",
    "text": "Data Sources\nWhile it is one thing to find a topic to write on, it is an altogether different animal to find data to use to test empirical research questions. You will find out quickly that the constraint to writing an empirical paper is not the set of topics or questions to write on (though that is often a challenge itself!), but the data available to use.\nDepending on the topic, you can also collect your own data, and many times you will want to create a custom dataset by simply combining data from different sources.\nDon’t forget to check out the Data Resources for ideas and examples of how and where to find data sets."
  },
  {
    "objectID": "assignments/research-paper.html#some-titles-of-former-students-papers",
    "href": "assignments/research-paper.html#some-titles-of-former-students-papers",
    "title": "Research Paper Assignment",
    "section": "Some Titles of Former Students’ Papers",
    "text": "Some Titles of Former Students’ Papers\n\nEffect of Economic Growth (GDP Per Capita) on Carbon dioxide emissions\nDoes Transfer Expenditure Impact a Team’s League Position?\nEmpirical Research about the impact of immigration into the United States on the employment of Black or African American citizens\nThe Performance and Pay of Quarterbacks in the National Football League\nBuy You a Vote\nA Study of the Environmental Costs of Economic Growth: A Look at the Environmental Kuznets Curve\nCorruption and Inequality: A Look Through the Shadows\nSweat More Make More: The Effect of Physical Activity on Income\nDoes Raising the Minimum Wage Actually Affect Unemployment?\nExploring the Effects of Children and Marriage on Men’s and Women’s Incomes\nThe Effect of GDP on Happiness\nInfluence of Socioeconomic Factors and Gun Laws on the Frequency of Gun Violence in America: A Statewide Analysis from 2010-2018\nThe Effect of Unemployment Benefits on the Unemployment Rate\nEconometric Approach to Time of Game in the MLB\nDoes Spending More on the Offensive Line & the Defensive Line Affect NFL Team Wins?\nIs Twitter Strong Enough to Measure NBA Player Performance?"
  },
  {
    "objectID": "assignments/research-paper.html#grading-rubric-and-deadlines",
    "href": "assignments/research-paper.html#grading-rubric-and-deadlines",
    "title": "Research Paper Assignment",
    "section": "Grading Rubric and Deadlines",
    "text": "Grading Rubric and Deadlines\nWhile it may be possible for many papers in your college career, is not a paper you can write the last minute and do well on. To ensure that you do not get too far behind, I have split the assignment into stages that are due at different intervals over the semester. Note that your topic can and may change depending on what you are able to find and work with. The hardest part is finding data that allows you to test a research question. It is primarily for this reason that writing an empirical paper on what you want is very difficult.\n\n\n\n\n\n\n\n\n\nAssignment\nPoints\nDue Date\nDescription\n\n\n\n\nAbstract\n5\nFri Oct 28\nShort summary of your ideas\n\n\nLiterature Review\n10\nFri Nov 18\n1-3 paragraphs on 2-3 scholarly sources\n\n\nData Description\n10\nMon Nov 28\nDescription of data sources, and some summary statistics\n\n\nPresentation\n5\nNov 30/Dec 5\nShort presentation of your project so far\n\n\nFinal Paper Due\n70\nFri Dec 9\nEmail to me paper, data, and code\n\n\n\nAll assignments are due as emails to me.\n\nAbstract: write a short paragraph (3-6 sentences) summarizing: what rough topic you want to look at, a specific research question that you think you can get data to test, where you think you might be able to get some data. It’s okay that much of this is speculative and you might change your mind or do very different things later!\nData Description: describe what data you managed to find (a few paragraphs): where is/are the dataset(s) from? What variables are included, and how are they measured (e.g. what units, categories, etc)? Give us some summary statistics of the data (a table would be nice, some scatterplots and histograms would be nice), are there any interesting patterns?\nLiterature Review: find 2-3 scholarly sources (ideally scholarly journal articles) that discuss your research question (or related topics), especially if they have empirical findings. Explain what these sources found, how they found it, and how your paper relates to this literature. Don’t worry about being original!\nPresentation: a 5-10 minute presentation of what you have so far. Different people will be at different stages, since your paper is not due yet! Use this as an opportunity to get feedback from me and your classmates, that might help you finish your project. Using some slides to show tables, models, and plots is highly recommended.\n\n\n\nFinal Paper Due: email the full paper to me (see below for what should be in the email).\n\nThe remaining 70% for the final product are broken down as follows:\n\n\n\nCategory\nPoints\n\n\n\n\nPersuasiveness\n10\n\n\nClarity\n10\n\n\nEconometric Validity\n20\n\n\nEconomic Soundness\n20\n\n\nOrganization\n5\n\n\nReferences\n5\n\n\nTOTAL\n70\n\n\n\n\nPersuasiveness: How persuasive is your argument? Would a reasonably educated college-level reader who is familiar with economics and statistics but not necessarily this course find themselves understanding your argument and agreeing with you? [Write for an audience wider than just members of this class. Therefore, don’t use terms, sources, or “inside jokes” that only other students in this class (and no one else) would understand.] Remember, your goal is not to convince me (though you may), your goal is to convince any educated reader, and I grade you the probability that this is likely. You are the lawyer, I am the judge, and your audience is the jury.\nClarity How clear is your paper? Is it clear what your research question is, how you answer it, and what your results are? Can you summarize these in a sentence or two? Are there confusing passages, excessive jargon or passive voice, or irrelevant arguments and examples?\nEconometric Validity: Is your econometric model sensible and plausible? Do you adequately address the assumptions and limitations of your model? Do you use appropriate data? Do you adequately describe your data, identify patterns, aberrations, and clearly generate testable hypotheses from your data? Would someone else, given your data, be able to replicate your findings?\nEconomic Soundness: Do you place your empirical question in a broader context of applying economic principles? Do you describe relevant economic policies, institutions, relationships, and/or other relevant economic principles to the questions you are asking? Are your theories and hypotheses about relationships between data plausible and intuitive? Do you makes sense of your hypotheses and the results of your analysis and connect them to sound economic principles? Is your question, strategy, and/or results of interest to economists? You will lose up to 20 points for papers that have no economic content.\nOrganization: Is your paper organized? Have you presented your separate arguments/examples in a logical order? Is it clear when you are moving on from one section to another? Is it clear when and where you are summarizing and concluding? Is your paper presented according to professional norms (e.g. summary statistics tables, regression tables, etc)?\nReferences: Does your paper use multiple scholarly references? Does it properly cite them in the text for main ideas borrowed and for direct quotations (if applicable)? Are they consistently listed at the end in a references section?\nStyle: Is your paper interesting and easy to read? Does it engage the reader? Is it written in active voice? This is somewhat subjective, and hence, the smallest portion of your grade."
  },
  {
    "objectID": "assignments/research-paper.html#emailing-your-final-version-to-me",
    "href": "assignments/research-paper.html#emailing-your-final-version-to-me",
    "title": "Research Paper Assignment",
    "section": "Emailing Your Final Version to Me",
    "text": "Emailing Your Final Version to Me\nWhen you send your final email (by Monday December 6), it should contain the following files:\n\nYour final paper as a .pdf. It should include an abstract and bibliography and all tables and figures contained within it.\nThe (commented!) code used for your data analysis (i.e. loading data, making tables, making plots, running regressions). These can be either .R files: one or multiple (one-per-task) are equally fine OR a .qmd file. I want to know how you reached the results you got! Reproducibility is the goal!\nYour data used, in whatever original format you found it (e.g. .csv, .xlsx, .dta)\n\nAgain, you are not obligated to use Quarto to write your paper. Microsoft Word is fine."
  },
  {
    "objectID": "content/1.1-content.html#overview",
    "href": "content/1.1-content.html#overview",
    "title": "1.1 — Introduction to Econometrics — Class Content",
    "section": " Overview",
    "text": "Overview\nWelcome to ECON 480 — Econometrics! Today’s lesson will be an overview of the content and the assignments of the course. Please read and familiarize yourself with the syllabus.\nThis is not just a “syllabus day,” as we need to hit the ground running, beginning with learning the software we will be using this semester. Starting next class, we will do a deep dive in to R for about 2 weeks. In preparation, please do the following before next class:\n\nGo to RStudio.cloud and register a (free) account with your Hood details.\nTry to install R and R Studio on your computer if you are able, before next class. You will still always have RStudio.cloud available all to use in your browser, but it is much better to have a real version of R and R Studio on your own machine to work with all semester."
  },
  {
    "objectID": "content/1.1-content.html#readings",
    "href": "content/1.1-content.html#readings",
    "title": "1.1 — Introduction to Econometrics — Class Content",
    "section": " Readings",
    "text": "Readings\nToday is introductory, but please heed this timeless message:\n\n\nA message to students from the Doggfather himself, @SnoopDogg pic.twitter.com/wsSANYv8u6\n\n— Ryan Briggs (@ryancbriggs) August 12, 2020\n\n\nPlease note going forward, the lesson numbers and topics (e.g. 1.1) on this website are my design, and will not match up with the textbook!"
  },
  {
    "objectID": "content/1.1-content.html#slides",
    "href": "content/1.1-content.html#slides",
    "title": "1.1 — Introduction to Econometrics — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/1.1-content.html#assignments",
    "href": "content/1.1-content.html#assignments",
    "title": "1.1 — Introduction to Econometrics — Class Content",
    "section": " Assignments",
    "text": "Assignments\n\nPreliminary Statistics Survey Due By Next Class\nPlease take the preliminary survey on your statistics and software background by nexrt class. This will help us all have a productive semester together."
  },
  {
    "objectID": "content/1.2-content.html#overview",
    "href": "content/1.2-content.html#overview",
    "title": "1.2 — Meet R — Class Content",
    "section": " Overview",
    "text": "Overview\nToday we begin the long slog to your mastery of R. We begin with the basics - how R works, how to use it, the different data types, and how to create and manipulate objects."
  },
  {
    "objectID": "content/1.2-content.html#readings",
    "href": "content/1.2-content.html#readings",
    "title": "1.2 — Meet R — Class Content",
    "section": " Readings",
    "text": "Readings\n\n Ch.1 in Wickham & Grolemund, http://r4ds.had.co.nz/introduction.html\n\nNow that we start working with R, you should consider this book to be your primary reference for R-related questions. We will broadly cover the first few chapters in order over the next 2-3 class periods."
  },
  {
    "objectID": "content/1.2-content.html#r-practice",
    "href": "content/1.2-content.html#r-practice",
    "title": "1.2 — Meet R — Class Content",
    "section": " R Practice",
    "text": "R Practice\nToday we will be working on practice problems. Answers will be posted later on that page."
  },
  {
    "objectID": "content/1.2-content.html#slides",
    "href": "content/1.2-content.html#slides",
    "title": "1.2 — Meet R — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/1.2-content.html#assignments",
    "href": "content/1.2-content.html#assignments",
    "title": "1.2 — Meet R — Class Content",
    "section": " Assignments",
    "text": "Assignments\n\nPreliminary Statistics Survey\nPlease take the preliminary survey on your statistics and software background. This will help us all have a productive semester together."
  },
  {
    "objectID": "content/1.3-content.html#overview",
    "href": "content/1.3-content.html#overview",
    "title": "1.3 — Data Visualization — Class Content",
    "section": " Overview",
    "text": "Overview\nToday we start the fun stuff - data visualization. We will cover how to build plots with the package ggplot2 (part of the tidyverse): I will lecture for the first half (again, it will be a lot of information that you can refer back to as needed over the semester) and you will practice making plots."
  },
  {
    "objectID": "content/1.3-content.html#readings",
    "href": "content/1.3-content.html#readings",
    "title": "1.3 — Data Visualization — Class Content",
    "section": " Readings",
    "text": "Readings\n\n Ch.3 in Wickham & Grolemund, http://r4ds.had.co.nz/introduction.html\n\n\nRecommended/Resources on ggplot2\n\nR Studio’s ggplot2 Cheat Sheet\nggplot2’s website reference section\nHadley Wickham’sR for Data Science book chapter on ggplot2\nSTHDA’s be awesome in ggplot2\nr-statistic’s top 50 ggplot2 visualizations\n\n\n\nRecommended/Resources on Data Visualization\n\nKieran Healy’s Data Visualization: A Practical Guide\nClaus Wilke’s Fundamentals of Data Visualization\nPolicyViz Better Presentations\nKarl Broman’s How to Display Data Badly\nI Want Hue"
  },
  {
    "objectID": "content/1.3-content.html#r-practice",
    "href": "content/1.3-content.html#r-practice",
    "title": "1.3 — Data Visualization — Class Content",
    "section": " R Practice",
    "text": "R Practice\nAnswers from last class’ practice problems on base R are posted on that page.\nToday you will be working on R practice problems on data visualization. Answers will be posted later on that page."
  },
  {
    "objectID": "content/1.3-content.html#slides",
    "href": "content/1.3-content.html#slides",
    "title": "1.3 — Data Visualization — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/1.4-content.html#overview",
    "href": "content/1.4-content.html#overview",
    "title": "1.4 — Data Wrangling — Class Content",
    "section": " Overview",
    "text": "Overview\nToday we will cover the heart of tidyverse and use it for “data wrangling”. Today will again be a lot of content thrown at you, so you can look back at this as a reference all semester. Then we will do more practice problems."
  },
  {
    "objectID": "content/1.4-content.html#readings",
    "href": "content/1.4-content.html#readings",
    "title": "1.4 — Data Wrangling — Class Content",
    "section": " Readings",
    "text": "Readings\n\n Chs. 5, 10-12, 18 in Wickham & Grolemund, R for Data Science\n\n\nReferences by Package/Task\n\ntibble\n\nR For Data Science, Chapter 10: Tibbles\n\nreadr and importing data\n\nR For Data Science, Chapter 11: Data Import\nR Studio Cheatsheet: Data Import\n\ndplyr and data wrangling\n\nR For Data Science, Chapter 5: Data Transformation\nR Studio Cheatsheet: Data Wrangling (New version)\n\ntidyr and tidying or reshaping data\n\nR For Data Science, Chapter 12: Tidy Data\nR Studio Cheatsheet: Data Wrangling\nR Studio Cheatsheet: Data Import\n\njoining data\n\nR For Data Science, Chapter 13: Relational Data\nR Studio Cheatsheet: Data Transformation"
  },
  {
    "objectID": "content/1.4-content.html#r-practice",
    "href": "content/1.4-content.html#r-practice",
    "title": "1.4 — Data Wrangling — Class Content",
    "section": " R Practice",
    "text": "R Practice\nAnswers from last class’ practice problems on base R are posted on that page.\nToday you will be working on R practice problems on data wrangling Answers will be posted later on that page."
  },
  {
    "objectID": "content/1.4-content.html#slides",
    "href": "content/1.4-content.html#slides",
    "title": "1.4 — Data Wrangling — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/1.5-content.html#overview",
    "href": "content/1.5-content.html#overview",
    "title": "1.5 — Optimize Workflow — Class Content",
    "section": " Overview",
    "text": "Overview\nToday I will give you about half the class period to finish your 1.4 practice problems and I can answer questions. Then I will briefly show you how you the magic of how you can improve your workflow, efficiency, automation, reproducibility, and safe file backups with R Markdown, R Projects, git and github. All of these tools are slightly advanced, and optional for you to use, but the learning curve pays off very high returns for the rest of your life! A good handful of people every semester tell me that they keep using these tools years in the future."
  },
  {
    "objectID": "content/1.5-content.html#readings",
    "href": "content/1.5-content.html#readings",
    "title": "1.5 — Optimize Workflow — Class Content",
    "section": " Readings",
    "text": "Readings\n\n Chs. 8, 27, 28, 29, 30 (but especially 27) in Wickham & Grolemund, R for Data Science\n\n\nOn Plain-Text Workflow\n\nHealey, Kieran, The Plain Person’s Guide to Plain Text Social Science\n\n\n\nOn Markdown Syntax\n\nMarkdown tutorial\nQuarto Documentation: Markdown Basics\n\n\n\nOn Quarto\n\nQuarto Documentation: Tutorial: Hello, Quarto\nQuarto Documentation: Tutorial: Computation\nQuarto Documentation: Tutorial: Authoring\nQuarto Documentation: Guide\n\n\n\nOn LaTeX\n\nWikibooks LaTeX (see esp. Mathematics chapter)\n\n\n\nOn Git and Github\n\nBryan, Jenny and Jim Hester, Happy Git and GitHub for the useR"
  },
  {
    "objectID": "content/1.5-content.html#r-practice",
    "href": "content/1.5-content.html#r-practice",
    "title": "1.5 — Optimize Workflow — Class Content",
    "section": " R Practice",
    "text": "R Practice\nAnswers from last class’ practice problems on base R are posted on that page.\nToday you will be working on R practice problems on data wrangling Answers will be posted later on that page.\nToday’s “practice problems” get you to practice the tools we are working with today. They are again not required, but will help you if you are interested."
  },
  {
    "objectID": "content/1.5-content.html#slides",
    "href": "content/1.5-content.html#slides",
    "title": "1.5 — Optimize Workflow — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/2.1-content.html#overview",
    "href": "content/2.1-content.html#overview",
    "title": "2.1 — Data 101 & Descriptive Statistics — Class Content",
    "section": " Overview",
    "text": "Overview\nToday we begin with a review and overview of using data and descriptive statistics. We want to quantify characteristics about samples as statistics, which we will later use to infer things about populations (between which we will later identify causal relationships).\nNext class will be on random variables and distributions. This full week is your crash course/review of basic statistics that we will need to start the “meat and potatoes” of this class: linear regression next Thursday As such, I’ll give you a brief homework next week to review these statistical concepts (with minimal use of R!)."
  },
  {
    "objectID": "content/2.1-content.html#readings",
    "href": "content/2.1-content.html#readings",
    "title": "2.1 — Data 101 & Descriptive Statistics — Class Content",
    "section": " Readings",
    "text": "Readings\n\n Math and Probability Background Appendix A in Bailey\n\nNow that we return to the statistics, we will do a minimal overview of basic statistics and distributions. Review Bailey’s appendices. Today, only A is really useful, the rest will come next class.\nChapter 2 is optional, but will give you a good overview of using data."
  },
  {
    "objectID": "content/2.1-content.html#appendix",
    "href": "content/2.1-content.html#appendix",
    "title": "2.1 — Data 101 & Descriptive Statistics — Class Content",
    "section": " Appendix",
    "text": "Appendix\nSee the online appendix for today’s content:\n\nThe Summation Operator"
  },
  {
    "objectID": "content/2.1-content.html#slides",
    "href": "content/2.1-content.html#slides",
    "title": "2.1 — Data 101 & Descriptive Statistics — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/2.1-content.html#assignments",
    "href": "content/2.1-content.html#assignments",
    "title": "2.1 — Data 101 & Descriptive Statistics — Class Content",
    "section": " Assignments",
    "text": "Assignments\n\nProblem Set 1 Due Wed Sep 21\nProblem Set 1 is due by the end of the day Wednesday September 21 on Blackboard assignments."
  },
  {
    "objectID": "content/2.2-content.html#overview",
    "href": "content/2.2-content.html#overview",
    "title": "2.2 — Random Variables & Distributions — Class Content",
    "section": " Overview",
    "text": "Overview\nToday we finish your crash course/review of basic statistics with random variables and distributions."
  },
  {
    "objectID": "content/2.2-content.html#readings",
    "href": "content/2.2-content.html#readings",
    "title": "2.2 — Random Variables & Distributions — Class Content",
    "section": " Readings",
    "text": "Readings\n\n Math and Probability Background Appendix A in Bailey\n\nNow that we return to the statistics, we will do a minimal overview of basic statistics and distributions. Review all of Bailey’s appendices.\nChapter 2 is optional, but will give you a good overview of using data."
  },
  {
    "objectID": "content/2.2-content.html#appendix",
    "href": "content/2.2-content.html#appendix",
    "title": "2.2 — Random Variables & Distributions — Class Content",
    "section": " Appendix",
    "text": "Appendix\nSee the online appendix for today’s content:\n\nProperties of Expected Value and Variance\nCreating Mathematical Functions in R\nGraphing Mathematical and Statistical Functions in R"
  },
  {
    "objectID": "content/2.2-content.html#slides",
    "href": "content/2.2-content.html#slides",
    "title": "2.2 — Random Variables & Distributions — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/2.3-content.html#overview",
    "href": "content/2.3-content.html#overview",
    "title": "2.3 — Simple Linear Regression — Class Content",
    "section": " Overview",
    "text": "Overview\nToday we start looking at associations between variables, which we will first attempt to quantify with measures like covariance and correlation. Then we turn to fitting a line to data via linear regression. We overview the basic regression model, the parameters and how they are derived, and see how to work with regressions in R with lm and the tidyverse package broom.\nWe consider an extended example about class sizes and test scores, which comes from a (Stata) dataset from an old textbook that I used to use, Stock and Watson, 2007. Download and follow along with the data from today’s example:1\n\n caschool.dta\n\nI have also made a RStudio Cloud project documenting all of the things we have been doing with this data that may help you when you start working with regressions (next class):\n\n Class Size Regression Analysis"
  },
  {
    "objectID": "content/2.3-content.html#readings",
    "href": "content/2.3-content.html#readings",
    "title": "2.3 — Simple Linear Regression — Class Content",
    "section": " Readings",
    "text": "Readings\n\n Ch. 3.1, Math and Probability Background Appendix A in Bailey\n\nNow that we return to the statistics, we will do a minimal overview of basic statistics and distributions. Review all of Bailey’s appendices.\nChapter 2 is optional, but will give you a good overview of using data."
  },
  {
    "objectID": "content/2.3-content.html#appendix",
    "href": "content/2.3-content.html#appendix",
    "title": "2.3 — Simple Linear Regression — Class Content",
    "section": " Appendix",
    "text": "Appendix\nSee the online appendix for today’s content:\n\nVariance\nCovariance\nCorrelation"
  },
  {
    "objectID": "content/2.3-content.html#slides",
    "href": "content/2.3-content.html#slides",
    "title": "2.3 — Simple Linear Regression — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/2.4-content.html#overview",
    "href": "content/2.4-content.html#overview",
    "title": "2.4 — Goodness of Fit and Bias — Class Content",
    "section": " Overview",
    "text": "Overview\nToday we continue looking at basic OLS regression. We will cover how to measure if a regression line is a good fit (using \\(R^2\\) and \\(\\sigma_u\\) or SER), and whether OLS estimators are biased. These will depend on four critical assumptions about \\(u\\).\nIn doing so, we begin an ongoing exploration into inferential statistics, which will finally become clear in another week. The most confusing part is recognizing that there is a sampling distribution of each OLS estimator. We want to measure the center of that sampling distribution, to see if the estimator is biased. Next class we will measure the spread of that distribution.\nWe continue our extended example about class sizes and test scores, which comes from a (Stata) dataset from an old textbook that I used to use, Stock and Watson, 2007. Download and follow along with the data from today’s example:1\n\n caschool.dta\n\nI have also made a RStudio Cloud project documenting all of the things we have been doing with this data that may help you when you start working with regressions (next class):\n\n Class Size Regression Analysis"
  },
  {
    "objectID": "content/2.4-content.html#readings",
    "href": "content/2.4-content.html#readings",
    "title": "2.4 — Goodness of Fit and Bias — Class Content",
    "section": " Readings",
    "text": "Readings\n\n Ch. 3.2-3.4, 3.7-3.8 in Bailey, Real Econometrics"
  },
  {
    "objectID": "content/2.4-content.html#appendix",
    "href": "content/2.4-content.html#appendix",
    "title": "2.4 — Goodness of Fit and Bias — Class Content",
    "section": " Appendix",
    "text": "Appendix\nSee the online appendix for today’s content:\n\nDeriving the OLS Estimators\nBias in \\(\\hat{\\beta_1}\\)\nProof of the Unbiasedness of \\(\\hat{\\beta_1}\\)"
  },
  {
    "objectID": "content/2.4-content.html#slides",
    "href": "content/2.4-content.html#slides",
    "title": "2.4 — Goodness of Fit and Bias — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/2.5-content.html#overview",
    "href": "content/2.5-content.html#overview",
    "title": "2.5 — Precision and Diagnostics — Class Content",
    "section": " Overview",
    "text": "Overview\nLast class and this class we are looking at the sampling distibution of OLS estimators (particularly \\(\\hat{\\beta_1})\\). Last class we looked at what the center of the distribution was - the true \\(\\beta_1\\) - so long as the assumptions about \\(u\\) hold:\n\nWhen \\(cor(X,u)=0\\), \\(X\\) is exogenous and the OLS estimators are unbiased.\nWhat \\(cor(X,u)\\neq 0\\), \\(X\\) is endogenous and the OLS estimators are biased.\n\nToday we continue looking at the sampling distibution by determining the variation in \\(\\hat{beta_1}\\) (it’s variance or its standard error1). We look at the formula and see the three major determinants of variation in \\(\\hat{\\beta_1}\\):\n\nGoodness of fit of the regression \\((SER\\) or \\(\\hat{\\sigma_u}\\)\nSample size \\(n\\)\nVariation in \\(X\\)\n\nWe also look at the diagnostics of a regression by looking at its residuals \\((\\hat{u_i})\\) for anomalies. We focus on the problem of heteroskedasticity (where the variation in \\(\\hat{u_i])\\) changes over the range of \\(X\\), which violates assumption 2 (errors are homoskedastic): how to detect it, test it, and fix it with some packages. We also look at outliers, which can bias the regression. Finally, we also look at how to present regression results.\nWe continue our extended example about class sizes and test scores, which comes from a (Stata) dataset from an old textbook that I used to use, Stock and Watson, 2007. Download and follow along with the data from today’s example:2\n\n caschool.dta\n\nI have also made a RStudio Cloud project documenting all of the things we have been doing with this data that may help you when you start working with regressions (next class):\n\n Class Size Regression Analysis"
  },
  {
    "objectID": "content/2.5-content.html#readings",
    "href": "content/2.5-content.html#readings",
    "title": "2.5 — Precision and Diagnostics — Class Content",
    "section": " Readings",
    "text": "Readings\n\n Finish Ch.3 in Bailey, Real Econometrics"
  },
  {
    "objectID": "content/2.5-content.html#r-practice",
    "href": "content/2.5-content.html#r-practice",
    "title": "2.5 — Precision and Diagnostics — Class Content",
    "section": " R Practice",
    "text": "R Practice\nToday (and next class) you will be working on practice problems. Answers will be posted on that page later."
  },
  {
    "objectID": "content/2.5-content.html#appendix",
    "href": "content/2.5-content.html#appendix",
    "title": "2.5 — Precision and Diagnostics — Class Content",
    "section": " Appendix",
    "text": "Appendix\nSee the online appendix for today’s content:\n\nDeriving the OLS Estimators\nBias in \\(\\hat{\\beta_1}\\)\nCorrelation"
  },
  {
    "objectID": "content/2.5-content.html#slides",
    "href": "content/2.5-content.html#slides",
    "title": "2.5 — Precision and Diagnostics — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/2.6-content.html#overview",
    "href": "content/2.6-content.html#overview",
    "title": "2.6 — Inference for Regression — Class Content",
    "section": " Overview",
    "text": "Overview\nWe begin with some more time for you to work on the R Practice from last class.\nThis week is about inferential statistics: using statistics calculated from a sample of data to infer the true (and unmeasurable) parameters that describe a population. In doing so, we can run hypothesis tests on our sample to determine a point estimate of a parameter, or construct a confidence interval from our sample to cast a range for the true parameter.\nThis is standard principles of statistics - you hopefully should have learned it before. If it has been a while (or never) since your last statistics class, this is one of the hardest concepts to understand at first glance. I recommend Khan Academy [From sampling distributions through significance tests, for this. Though the whole class is helpful!] or Google for these concepts, as every statistics class will cover them in the standard way.\nThat being said, I do not cover inferential statistics in the standard way (see the appendix today below for an overview of the standard way). I think it will be more intuitive if I show you where these concepts come from by simulating a sampling distribution, as opposed to reciting the theoretical sampling distributions."
  },
  {
    "objectID": "content/2.6-content.html#readings",
    "href": "content/2.6-content.html#readings",
    "title": "2.6 — Inference for Regression — Class Content",
    "section": " Readings",
    "text": "Readings\n\n Ch.4 in Bailey, Real Econometrics\n Ch. 10 (optionally 8-9) in Modern Dive\n Visualizing Probability and Inference\n\nBailey teaches inferential statistics in the classical way (with reference to theoretical \\(Z\\) and \\(t\\) distributions, and \\(Z\\) and \\(t\\) tests). This is all valid. Again, you may wish to brush up with Khan Academy [From sampling distributions through significance tests, for this. Though the whole class is helpful!].\nThe latter “book” (also free online, like R4DS) uses the infer package to run simulations for inferential statistics. Chapter 10 is focused on regression (but I also recommend the chapters leading up to it, which are on inferential statistics broadly, using this method).\nThe final link is a great website for visualizing basic statistic concepts like probability, distributions, confidence intervals, hypothesis tests, central limit theorem, and regression."
  },
  {
    "objectID": "content/2.6-content.html#r-practice",
    "href": "content/2.6-content.html#r-practice",
    "title": "2.6 — Inference for Regression — Class Content",
    "section": " R Practice",
    "text": "R Practice\nToday we will finish working on practice problems. Answers will be posted on that page later."
  },
  {
    "objectID": "content/2.6-content.html#appendix",
    "href": "content/2.6-content.html#appendix",
    "title": "2.6 — Inference for Regression — Class Content",
    "section": " Appendix",
    "text": "Appendix\nSee the online appendix for today’s content:\n\nInference via Simulation vs. Classical Statistics"
  },
  {
    "objectID": "content/2.6-content.html#slides",
    "href": "content/2.6-content.html#slides",
    "title": "2.6 — Inference for Regression — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/2.7-content.html#overview",
    "href": "content/2.7-content.html#overview",
    "title": "2.7 — Hypothesis Testing for Regression — Class Content",
    "section": " Overview",
    "text": "Overview\nToday we finish our discussion of statistical inference, focusing on hypothesis testing about our regression. We will hopefully have time at the end to work on a few practice problems."
  },
  {
    "objectID": "content/2.7-content.html#readings",
    "href": "content/2.7-content.html#readings",
    "title": "2.7 — Hypothesis Testing for Regression — Class Content",
    "section": " Readings",
    "text": "Readings\n\n Ch.4 in Bailey, Real Econometrics\n Ch.4 in Bailey, Real Econometrics\n Ch. 10 (optionally 8-9) in Modern Dive\n Visualizing Probability and Inference\n Last Week Tonight with John Oliver: Scientific Studies\n“There is Still Only One Test”\n\nThe readings are the same as last class, plus one highly recommended additional video: be sure to watch the excellent and hilarious discussion of the limits and misuses of scientific studies and statistical significance \\((p\\)-values) in the Last Week Tonight clip. I also added the blog post that I reference in class about all hypothesis testing is really one common procedure."
  },
  {
    "objectID": "content/2.7-content.html#r-practice",
    "href": "content/2.7-content.html#r-practice",
    "title": "2.7 — Hypothesis Testing for Regression — Class Content",
    "section": " R Practice",
    "text": "R Practice\nToday we be working on practice problems. Answers will be posted on that page later."
  },
  {
    "objectID": "content/2.7-content.html#slides",
    "href": "content/2.7-content.html#slides",
    "title": "2.7 — Hypothesis Testing for Regression — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/3.1-content.html#overview",
    "href": "content/3.1-content.html#overview",
    "title": "3.1 — The Fundamental Problem of Causal Inference — Class Content",
    "section": " Overview",
    "text": "Overview\nToday we begin extending the foundation of simple linear regression with one variable into more advanced models that can plausibly claim (when we are done) to measure causal relationships between \\(X\\) and \\(Y\\). The rest of the semester is primarily extending regression from one \\(X\\) to many, and changing the functional form to fit various idiosyncracies of different variables, or use clever techniques to isolate marginal effects of interest.\nWe begin by covering the fundamental problem of causal inference, that we can never observe counterfactual states of the world. If we could, then we could easily measure the causal effect of \\(X \\mapsto Y\\) by comparing how \\(Y\\) is different when \\(X\\) is different. The next best thing we can do is run a random control trial (RCT) where individuals are randomly assigned to groups to be given (different) treatment(s), and then we can compare the average outcome across groups. Random assignment ensures the only thing that differs across group outcomes is whether or not the group was given treatment, estimating the causal effect of treatment on the outcome.\nFor now, we will understand causality to mean the average treatment effect (ATE) from a RCT. RCTs are both popular and controversial. Last year’s Nobel Prize winners in economics won for their use of RCTs in development economics, but they have drawn significant criticism from other top economists as not being sufficiently generalizable.\nOf course, the bigger problem is it is very difficult, often impossible, to run a RCT to test a hypothesis. So economists have developed a toolkit of clever techniques to identify causal effects in “natural experiments” or “quasi-experiments” that sufficiently simulate a RCT. Knowledge of this repertoire of tools is truly why modern economists are in demand by government and business (not supply and demand models, etc)!"
  },
  {
    "objectID": "content/3.1-content.html#readings",
    "href": "content/3.1-content.html#readings",
    "title": "3.1 — The Fundamental Problem of Causal Inference — Class Content",
    "section": " Readings",
    "text": "Readings\n\n Ch.1 in Bailey, Real Econometrics\n Ch. 4 in Cunningham (2020), Causal Inference, the Mixtape\n Rubin Causal Model\n\nBailey begins the book with a discussion of causality and random control trials that is pretty good.\nThe potential outcomes notation (e.g. \\(Y_i^{1}\\) and \\(Y_i^{0})\\) and model comes from a very famous 1974 paper by Donald Rubin in psychology. You can read more about it in Cunningham (2020) above, or the Wikipedia entry on the model.\nScott Cunningham’s excellent (and free!) Causal Inference, the Mixtape has a great discussion of the history, and examples, of potential outcomes in an accessible way.\nThe classic example that most economists (including myself) were taught about causality is the treatment of the Rubin model in Angrist and Pischke’s Mostly Harmless Econometrics (one of the classic books on econometrics). You do not need to buy that book for this class, but if you will be doing data work in your future, or going to graduate school, this book is a must own and read:\n\n Angrist and Pischke, 2009, Mostly Harmless Econometrics\n\nMy health insurance example is lifted directly out of this book.\nHere’s also a great list of famous social science (including economics) papers that use natural experiments:\n\n List of 19 Natural Experiments\n\nFor more on John Snow and the birth of epidemiology, the excellent PBS show Victoria has a full episode (and great resources) about the cholera outbreak."
  },
  {
    "objectID": "content/3.1-content.html#slides",
    "href": "content/3.1-content.html#slides",
    "title": "3.1 — The Fundamental Problem of Causal Inference — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/3.2-content.html#overview",
    "href": "content/3.2-content.html#overview",
    "title": "3.2 — DAGs — Class Content",
    "section": " Overview",
    "text": "Overview\nToday we return to a more nuanced discussion of causality, given what we have learned about the fundamental problem of causal inference (counterfactuals and potential outcomes). RCTs are great, but they are not everything — and in any case, you are never going to be able to design and run an RCT in the overwhelming majority of studies.\nNow that we understand counterfactuals, we can apply our idea of exogeneity to argue that indeed, yes, correlation does imply causation when \\(X\\) is exogenous! That is, \\(X\\) being correlated with \\(Y\\) implies there is a causal connection between \\(X\\) and \\(Y\\), and if we are certain that \\(cor(X,u)=0\\), then we are clearly measuring the causal effect of \\(X \\rightarrow Y\\)! If \\(cor(X,u) \\neq 0\\) and \\(X\\) is endogenous, there is still a causal connection between \\(X\\) and \\(Y\\), but it goes through other variables that jointly cause \\(X\\) and \\(Y\\).\nWe also introduce a new tool for thinking about simple causal models, the directed acyclic graph (DAG). These are a hip new trend for thinking about causal inference, so new and trendy that they aren’t really in any mainstream textbooks yet!\nDAGS and DAG rules (front doors, back doors, colliders, mediators, etc.) will allow you to visually map the causal relationships between variables and describe to you the variables you must control for in order to properly identify the causal effect you are trying to measure. I show you a simply tool, daggity.net that will help you do this, as well as ggdag in R."
  },
  {
    "objectID": "content/3.2-content.html#readings",
    "href": "content/3.2-content.html#readings",
    "title": "3.2 — DAGs — Class Content",
    "section": " Readings",
    "text": "Readings\n\nRecommended Reading\n\nOn the Credibility Revolution in Econometrics\n\n Angrist & Pischke, 2010, “The Credibility Revolution in Empirical Economics: How Better Research Design is Taking the Con out of Econometrics”\n\n\n\nOn DAGs\nDAGs are a trendy new concept in econometrics and causal inference, so much so that they have yet to find their way into any major econometrics textbook! There are some resources, however, that you can look to for understanding how they work (and I base much of my lecture off of them).\n\n Ch. 3 in Cunningham (2020), Causal Inference, the Mixtape\n Chs. 6-9 in Huntington-Klein (2022), The Effect: An Introduction to Research Design and Causality\n Pearl and MacKenzie, (2018), The Book of Why\n Heiss (2020), Causal Inference”\n Huntington-Klein (2019), Dagitty.net Cheat Sheet”\n Huntington-Klein (2019), Causal Diagrams Cheat Sheet”\n My blog post on “Econometrics, Data Science, and Causal Inference”\n\nThe best book to get more into the philosophy of causality and the major origin of DAGs is Judea Pearl (and David McKenzie)’s The Book of Why. We owe much to Pearl, he is the flagship of the causal revolution (outside of econometrics).1 And his twitter is pretty amusing.\nThe best instantiation of DAGs and causal inference into a “textbook” on econometrics and methods is Scott Cunningham’s (open source!) Causal Inference: The Mixtape chapter on DAGs. Nick Huntington-Klein has some great lecture slides, and some cheat sheets on using Dagitty.net and understanding DAGs.\nAndrew Heiss, a political science professor, has a great recent book chapter on causal inference using DAGs, complete with instructions on how to do it in R and dagitty.net.\nFinally, I have a blog post discussing the difference between econometrics, causal inference, and data science. The end touches on causality, DAGs, and Pearl."
  },
  {
    "objectID": "content/3.2-content.html#slides",
    "href": "content/3.2-content.html#slides",
    "title": "3.2 — DAGs — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/3.3-content.html#overview",
    "href": "content/3.3-content.html#overview",
    "title": "3.3 — Omitted Variable Bias — Class Content",
    "section": " Overview",
    "text": "Overview\nToday we return to our regression models, now knowing something about identifying causal effects. We know from DAGs that we often need to “adjust for” or “control for variables” in order to identify the causal effect we are interested in. Now we give a particular name and set of conditions for when we need to control a variable: .b[“omitted variable bias”], where some variable both causes \\(Y\\) (is in \\(u)\\), and is correlated with \\(X\\). To avoid introducing the bias, we now include it as an additional independent variable in our regression.\nThus, we now begin exploring multivariate regression with multiple regressors:\n\\[Y_i=\\beta_0+\\beta_1 X_{1i}+ \\beta_2 X_{2i} + u_i\\]\nNext class we will learn more about how the introduction of additional variables affects our model.\nWe continue the extended example about class sizes and test scores, which comes from a (Stata) dataset from an old textbook that I used to use, Stock and Watson, 2007. Download and follow along with the data from today’s example:1\n\n caschool.dta\n\nI have also made a RStudio Cloud project documenting all of the things we have been doing with this data that may help you when you start working with regressions:\n\n Class Size Regression Analysis (Cloud R project)\n Class Size Regression Analysis (output)"
  },
  {
    "objectID": "content/3.3-content.html#readings",
    "href": "content/3.3-content.html#readings",
    "title": "3.3 — Omitted Variable Bias — Class Content",
    "section": " Readings",
    "text": "Readings\n\n Ch. 5.1 in Bailey, Real Econometrics"
  },
  {
    "objectID": "content/3.3-content.html#slides",
    "href": "content/3.3-content.html#slides",
    "title": "3.3 — Omitted Variable Bias — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/3.5-content.html#overview",
    "href": "content/3.5-content.html#overview",
    "title": "3.5 — Writing an Empirical Paper — Class Content",
    "section": " Overview",
    "text": "Overview\nToday we discuss the elements of a good empirical paper, with an eye towards your paper projects.\nI highly recommend you look at my example repositories:\n\n Workflow\n Example Empirical Paper\n\nThe first is one that I showed you back in class 1.5 when we talked about R Markdown and having a reproducible, plain-text, workflow. It has an example workflow setup that I recommend you follow: create an R project for your project, and organize all of your data, code, images, & text in this set of folders inside of your project.1 You can look at Example_paper.qmd to see my text and code in R markdown to write the paper, and look at Example_paper.qdf to see the final product.\nThe second is a quick and dirty example of what I am looking for in your paper project in this class, using the ongoing example of the effect of class size on test scores. You can also look at paper.qmd to see my text and code in R markdown, and paper.pdf to see the final product. Note that this is an example I put quickly together with minimal effort as an example for you all, it does not necessarily mean it is an “A” quality paper!\nIf you are using R markdown to write your final paper, you should consult class 1.5 again, along with the 1.5 “R practice” that deals with the workflow repository."
  },
  {
    "objectID": "content/3.5-content.html#readings",
    "href": "content/3.5-content.html#readings",
    "title": "3.5 — Writing an Empirical Paper — Class Content",
    "section": " Readings",
    "text": "Readings\n\n Ch. 5.1, 5.2, 5.4 in Bailey, Real Econometrics"
  },
  {
    "objectID": "content/3.5-content.html#r-practice",
    "href": "content/3.5-content.html#r-practice",
    "title": "3.5 — Writing an Empirical Paper — Class Content",
    "section": " R Practice",
    "text": "R Practice\nToday you will be finishing R practice problems on multivariate regression. Answers will be posted later on that page."
  },
  {
    "objectID": "content/3.5-content.html#assignments",
    "href": "content/3.5-content.html#assignments",
    "title": "3.5 — Writing an Empirical Paper — Class Content",
    "section": " Assignments",
    "text": "Assignments\n\nProblem Set 4 Due Fri Nov 11\nProblem Set 4 is due by the end of the day on Tuesday, November 11."
  },
  {
    "objectID": "content/3.5-content.html#slides",
    "href": "content/3.5-content.html#slides",
    "title": "3.5 — Writing an Empirical Paper — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/4.1-content.html#overview",
    "href": "content/4.1-content.html#overview",
    "title": "4.1 — Multivariate OLS Estimators — Class Content",
    "section": " Overview",
    "text": "Overview\nToday we continue looking at multivariate regression, and see how the introduction of additional variables affects our model: the interpretation of the marginal effects (and we will measure an example of omitted variable bias), the standard errors of the estimators, and the goodness of fit of the regression.\nWe continue the extended example about class sizes and test scores, which comes from a (Stata) dataset from an old textbook that I used to use, Stock and Watson, 2007. Download and follow along with the data from today’s example:1\n\n caschool.dta\n\nI have also made a RStudio Cloud project documenting all of the things we have been doing with this data that may help you when you are working with regressions:\n\n Class Size Regression Analysis\n Class Size Regression Analysis (output)"
  },
  {
    "objectID": "content/4.1-content.html#readings",
    "href": "content/4.1-content.html#readings",
    "title": "4.1 — Multivariate OLS Estimators — Class Content",
    "section": " Readings",
    "text": "Readings\n\n Ch. 5.1, 5.2, 5.4 in Bailey, Real Econometrics"
  },
  {
    "objectID": "content/4.1-content.html#r-practice",
    "href": "content/4.1-content.html#r-practice",
    "title": "4.1 — Multivariate OLS Estimators — Class Content",
    "section": " R Practice",
    "text": "R Practice\nToday you will be working on R practice problems on multivariate regression. Answers will be posted later on that page."
  },
  {
    "objectID": "content/4.1-content.html#assignments",
    "href": "content/4.1-content.html#assignments",
    "title": "4.1 — Multivariate OLS Estimators — Class Content",
    "section": " Assignments",
    "text": "Assignments\n\nProblem Set 4 Due Fri Nov 11\nProblem Set 4 is due by the end of the day on Tuesday, November 11."
  },
  {
    "objectID": "content/4.1-content.html#happy-halloween",
    "href": "content/4.1-content.html#happy-halloween",
    "title": "4.1 — Multivariate OLS Estimators — Class Content",
    "section": "Happy Halloween!",
    "text": "Happy Halloween!"
  },
  {
    "objectID": "content/4.1-content.html#slides",
    "href": "content/4.1-content.html#slides",
    "title": "4.1 — Multivariate OLS Estimators — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/4.2-content.html#overview",
    "href": "content/4.2-content.html#overview",
    "title": "4.2 — Writing an Empirical Paper — Class Content",
    "section": " Overview",
    "text": "Overview\nToday we discuss the elements of a good empirical paper, with an eye towards your paper projects.\nI highly recommend you look at my example repositories:\n\n Workflow\n Example Empirical Paper\n\nThe first is one that I showed you back in class 1.5 when we talked about R Markdown and having a reproducible, plain-text, workflow. It has an example workflow setup that I recommend you follow: create an R project for your project, and organize all of your data, code, images, & text in this set of folders inside of your project.1 You can look at Example_paper.qmd to see my text and code in R markdown to write the paper, and look at Example_paper.qdf to see the final product.\nThe second is a quick and dirty example of what I am looking for in your paper project in this class, using the ongoing example of the effect of class size on test scores. You can also look at paper.qmd to see my text and code in R markdown, and paper.pdf to see the final product. Note that this is an example I put quickly together with minimal effort as an example for you all, it does not necessarily mean it is an “A” quality paper!\nIf you are using R markdown to write your final paper, you should consult class 1.5 again, along with the 1.5 “R practice” that deals with the workflow repository."
  },
  {
    "objectID": "content/4.2-content.html#readings",
    "href": "content/4.2-content.html#readings",
    "title": "4.2 — Writing an Empirical Paper — Class Content",
    "section": " Readings",
    "text": "Readings\n\n Ch. 5.1, 5.2, 5.4 in Bailey, Real Econometrics"
  },
  {
    "objectID": "content/4.2-content.html#r-practice",
    "href": "content/4.2-content.html#r-practice",
    "title": "4.2 — Writing an Empirical Paper — Class Content",
    "section": " R Practice",
    "text": "R Practice\nToday you will be finishing R practice problems on multivariate regression. Answers will be posted later on that page."
  },
  {
    "objectID": "content/4.2-content.html#assignments",
    "href": "content/4.2-content.html#assignments",
    "title": "4.2 — Writing an Empirical Paper — Class Content",
    "section": " Assignments",
    "text": "Assignments\n\nProblem Set 4 Due Fri Nov 11\nProblem Set 4 is due by the end of the day on Tuesday, November 11."
  },
  {
    "objectID": "content/4.2-content.html#slides",
    "href": "content/4.2-content.html#slides",
    "title": "4.2 — Writing an Empirical Paper — Class Content",
    "section": " Slides",
    "text": "Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. The lower button will allow you to download a PDF version of the slides.\nI suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n\n\n\n\n Download as PDF"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Overview",
    "section": "",
    "text": "Please note that the lesson numbers, topics, and titles (e.g. 1.1) are my design, and do not match up with the textbook!"
  },
  {
    "objectID": "credits.html",
    "href": "credits.html",
    "title": "Credits",
    "section": "",
    "text": "This website is written in R Markdown, built with Hugo, pushed to Github, and hosted by Netlify.\nThe theme is based on Academia-hugo, which I modified.\nThe inspiration for making individual course websites, along with the basic structure, comes from the incomparable Andrew Heiss. See his website for examples, as well as a number of helpful tutorials. Sadly, I had to figure out how these websites work on my own, and attempted to document my process in the README of this repository for those interested. One day I may make a guide.\nThe course hex sticker I made via hexSticker, and keep all of my course hex stickers in this repository."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ECON 480 — Econometrics",
    "section": "",
    "text": "Instructor\n\n   Dr. Ryan Safner\n   114 Rosenstock\n   Office Hours: MW 1:30-2:30PM\n   safner@hood.edu\n\n\n\nCourse details\n\n   MW 11:30 AM—12:55 PM\n   Rosenstock Trading Room\n   Aug 22—Dec 13, 2022\n   Slack"
  },
  {
    "objectID": "r/1.2-r.html",
    "href": "r/1.2-r.html",
    "title": "1.2 — Meet R — Practice",
    "section": "",
    "text": "Quarto file  R Studio Cloud"
  },
  {
    "objectID": "r/1.2-r.html#answers",
    "href": "r/1.2-r.html#answers",
    "title": "1.2 — Meet R — Practice",
    "section": "Answers",
    "text": "Answers\n\n Answer Key (html)   Answer Key (.qmd)"
  },
  {
    "objectID": "r/1.2-r.html#getting-set-up",
    "href": "r/1.2-r.html#getting-set-up",
    "title": "1.2 — Meet R — Practice",
    "section": "Getting Set Up",
    "text": "Getting Set Up\nFirst, create a new R project on your computer by choosing File -> New Project..., create a new directory (a folder somewhere on your computer), and then choose New Project and then give it a name like 1.2-R-Practice [This is a good time to practice not using spaces in file and folder names!].\nThen, download and open the Quarto file above and move it to the file folder on your computer that you just created for the R Project, and open it in R Studio. If there are any packages that you are missing (such as knitr or rmarkdown), a yellow banner on top may inform you Package X is required but is not installed, click Install to install them, or equivalently use the console to install.packages().\nAlternatively, you can do everything on RStudio.Cloud, which has its own R Project that includes the Quarto file."
  },
  {
    "objectID": "r/1.3-r.html",
    "href": "r/1.3-r.html",
    "title": "1.3 — Data Visualization — Practice",
    "section": "",
    "text": "Quarto file  R Studio Cloud"
  },
  {
    "objectID": "r/1.3-r.html#answers",
    "href": "r/1.3-r.html#answers",
    "title": "1.3 — Data Visualization — Practice",
    "section": "Answers",
    "text": "Answers\n\n Answer Key (html)   Answer Key (.qmd)"
  },
  {
    "objectID": "r/1.3-r.html#getting-set-up",
    "href": "r/1.3-r.html#getting-set-up",
    "title": "1.3 — Data Visualization — Practice",
    "section": "Getting Set Up",
    "text": "Getting Set Up\nFirst, create a new R project on your computer by choosing File -> New Project..., create a new directory (a folder somewhere on your computer), and then choose New Project and then give it a name like 1.3-R-Practice [This is a good time to practice not using spaces in file and folder names!].\nThen, download and open the Quarto file above and move it to the file folder on your computer that you just created for the R Project, and open it in R Studio. If there are any packages that you are missing, a yellow banner on top may inform you Package X is required but is not installed, click Install to install them, or equivalently use the console to install.packages().\nAlternatively, you can do everything on RStudio.Cloud, which has its own R Project that includes the Quarto file."
  },
  {
    "objectID": "r/1.4-r.html",
    "href": "r/1.4-r.html",
    "title": "1.4 — Data Wrangling — Practice",
    "section": "",
    "text": "Quarto file  R Studio Cloud"
  },
  {
    "objectID": "r/1.4-r.html#answers",
    "href": "r/1.4-r.html#answers",
    "title": "1.4 — Data Wrangling — Practice",
    "section": "Answers",
    "text": "Answers\n\n Answer Key (html)   Answer Key (.qmd)"
  },
  {
    "objectID": "r/1.4-r.html#getting-set-up",
    "href": "r/1.4-r.html#getting-set-up",
    "title": "1.4 — Data Wrangling — Practice",
    "section": "Getting Set Up",
    "text": "Getting Set Up\nFirst, create a new R project on your computer by choosing File -> New Project..., create a new directory (a folder somewhere on your computer), and then choose New Project and then give it a name like 1.4-R-Practice [This is a good time to practice not using spaces in file and folder names!].\nThen, download and open the Quarto file above and move it to the file folder on your computer that you just created for the R Project, and open it in R Studio. If there are any packages that you are missing, a yellow banner on top may inform you Package X is required but is not installed, click Install to install them, or equivalently use the console to install.packages().\nAlternatively, you can do everything on RStudio.Cloud, which has its own R Project that includes the Quarto file."
  },
  {
    "objectID": "r/1.5-r.html",
    "href": "r/1.5-r.html",
    "title": "1.5 — Optimize Workflow — Practice",
    "section": "",
    "text": "We are going to clone my repository on Github into R Studio on your computer. Go to my workflow repository on github. Click the green code button. See there are several options. Feel free to try each of them out:\n\n\nOn Your Computer (Download): Download ZIP will download a zipped file to your computer containing the entire repository. You will need to unzip this (sometimes it’s automatic, as on a Mac), and then it will create a folder on your computer. Open up the workflow.Rproj file, which will open the project in R Studio.\nOn R Studio Cloud: Highlight the url code (or click the copy button on the right). On Rstudio Cloud, click on the arrow on the right of the New Project button and select New Project from GitHub Repository. Paste the url you copied from Github here. This will open the repository as a new project in your cloud workspace\nOn Your Computer (via Git) ADVANCED: If you have git installed (see the ultimate guide for help doing this), copy the url code on GitHub. Next, go to R Studio on your computer, click File -> New Project -> Version Control -> Git and type paste the url in the first field. The final field is where on your computer you want to save this folder. It will then open up the project in R Studio."
  },
  {
    "objectID": "r/1.5-r.html#part-2",
    "href": "r/1.5-r.html#part-2",
    "title": "1.5 — Optimize Workflow — Practice",
    "section": "Part 2",
    "text": "Part 2\nNow that you have the workflow project on your computer (or cloud), let’s explore it. Notice in the file viewer pane in the bottom right, there are several folders and files there. Look at the top of the pane is the file path (this is where you can find this folder on your computer). Mine looks like this, for example:\n\nThe nice thing with projects is that any files you open or save are in this folder on your computer!"
  },
  {
    "objectID": "r/1.5-r.html#part-3",
    "href": "r/1.5-r.html#part-3",
    "title": "1.5 — Optimize Workflow — Practice",
    "section": "Part 3",
    "text": "Part 3\nOpen up Example_paper.qmd. This is an example of a file I would use to write a paper. Note the parts of the .Rmd file: - The yaml at the top - The text written in markdown - R chunks scattered throughout"
  },
  {
    "objectID": "r/1.5-r.html#part-4",
    "href": "r/1.5-r.html#part-4",
    "title": "1.5 — Optimize Workflow — Practice",
    "section": "Part 4",
    "text": "Part 4\nI have this set to Render to produce a PDF. For you to do this, you will need to install a distribution of LaTeX. Unless you intend to use LaTeX in the future (for math classes, going to graduate school, or you are a masochist…), we can get around this with a lightweight version we can install inside of R. Run the following code in the console:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nThis will probably take a few minutes to install. Like any package, you only ever have to do this once! Once this is complete, now try to Render Example_paper.qmd to PDF by clicking the Render button at the top. View your PDF that pops up!1"
  },
  {
    "objectID": "r/1.5-r.html#part-5",
    "href": "r/1.5-r.html#part-5",
    "title": "1.5 — Optimize Workflow — Practice",
    "section": "Part 5",
    "text": "Part 5\nNow let’s learn more about writing with markdown syntax. Complete this brief tutorial to practice!"
  },
  {
    "objectID": "r/1.5-r.html#part-6",
    "href": "r/1.5-r.html#part-6",
    "title": "1.5 — Optimize Workflow — Practice",
    "section": "Part 6",
    "text": "Part 6\nJust practice working in the Example_paper.qmd file. Create a new R chunk (anywhere) and write some R code to open data/clean_data.csv (which you can find by clicking through the folders too) and save it as an R object. (You may need to load tidyverse!) Run only this chunk by clicking the green play button at the upper right corner of the chunk. Make sure it works. Using projects to organize your files is much simpler than worrying about your working directory!"
  },
  {
    "objectID": "r/1.5-r.html#part-7",
    "href": "r/1.5-r.html#part-7",
    "title": "1.5 — Optimize Workflow — Practice",
    "section": "Part 7",
    "text": "Part 7\nFind my slides on the class github page. Note there is a lot going on here, because this is a pretty full website. They are in the slides folder. Now if you are ever curious how I did something in my slides, you can see the source .qmd files. Note the slides are quite advanced - I use a fair bit of html and css formatting to make them pretty!"
  },
  {
    "objectID": "r/1.5-r.html#part-8",
    "href": "r/1.5-r.html#part-8",
    "title": "1.5 — Optimize Workflow — Practice",
    "section": "Part 8",
    "text": "Part 8\nNow we are just exposing you to more .qmd files. Look at the answer key for 1.4 R practice and download and open the  markdown file. Take a look through it and see how it works, then when you are ready, click the Render button. It will make the html webpage (which you can open in any browser) that is identical to the answer key web page I made!"
  },
  {
    "objectID": "r/2.5-r.html",
    "href": "r/2.5-r.html",
    "title": "2.5 — Precision and Diagnostics — Practice",
    "section": "",
    "text": "Quarto file  R Studio Cloud"
  },
  {
    "objectID": "r/2.5-r.html#answers",
    "href": "r/2.5-r.html#answers",
    "title": "2.5 — Precision and Diagnostics — Practice",
    "section": "Answers",
    "text": "Answers\n\n Answer Key (html)   Answer Key (.qmd)"
  },
  {
    "objectID": "r/2.5-r.html#getting-set-up",
    "href": "r/2.5-r.html#getting-set-up",
    "title": "2.5 — Precision and Diagnostics — Practice",
    "section": "Getting Set Up",
    "text": "Getting Set Up\nFirst, create a new R project on your computer by choosing File -> New Project..., create a new directory (a folder somewhere on your computer), and then choose New Project and then give it a name like 2.5-R-Practice [This is a good time to practice not using spaces in file and folder names!].\nThen, download and open the Quarto file above and move it to the file folder on your computer that you just created for the R Project, and open it in R Studio. If there are any packages that you are missing, a yellow banner on top may inform you Package X is required but is not installed, click Install to install them, or equivalently use the console to install.packages().\nAlternatively, you can do everything on RStudio.Cloud, which has its own R Project that includes the Quarto file."
  },
  {
    "objectID": "r/2.7-r.html",
    "href": "r/2.7-r.html",
    "title": "2.7 — Hypothesis Testing for Regression — Practice",
    "section": "",
    "text": "Quarto file R Studio Cloud"
  },
  {
    "objectID": "r/2.7-r.html#answers",
    "href": "r/2.7-r.html#answers",
    "title": "2.7 — Hypothesis Testing for Regression — Practice",
    "section": "Answers",
    "text": "Answers\n\n Answer Key (html)   Answer Key (.qmd)"
  },
  {
    "objectID": "r/2.7-r.html#getting-set-up",
    "href": "r/2.7-r.html#getting-set-up",
    "title": "2.7 — Hypothesis Testing for Regression — Practice",
    "section": "Getting Set Up",
    "text": "Getting Set Up\nFirst, create a new R project on your computer by choosing File -> New Project..., create a new directory (a folder somewhere on your computer), and then choose New Project and then give it a name like 2.7-R-Practice [This is a good time to practice not using spaces in file and folder names!].\nThen, download and open the Quarto file above and move it to the file folder on your computer that you just created for the R Project, and open it in R Studio. If there are any packages that you are missing, a yellow banner on top may inform you Package X is required but is not installed, click Install to install them, or equivalently use the console to install.packages().\nAlternatively, you can do everything on RStudio.Cloud, which has its own R Project that includes the Quarto file."
  },
  {
    "objectID": "r/3.2-r.html",
    "href": "r/3.2-r.html",
    "title": "3.2 — DAGs — Practice",
    "section": "",
    "text": "Answer Key (html)   Quarto file\n\nFor each of the following examples:\n\nWrite out all of the causal pathways from X (treatment of interest) to Y (outcome of interest).\nIdentify which variable(s) need to be controlled to estimate the causal effect of X on Y. You can use dagitty.net to help you, but you should start trying to recognize these on your own!\nDraw the DAGs in r using ggdag. After setting up the dag with dagify() (and specifying exposure and outcome inside dagify), pipe that into ggdag(). Try again piping it instead into ggdag_status() (to highlight what is X and what is Y). Try again piping it instead into ggdag_adjustment_set() to show what needs to be controlled.\n\nDon’t forget to install ggdag!"
  },
  {
    "objectID": "r/3.2-r.html#question-1",
    "href": "r/3.2-r.html#question-1",
    "title": "3.2 — DAGs — Practice",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "r/3.2-r.html#question-2",
    "href": "r/3.2-r.html#question-2",
    "title": "3.2 — DAGs — Practice",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "r/3.2-r.html#question-3",
    "href": "r/3.2-r.html#question-3",
    "title": "3.2 — DAGs — Practice",
    "section": "Question 3",
    "text": "Question 3"
  },
  {
    "objectID": "r/3.2-r.html#question-4",
    "href": "r/3.2-r.html#question-4",
    "title": "3.2 — DAGs — Practice",
    "section": "Question 4",
    "text": "Question 4"
  },
  {
    "objectID": "r/3.2-r.html#question-5",
    "href": "r/3.2-r.html#question-5",
    "title": "3.2 — DAGs — Practice",
    "section": "Question 5",
    "text": "Question 5"
  },
  {
    "objectID": "r/4.1-r.html",
    "href": "r/4.1-r.html",
    "title": "4.1 — Multivariate OLS Estimators — Practice",
    "section": "",
    "text": "Quarto file R Studio Cloud\nDownload and read in (read_csv) the data below.\nThis data comes from a paper by Makowsky and Strattman (2009) that we will examine later. Even though state law sets a formula for tickets based on how fast a person was driving, police officers in practice often deviate from that formula. This dataset includes information on all traffic stops. An amount for the fine is given only for observations in which the police officer decided to assess a fine. There are a number of variables in this dataset, but the one’s we’ll look at are:\nWe want to explore who gets fines, and how much. We’ll come back to the other variables (which are categorical) in this dataset in later lessons."
  },
  {
    "objectID": "r/4.1-r.html#question-1",
    "href": "r/4.1-r.html#question-1",
    "title": "4.1 — Multivariate OLS Estimators — Practice",
    "section": "Question 1",
    "text": "Question 1\nHow does the age of a driver affect the amount of the fine? Make a scatterplot of the Amount of the fine (y) and the driver’s Age (x) along with a regression line."
  },
  {
    "objectID": "r/4.1-r.html#question-2",
    "href": "r/4.1-r.html#question-2",
    "title": "4.1 — Multivariate OLS Estimators — Practice",
    "section": "Question 2",
    "text": "Question 2\nNext, we’ll want to find the correlation between Amount and Age. Do this first.\nThen notice that it won’t work. This is because there are a lot of NAs (missing data) for Amount (if tried to get the mean() of Amount, it would do the same thing.\nYou can verify the NAs with:\n\ndata %>% # use your named dataframe!\n  select(Amount) %>%\n  summary()\n\n# OR\n# data %>% count(Amount) # but this has a lot of rows!\n\nIn order to run a correlation, we need to drop or ignore all of the NAs. You could filter() the data:\n\n# this would OVERWRITE data\ndata <- data %>%\n  filter(!is.na(Amount)) # remove all NAs\n\nOr, if you don’t want to change your data, the cor() command allows you to set use = \"pairwise.complete.obs\" as an argument."
  },
  {
    "objectID": "r/4.1-r.html#question-3",
    "href": "r/4.1-r.html#question-3",
    "title": "4.1 — Multivariate OLS Estimators — Practice",
    "section": "Question 3",
    "text": "Question 3\nWe want to estimate the following model:\n\\[\\widehat{\\text{Amount}_i}= \\hat{\\beta_0}+\\hat{\\beta_1}\\text{Age}_i\\]\nRun a regression, and save it as an object. Then get a summary() of it.\n\nPart A\nWrite out the estimated regression equation.\n\n\nPart B\nWhat is \\(\\hat{\\beta_0}\\) for this model? What does it mean in the context of our question?\n\n\nPart C\nWhat is \\(\\hat{\\beta_1}\\) for this model? What does it mean in the context of our question?\n\n\nPart D\nWhat is the marginal effect of Age on Amount?"
  },
  {
    "objectID": "r/4.1-r.html#question-4",
    "href": "r/4.1-r.html#question-4",
    "title": "4.1 — Multivariate OLS Estimators — Practice",
    "section": "Question 4",
    "text": "Question 4\nRedo question 3 with the broom package. Try out tidy() and glance(). This is just to keep you versatile!"
  },
  {
    "objectID": "r/4.1-r.html#question-5",
    "href": "r/4.1-r.html#question-5",
    "title": "4.1 — Multivariate OLS Estimators — Practice",
    "section": "Question 5",
    "text": "Question 5\nHow big would the difference in expected fine be for two drivers, one 18 years old and one 40 years old?"
  },
  {
    "objectID": "r/4.1-r.html#question-6",
    "href": "r/4.1-r.html#question-6",
    "title": "4.1 — Multivariate OLS Estimators — Practice",
    "section": "Question 6",
    "text": "Question 6\nNow run the regression again, controlling for speed (MPHover).\n\nPart A\nWrite the new regression equation.\n\n\nPart B\nWhat is the marginal effect of Age on Amount? What happened to it, compared to Question 3D?\n\n\nPart C\nWhat is the marginal effect of MPHover on Amount?\n\n\nPart D\nWhat is \\(\\hat{\\beta_0}\\) for our model, and what does it mean in the context of our question?\n\n\nPart E\nWhat is the adjusted \\(\\bar{R}^2\\)? What does it mean?"
  },
  {
    "objectID": "r/4.1-r.html#question-7",
    "href": "r/4.1-r.html#question-7",
    "title": "4.1 — Multivariate OLS Estimators — Practice",
    "section": "Question 7",
    "text": "Question 7\nNow suppose both the 18 year old and the 40 year old each went 10 MPH over the speed limit. How big would the difference in expected fine be for the two drivers?"
  },
  {
    "objectID": "r/4.1-r.html#question-8",
    "href": "r/4.1-r.html#question-8",
    "title": "4.1 — Multivariate OLS Estimators — Practice",
    "section": "Question 8",
    "text": "Question 8\nWhat is the difference in expected fine between two 18 year-olds, one who went 10 MPH over, and one who went 30 MPH over?"
  },
  {
    "objectID": "r/4.1-r.html#question-9",
    "href": "r/4.1-r.html#question-9",
    "title": "4.1 — Multivariate OLS Estimators — Practice",
    "section": "Question 9",
    "text": "Question 9\nUse the modelsummary package’s modelsummary() command to make a regression table of your two regressions: the one from question 3, and the one from question 6."
  },
  {
    "objectID": "r/4.1-r.html#question-10",
    "href": "r/4.1-r.html#question-10",
    "title": "4.1 — Multivariate OLS Estimators — Practice",
    "section": "Question 10",
    "text": "Question 10\nAre our two independent variables multicollinear? Do younger people tend to drive faster?\n\nPart A\nGet the correlation between Age and MPHover.\n\n\nPart B\nMake a scatterplot of MPHover (y) on Age (x).\n\n\nPart C\nRun an auxiliary regression of MPHover on Age.\n\n\nPart D\nInterpret the coefficient on Age from this regression.\n\n\nPart E\nLook at your regression table in question 10. What happened to the standard error on Age? Why (consider the formula for variance of \\(\\hat{\\beta_1})\\)?\n\n\nPart F\nCalculate the Variance Inflation Factor (VIF) using the car package’s vif() command. Run it on your regression object saved from Question 7.\n\n\nPart G\nCalculate the VIF manually, using what you learned in this question."
  },
  {
    "objectID": "r/4.1-r.html#question-11",
    "href": "r/4.1-r.html#question-11",
    "title": "4.1 — Multivariate OLS Estimators — Practice",
    "section": "Question 11",
    "text": "Question 11\nLet’s now think about the omitted variable bias. Suppose the “true” model is the one we ran from Question 7.\n\nPart A\nDo you suppose that MPHover fits the two criteria for omitted variable bias?\n\n\nPart B\nLook at the regression we ran in Question 4. Consider this the “omitted” regression, where we left out MPHover. Does our estimate of the marginal effect of Age on Amount overstate or understate the true marginal effect?\n\n\nPart C\nUse the “true” model (Question 7), the “omitted” regression (Question 3), and our “auxiliary” regression (Question 10) to identify each of the following parameters that describe our biased estimate of the marginal effect of Age on Amount:\n\\[\\alpha_1=\\beta_1+\\beta_2\\delta_1\\]\nSee the notation I used in class.\n\n\nPart D\nFrom your answer in part C, how large is the omitted variable bias from leaving out MPHover?"
  },
  {
    "objectID": "r/4.1-r.html#question-12",
    "href": "r/4.1-r.html#question-12",
    "title": "4.1 — Multivariate OLS Estimators — Practice",
    "section": "Question 12",
    "text": "Question 12\nMake a coefficient plot of your coefficients from the regression in Question 6. The package modelsummary (which you will need to install and load) has a great command modelplot() to do this on your regression object."
  },
  {
    "objectID": "resources/R-packages.html",
    "href": "resources/R-packages.html",
    "title": "R Packages Used in this Course",
    "section": "",
    "text": "You can install all of these packages at once with the following command:\n\ninstall.packages(c(\"tidyverse\", \"ggrepel\", \"broom\", \"car\", \"estimatr\", \"lmtest\",\n                   \"huxtable\", \"infer\", \"dagitty\", \"ggdag\", \"modelsummary\", \"fixest\"))\n\n† Indicates package is part of the tidyverse\n\n\n\nName\nType\nDescription/Reason(s) for Use\nClasses Used\n\n\n\n\nggplot2†\nPlotting\nFor nice plots\n[1.3]\n\n\ngganimate\nPlotting\nFor animating plots\n[1.3]\n\n\nhaven†\nData Wrangling\nFor importing nonstandard data files\n[1.4]\n\n\ndplyr†\nData Wrangling\nFor manipulating data (part of tidyverse)\n[1.4]\n\n\nreadr†\nData Wrangling\nFor importing most data files\n[1.4]\n\n\ntidyr†\nData Wrangling\nFor reshaping data (wide and long)\n[1.4]\n\n\nmagrittr†\nData Wrangling\nFor the pipe\n[1.4]\n\n\ntibble†\nData Wrangling\nFor a friendlier data.frame\n[1.4]\n\n\nggrepel\nPlotting\nFor annotating text that doesn’t cover observations\n[1.4]\n\n\nbroom\nModels\nFor tidying regression output\n[2.3]\n\n\ncar\nModels\nFor testing for outliers\n[2.5]\n\n\nestimatr\nModels\nFor calculating heteroskedasticity-robust standard errors\n[2.5]\n\n\nlmtest\nModels\nFor testing for heteroskedasticity\n[2.5]\n\n\nhuxtable\nOutput\nFor making nice regression tables\n[2.5]\n\n\ninfer\nModels\nFor simulation and statistical inference\n[2.6]\n\n\ndagitty\nModels\nFor working with DAGs in R\n[3.2]\n\n\nggdag\nPlotting\nFor plotting DAGs in ggplot\n[3.2]\n\n\nmodelsummary\nOutput\nFor making nice regression tables\n[3.5]\n\n\nfixest\nModels\nFor working with panel data\n[4.1]\n\n\n\n\n\n\n\nFootnotes\n\n\nNote, many of these packages have multiple uses beyond our purposes!↩︎"
  },
  {
    "objectID": "resources/appendices/2.1-appendix.html",
    "href": "resources/appendices/2.1-appendix.html",
    "title": "2.1 — Data 101 & Descriptive Statistics — Appendix",
    "section": "",
    "text": "Footnotes\n\n\nFor more beyond the mere definition, see the appendix on Covariance and Correlation↩︎"
  },
  {
    "objectID": "resources/appendices/2.2-appendix.html",
    "href": "resources/appendices/2.2-appendix.html",
    "title": "2.2 — Random Variables & Distributions — Appendix",
    "section": "",
    "text": "There are several useful mathematical properties of expected value and variance.\nProperty 1: the expected value of a constant is itself, and the variance of a constant is 0.\n\\[\\begin{align*}\nE(c)&=c\\\\\nvar(c)&=0\\\\\nsd(c)&=0\\\\\n\\end{align*}\\]\nFor any constant, \\(c\\)\n\nExample: \\(E(2)=2\\), \\(var(2)=0\\), \\(sd(2)=0\\)\n\nProperty 2: adding or subtracting a constant to a random variable and then taking the mean or variance:\n\\[\\begin{align*}\nE(X \\pm c)&=E(X) \\pm c\\\\\nvar(X \\pm c)&=X\\\\\nsd(X \\pm c)&=X\\\\\n\\end{align*}\\]\nFor any constant, \\(c\\)\n\nExample: \\(E(2+X)=2+E(X)\\), \\(var(2+X)=var(X)\\), \\(sd(2+X)=sd(X)\\)\n\nProperty 3: multiplying a constant to a random variable and then taking the mean or variance:\n\\[\\begin{align*}\nE(aX)&=E(X) aE(X)\\\\\nvar(aX)&=a^2var(X)\\\\\nsd(aX)&=|a|sd(X)\\\\\n\\end{align*}\\]\nFor any constant, \\(a\\)\n\nExample: \\(E(2X)=2E(X)\\), \\(var(2X)=4var(X)\\), \\(sd(2X)=2sd(X)\\)\n\nProperty 4: the expected value of the sum of two random variables is equal to the sum of each random variable’s expected value:\n\\[E(X \\pm Y)=E(X) \\pm E(Y)\\]"
  },
  {
    "objectID": "resources/appendices/2.2-appendix.html#creating-mathematical-functions-in-r",
    "href": "resources/appendices/2.2-appendix.html#creating-mathematical-functions-in-r",
    "title": "2.2 — Random Variables & Distributions — Appendix",
    "section": "Creating Mathematical Functions in R",
    "text": "Creating Mathematical Functions in R\nYou can create custom mathematical functions using mosaic by defining an R function() with multiple arguments. As a simple example, make the function \\(f(x) = 10x-x^2\\) (with one argument, \\(x\\) since it is a univariate function) as follows:\n\n# store as a named function, I'll call it \"my_function\"\nmy_function <- function(x){10*x - x^2}\n\n# look at it\nmy_function\n\nfunction(x){10*x - x^2}\n\n\nThere are some notational requirements from R for making functions. Any coefficient in front of a variable (such as the 10 in 10x must be explicitly multiplied by the variable, as in 10*x).\nTo use the function to calculate its value at a particular value of x, simply define what the (x) is and run your named function on it:\n\n# f of 2 \nmy_function(2)\n\n[1] 16\n\n# f of 2 and 4\nmy_function(c(2,4))\n\n[1] 16 24\n\n# f of 2 through 7\nmy_function(2:7)\n\n[1] 16 21 24 25 24 21\n\n# ALTERNATIVELY\n# define x first as a vector and then run function on it\n\nx <- c(2,4)\nmy_function(x)\n\n[1] 16 24"
  },
  {
    "objectID": "resources/appendices/2.2-appendix.html#graphing-mathematical-and-statistical-functions-in-r",
    "href": "resources/appendices/2.2-appendix.html#graphing-mathematical-and-statistical-functions-in-r",
    "title": "2.2 — Random Variables & Distributions — Appendix",
    "section": "Graphing Mathematical and Statistical Functions in R",
    "text": "Graphing Mathematical and Statistical Functions in R\nIn ggplot there is a dedicated stat_function() (equivalent to a geom_ layer) to graph mathematical and statistical functions. All that is needed is a data.frame of a range of x values to act as the source for data, and set x equal to those values for aesthetics.\n\nlibrary(tidyverse)\n# x values are integers 1 through 10\nggplot(data = tibble(x = 1:10))+\n  aes(x = x)\n\n\n\n\nThen we add the stat_function, where fun = is the most important argument where you define the to function to graph as your function created above, for example, our my_function.\n\nggplot(data = tibble(x = 1:10))+\n  aes(x = x)+\n  stat_function(fun = my_function) \n\n\n\n\nYou can also adjust things like size, color, and line type.\n\nggplot(data = tibble(x = 1:10))+\n  aes(x = x)+\n  stat_function(fun = my_function,\n                color = \"blue\",\n                size = 2,\n                linetype = \"dashed\") \n\n\n\n\n\nBult-in Statistical Functions\nThere are some standard statistical distributions built into R. They require a combination of a specific prefix and a distribution.\nPrefixes:\n\n\n\nAction/Type\nPrefix\n\n\n\n\nrandom draw\nr\n\n\ndensity (pdf)\nd\n\n\ncumulative density (cdf)\np\n\n\nquantile (inverse cdf)\nq\n\n\n\nDistributions:\n\n\n\nDistribution\nName in R\n\n\n\n\nNormal\nnorm\n\n\nUniform\nunif\n\n\nStudent’s t\nt\n\n\nBinomial\nbinom\n\n\nNegative binomial\nnbinom\n\n\nHypergeometric\nhyper\n\n\nWeibull\nweibull\n\n\nBeta\nbeta\n\n\nGamma\ngamma\n\n\n\nThus, what you want is a combination of the prefix and the distribution.\n\n\nSome common examples:\n\nTake random draws from a normal distribution:\n\n\nrnorm(n = 10, # take 10 draws from a normal distribution with:\n      mean = 2, # mean of 2\n      sd = 1) # sd of 1\n\n [1] 3.4582468 2.0740456 0.9092114 1.1790483 2.1474758 3.0726526 3.6894126\n [8] 3.0430861 1.9429537 0.5569242\n\n\n\nGet probability of a random variable being less than or equal to a value (cdf) from a normal distribution:\n\n\n# find probability of area to the LEFT of a number on pdf (note this = cdf of that number!)\npnorm(q = 80, # number is 80 from a distribution where: \n      mean = 200, # mean is 100\n      sd = 100, # sd is 100\n      lower.tail = TRUE) # looking to the LEFT in lower tail\n\n[1] 0.1150697\n\n\n\nFind the value of a distribution that is a specified percentile.\n\n\n# find the 38th percentile value\nqnorm(p = 0.38, # 38th percentile from a distribution where:\n      mean = 200, # mean is 200\n      sd = 100) # sd is 100\n\n[1] 169.4519\n\n\n\n\nGraphing Statistical Functions\nYou can also graph these commonly used statistical functions by setting fun = the named functions in your stat_function() layer. If you want to specify the mean and standard deviation, use args = list() to include the required arguments from the named function above (e.g. dnorm needs mean and sd).\n\nggplot(data = tibble(x = -400:600))+\n  aes(x = x)+\n  stat_function(fun = dnorm,\n                args = list(mean = 200, sd = 200),\n                color = \"blue\",\n                size = 2,\n                linetype = \"dashed\") \n\n\n\n\nIf you don’t include this, it will graph the standard distribution:\n\nggplot(data = tibble(x = -4:4))+\n  aes(x = x)+\n  stat_function(fun = dnorm,\n                color = \"blue\",\n                size = 2,\n                linetype = \"dashed\") \n\n\n\n\nTo add shading under a distribution, simply add a duplicate of the stat_function() layer, but add geom=\"area\" to indicate the area beneath the function should be filled, and you can limit the domain of the fill with xlim=c(start,end), where start and end are the x-values for the endpoints of the fill.\n\n# graph normal distribution and shade area between -2 and 2\nggplot(data = tibble(x = -4:4))+\n  aes(x = x)+\n  # graph the curve\n  stat_function(fun = dnorm,\n                color = \"blue\",\n                size = 2,\n                linetype = \"dashed\")+\n  # shade area under curve (between -2 and 2)\n  stat_function(fun = dnorm,\n                xlim = c(-2,2),\n                geom = \"area\",\n                fill = \"green\",\n                alpha = 0.5)\n\n\n\n\nHence, here is one graph from my slides:\n\nggplot(data = tibble(x=35:115))+\n  aes(x = x)+\n  stat_function(fun = dnorm,\n                args = list(mean = 75, sd = 10),\n                geom = \"area\",\n                size = 2,\n                fill = \"gray\",\n                alpha = 0.5)+\n  stat_function(fun = dnorm,\n                args = list(mean = 75, sd = 10),\n                geom = \"area\",\n                xlim = c(65,85),\n                fill = \"#e64173\")+\n  labs(x = \"X\",\n       y = \"Probability\")+\n  scale_x_continuous(breaks = seq(35,115,5))+\n    scale_y_continuous(limits = c(0,0.045),\n                     expand = c(0,0))+\n  theme_classic(base_family = \"Fira Sans Condensed\",\n           base_size = 20)"
  },
  {
    "objectID": "resources/appendices/2.3-appendix.html",
    "href": "resources/appendices/2.3-appendix.html",
    "title": "2.3 — Simple Linear Regression — Appendix",
    "section": "",
    "text": "Recall the variance of a discrete random variable \\(X\\), denoted \\(var(X)\\) or \\(\\sigma^2\\), is the expected value (probability-weighted average) of the squared deviations of \\(X_i\\) from it’s mean (or expected value) \\(\\bar{X}\\) or \\(E(X)\\).1\n\\[\\begin{align*}\n\\sigma_X^2&=E(X-E(X))\\\\\n&=\\sum^n_{i=1} (X_i-\\bar{X})^2 p_i\n\\end{align*}\\]\nFpr continuous data (if all possible values of \\(X_i\\) are equally likely or we don’t know the probabilities), we can write variance as a simple average of squared deviations from the mean:\n\\[\\begin{align*}\n\\sigma_X^2&=\\frac{1}{n}\\sum^n_{i=1}(X_i-\\bar{X})^2  \n\\end{align*}\\]\nVariance has some useful properties:\nProperty 1: The variance of a constant is 0\n\\[var(c)=0 \\text{ iff } P(X=c)=1\\]\nIf a random variable takes the same value (e.g. 2) with probability 1.00, \\(E(2)=2\\), so the average squared deviation from the mean is 0, because there are never any values other than 2.\nProperty 2: The variance is unchanged for a random variable plus/minus a constant\n\\[var(X\\pm c)\\]\nSince the variance of a constant is 0.\nProperty 3: The variance of a scaled random variable is scaled by the square of the coefficient\n\\[var(aX)=a^2var(X)\\]\nProperty 4: The variance of a linear transformation of a random variable is scaled by the square of the coefficient\n\\[var(aX+b)=a^2var(X)\\]"
  },
  {
    "objectID": "resources/appendices/2.3-appendix.html#covariance",
    "href": "resources/appendices/2.3-appendix.html#covariance",
    "title": "2.3 — Simple Linear Regression — Appendix",
    "section": "Covariance",
    "text": "Covariance\nFor two random variables, \\(X\\) and \\(Y\\), we can measure their covariance (denoted \\(cov(X,Y)\\) or \\(\\sigma_{X,Y}\\))2 to quantify how they vary together. A good way to think about this is: when \\(X\\) is above its mean, would we expect \\(Y\\) to also be above its mean (and covary positively), or below its mean (and covary negatively). Remember, this is describing the joint probability distribution for two random variables.\n\\[\\begin{align*}\n\\sigma_{X,Y}&=E\\big[(X-\\bar{X})(Y-\\bar{Y})\\big]\n\\end{align*}\\]\nAgain, in the case of equally probable values for both \\(X\\) and \\(Y\\), covariance is sometimes written:\n\\[\\begin{align*}\n\\sigma_{X,Y}&=\\frac{1}{N}\\sum_{i=1}^n(X-\\bar{X})(Y-\\bar{Y})\n\\end{align*}\\]\nCovariance also has a number of useful properties:\nProperty 1: The covariance of a random variable \\(X\\) and a constant \\(c\\) is 0\n\\[cov(X,c)=0\\]\nProperty 2: The covariance of a random variable and itself is the variable’s variance\n\\[\\begin{align*}\n    cov(X,X)&=var(X)\\\\\n    \\sigma_{X,X}&=\\sigma^2_X\\\\\n    \\end{align*}\\]\nProperty 3: The covariance of a two random variables \\(X\\) and \\(Y\\) each scaled by a constant \\(a\\) and \\(b\\) is the product of the covariance and the constants\n\\[cov(aX,bY)=a\\times b \\times cov(X,Y)\\]\nProperty 4: If two random variables are independent, their covariance is 0\n\\[cov(X,Y)=0 \\text{ iff } X \\text{ and } Y \\text{ are independent:}  E(XY)=E(X)\\times E(Y)\\]"
  },
  {
    "objectID": "resources/appendices/2.3-appendix.html#correlation",
    "href": "resources/appendices/2.3-appendix.html#correlation",
    "title": "2.3 — Simple Linear Regression — Appendix",
    "section": "Correlation",
    "text": "Correlation\nCovariance, like variance, is often cumbersome, and the numerical value of the covariance of two random variables does not really mean much. It is often convenient to normalize the covariance to a decimal between \\(-1\\) and 1. We do this by dividing by the product of the standard deviations of \\(X\\) and \\(Y\\). This is known as the correlation coefficient between \\(X\\) and \\(Y\\), denoted \\(corr(X,Y)\\) or \\(\\rho_{X,Y}\\) (for populations) or \\(r_{X,Y}\\) (for samples):\n\\[\\begin{align*}  \nr_{X,Y}&=\\frac{cov(X,Y)}{sd(X)sd(Y)}\\\\\n&=\\frac{E\\big[(X-\\bar{X})(Y-\\bar{Y})\\big]}{\\sqrt{E\\big[X-\\bar{X}\\big]}\\sqrt{E\\big[Y-\\bar{Y}\\big]}}\\\\\n&=\\frac{\\sigma_{X,Y}}{\\sigma_X \\sigma_Y}\\\\\n\\end{align*}\\]\nNote this also means that covariance is the product of the standard deviation of \\(X\\) and \\(Y\\) and their correlation coefficient:\n\\[\\begin{align*}\n\\sigma_{X,Y}&=r_{X,Y}\\sigma_X \\sigma_Y  \\\\\ncov(X,Y)&=corr(X,Y)\\times sd(X) \\times sd(Y)    \\\\\n\\end{align*}\\]\nAnother way to reach the (sample) correlation coefficient is by finding the average joint \\(Z\\)-score of each pair of \\((X_i,Y_i)\\):\n\\[\\begin{align*}\nr_{X,Y}&=\\frac{1}{n}\\frac{\\displaystyle\\sum^n_{i=1}(X_i-\\bar{X})(Y_i-\\bar{Y}))}{s_X s_Y} && \\text{Definition of sample correlation}\\\\\n&=\\frac{1}{n}\\displaystyle\\sum^n_{i=1}\\bigg(\\frac{X_i-\\bar{X}}{s_X}\\bigg)\\bigg(\\frac{Y_i-\\bar{Y}}{s_Y}\\bigg) && \\text{Breaking into separate sums} \\\\\n&=\\frac{1}{n}\\displaystyle\\sum^n_{i=1}(Z_X)(Z_Y) && \\text{Recognize each sum is the z-score for that r.v.} \\\\\n\\end{align*}\\]\nCorrelation has some useful properties that should be familiar to you:\n\nCorrelation is between \\(-1\\) and 1\nA correlation of -1 is a downward sloping straight line\nA correlation of 1 is an upward sloping straight line\nA correlation of 0 implies no relationship\n\n\nCalculating Correlation Example\nWe can calculate the correlation of a simple data set (of 4 observations) using R to show how correlation is calculated. We will use the \\(Z\\)-score method. Begin with a simple set of data in \\((X_i, Y_i)\\) points:\n\\[ (1,1), (2,2), (3,4), (4,9) \\]\n\nlibrary(tidyverse)\n\ncorr_example <- tibble(x = c(1,2,3,4),\n                       y = c(1,2,4,9))\n\nggplot(data = corr_example)+\n  aes(x = x,\n      y = y)+\n  geom_point()\n\n\n\ncorr_example %>%\n  summarize(mean_x = mean(x), #find mean of x, its 2.5\n            sd_x = sd(x), #find sd of x, its 1.291\n            mean_y = mean(y), #find mean of y, its 4\n            sd_y = sd(y)) #find sd of y, its 3.559\n\n# A tibble: 1 × 4\n  mean_x  sd_x mean_y  sd_y\n   <dbl> <dbl>  <dbl> <dbl>\n1    2.5  1.29      4  3.56\n\n#take z score of x,y for each pair and multiply them\n\ncorr_example <- corr_example %>%\n  mutate(z_product = ((x - mean(x))/sd(x)) * ((y - mean(y))/sd(y)))\n\ncorr_example %>%\n  summarize(avg_z_product = sum(z_product)/(n() - 1), # average z products over n-1\n            actual_corr = cor(x,y), #compare our answer to actual cor() command!\n            covariance = cov(x,y)) # just for kicks, what's the covariance? \n\n# A tibble: 1 × 3\n  avg_z_product actual_corr covariance\n          <dbl>       <dbl>      <dbl>\n1         0.943       0.943       4.33"
  },
  {
    "objectID": "resources/appendices/2.4-appendix.html",
    "href": "resources/appendices/2.4-appendix.html",
    "title": "2.4 — Goodness of Fit and Bias — Appendix",
    "section": "",
    "text": "The population linear regression model is:\n\\[Y_i=\\beta_0+\\beta_1 X_i + u _i\\]\nThe errors \\((u_i)\\) are unobserved, but for candidate values of \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\), we can obtain an estimate of the residual. Algebraically, the error is:\n\\[\\hat{u_i}=    Y_i-\\hat{\\beta_0}-\\hat{\\beta_1}X_i\\]\nRecall our goal is to find \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) that minimizes the sum of squared errors (SSE):\n\\[SSE= \\sum^n_{i=1} \\hat{u_i}^2\\]\nSo our minimization problem is:\n\\[\\min_{\\hat{\\beta_0}, \\hat{\\beta_1}} \\sum^n_{i=1} (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1}X_i)^2\\]\nUsing calculus, we take the partial derivatives and set it equal to 0 to find a minimum. The first order conditions are:\n\\[\\begin{align*}\n\\frac{\\partial SSE}{\\partial \\hat{\\beta_0}}&=-2\\displaystyle\\sum^n_{i=1} (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1} X_i)=0\\\\\n\\frac{\\partial SSE}{\\partial \\hat{\\beta_1}}&=-2\\displaystyle\\sum^n_{i=1} (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1} X_i)X_i=0\\\\\n\\end{align*}\\]\n\n\nWorking with the first FOC, divide both sides by \\(-2\\):\n\\[\\displaystyle\\sum^n_{i=1} (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1} X_i)=0\\]\nThen expand the summation across all terms and divide by \\(n\\):\n\\[\\underbrace{\\frac{1}{n}\\sum^n_{i=1} Y_i}_{\\bar{Y}}-\\underbrace{\\frac{1}{n}\\sum^n_{i=1}\\hat{\\beta_0}}_{\\hat{\\beta_0}}-\\underbrace{\\frac{1}{n}\\sum^n_{i=1} \\hat{\\beta_1} X_i}_{\\hat{\\beta_1}\\bar{X}}=0\\]\nNote the first term is \\(\\bar{Y}\\), the second is \\(\\hat{\\beta_0}\\), the third is \\(\\hat{\\beta_1}\\bar{X}\\).1\nSo we can rewrite as:\n\\[\\bar{Y}-\\hat{\\beta_0}-\\beta_1=0\\]\nRearranging:\n\\[\\hat{\\beta_0}=\\bar{Y}-\\bar{X}\\beta_1\\]\n\n\n\nTo find \\(\\hat{\\beta_1}\\), take the second FOC and divide by \\(-2\\):\n\\[\\displaystyle\\sum^n_{i=1} (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1} X_i)X_i=0\\]\nFrom the formula for \\(\\hat{\\beta_0}\\), substitute in for \\(\\hat{\\beta_0}\\):\n\\[\\displaystyle\\sum^n_{i=1} \\bigg(Y_i-[\\bar{Y}-\\hat{\\beta_1}\\bar{X}]-\\hat{\\beta_1} X_i\\bigg)X_i=0\\]\nCombining similar terms:\n\\[\\displaystyle\\sum^n_{i=1} \\bigg([Y_i-\\bar{Y}]-[X_i-\\bar{X}]\\hat{\\beta_1}\\bigg)X_i=0\\]\nDistribute \\(X_i\\) and expand terms into the subtraction of two sums (and pull out \\(\\hat{\\beta_1}\\) as a constant in the second sum:\n\\[\\displaystyle\\sum^n_{i=1} [Y_i-\\bar{Y}]X_i-\\hat{\\beta_1}\\displaystyle\\sum^n_{i=1}[X_i-\\bar{X}]X_i=0\\]\nMove the second term to the righthand side:\n\\[\\displaystyle\\sum^n_{i=1} [Y_i-\\bar{Y}]X_i=\\hat{\\beta_1}\\displaystyle\\sum^n_{i=1}[X_i-\\bar{X}]X_i\\]\nDivide to keep just \\(\\hat{\\beta_1}\\) on the right:\n\\[\\frac{\\displaystyle\\sum^n_{i=1} [Y_i-\\bar{Y}]X_i}{\\displaystyle\\sum^n_{i=1}[X_i-\\bar{X}]X_i}=\\hat{\\beta_1}\\]\nNote that from the rules about summation operators:\n\\[\\displaystyle\\sum^n_{i=1} [Y_i-\\bar{Y}]X_i=\\displaystyle\\sum^n_{i=1} (Y_i-\\bar{Y})(X_i-\\bar{X})\\]\nand:\n\\[\\displaystyle\\sum^n_{i=1} [X_i-\\bar{X}]X_i=\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})(X_i-\\bar{X})=\\displaystyle\\sum^n_{i=1}(X_i-\\bar{X})^2\\]\nPlug in these two facts:\n\\[\\frac{\\displaystyle\\sum^n_{i=1} (Y_i-\\bar{Y})(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1}(X_i-\\bar{X})^2}=\\hat{\\beta_1}\\]"
  },
  {
    "objectID": "resources/appendices/2.4-appendix.html#algebraic-properties-of-ols-estimators",
    "href": "resources/appendices/2.4-appendix.html#algebraic-properties-of-ols-estimators",
    "title": "2.4 — Goodness of Fit and Bias — Appendix",
    "section": "Algebraic Properties of OLS Estimators",
    "text": "Algebraic Properties of OLS Estimators\nThe OLS residuals \\(\\hat{u}\\) and predicted values \\(\\hat{Y}\\) are chosen by the minimization problem to satisfy:\n\nThe expected value (average) error is 0:\n\n\\[E(u_i)=\\frac{1}{n}\\displaystyle \\sum_{i=1}^n \\hat{u_i}=0\\]\n\nThe covariance between \\(X\\) and the errors is 0:\n\n\\[\\hat{\\sigma}_{X,u}=0\\]\nNote the first two properties imply strict exogeneity. That is, this is only a valid model if \\(X\\) and \\(u\\) are not correlated.\n\nThe expected predicted value of \\(Y\\) is equal to the expected value of \\(Y\\):\n\n\\[\\bar{\\hat{Y}}=\\frac{1}{n} \\displaystyle\\sum_{i=1}^n \\hat{Y_i} = \\bar{Y}\\]\n\nTotal sum of squares is equal to the explained sum of squares plus sum of squared errors:\n\n\\[\\begin{align*}TSS&=ESS+SSE\\\\\n\\sum_{i=1}^n (Y_i-\\bar{Y})^2&=\\sum_{i=1}^n (\\hat{Y_i}-\\bar{Y})^2 + \\sum_{i=1}^n {u}^2\\\\\n\\end{align*}\\]\nRecall \\(R^2\\) is \\(\\frac{ESS}{TSS}\\) or \\(1-SSE\\)\n\nThe regression line passes through the point \\((\\bar{X},\\bar{Y})\\), i.e. the mean of \\(X\\) and the mean of \\(Y\\)."
  },
  {
    "objectID": "resources/appendices/2.4-appendix.html#bias-in-hatbeta_1",
    "href": "resources/appendices/2.4-appendix.html#bias-in-hatbeta_1",
    "title": "2.4 — Goodness of Fit and Bias — Appendix",
    "section": "Bias in \\(\\hat{\\beta_1}\\)",
    "text": "Bias in \\(\\hat{\\beta_1}\\)\nBegin with the formula we derived for \\(\\hat{\\beta_1}\\):\n\\[\\hat{\\beta_1}=\\frac{\\displaystyle \\sum^n_{i=1} (Y_i-\\bar{Y})(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}\\]\nRecall from Rule 6 of summations, we can rewrite the numerator as\n\\[\\begin{align*}\n    =&\\displaystyle \\sum^n_{i=1} (Y_i-\\bar{Y})(X_i-\\bar{X})\\\\\n    =& \\displaystyle \\sum^n_{i=1} Y_i(X_i-\\bar{X})\\\\\n\\end{align*}\\]\n\\[\\hat{\\beta_1}=\\frac{\\displaystyle \\sum^n_{i=1} Y_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}\\]\nWe know the true population relationship is expressed as:\n\\[Y_i=\\beta_0+\\beta_1 X_i+u_i\\]\nSubstituting this in for \\(Y_i\\) in equation 2:\n\\[\\hat{\\beta_1}=\\frac{\\displaystyle \\sum^n_{i=1} (\\beta_0+\\beta_1X_i+u_i)(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}\\]\nBreaking apart the sums in the numerator:\n\\[\\hat{\\beta_1}=\\frac{\\displaystyle \\sum^n_{i=1} \\beta_0(X_i-\\bar{X})+\\displaystyle \\sum^n_{i=1} \\beta_1X_i(X_i-\\bar{X})+\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}\\]\nWe can simplify equation 4 using Rules 4 and 5 of summations\n\nThe first term in the numerator \\(\\left[\\displaystyle \\sum^n_{i=1} \\beta_0(X_i-\\bar{X})\\right]\\) has the constant \\(\\beta_0\\), which can be pulled out of the summation. This gives us the summation of deviations, which add up to 0 as per Rule 4:\n\n\\[\\begin{align*}\n\\displaystyle \\sum^n_{i=1} \\beta_0(X_i-\\bar{X})&= \\beta_0 \\displaystyle \\sum^n_{i=1} (X_i-\\bar{X})\\\\\n&=\\beta_0 (0)\\\\\n&=0\\\\\n\\end{align*}\\]\n\nThe second term in the numerator \\(\\left[\\displaystyle \\sum^n_{i=1} \\beta_1X_i(X_i-\\bar{X})\\right]\\) has the constant \\(\\beta_1\\), which can be pulled out of the summation. Additionally, Rule 5 tells us \\(\\displaystyle \\sum^n_{i=1} X_i(X_i-\\bar{X})=\\displaystyle \\sum^n_{i=1}(X_i-\\bar{X})^2\\):\n\n\\[\\begin{align*}\n\\displaystyle \\sum^n_{i=1} \\beta_1X_1(X_i-\\bar{X})&= \\beta_1 \\displaystyle \\sum^n_{i=1} X_i(X_i-\\bar{X})\\\\\n&=\\beta_1\\displaystyle \\sum^n_{i=1}(X_i-\\bar{X})^2\\\\\n\\end{align*}\\]\nWhen placed back in the context of being the numerator of a fraction, we can see this term simplifies to just \\(\\beta_1\\):\n\\[\\begin{align*}\n    \\frac{\\beta_1\\displaystyle \\sum^n_{i=1}(X_i-\\bar{X})^2}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} &=\\frac{\\beta_1}{1} \\times \\frac{\\displaystyle \\sum^n_{i=1}(X_i-\\bar{X})^2}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}\\\\\n    &=\\beta_1   \\\\\n\\end{align*}\\]\nThus, we are left with:\n\\[\\hat{\\beta_1}=\\beta_1+\\frac{\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}\\]\nNow, take the expectation of both sides:\n\\[E[\\hat{\\beta_1}]=E\\left[\\beta_1+\\frac{\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} \\right]\\]\nWe can break this up, using properties of expectations. First, recall \\(E[a+b]=E[a]+E[b]\\), so we can break apart the two terms.\n\\[E[\\hat{\\beta_1}]=E[\\beta_1]+E\\left[\\frac{\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} \\right]\\]\nSecond, the true population value of \\(\\beta_1\\) is a constant, so \\(E[\\beta_1]=\\beta_1\\).\nThird, since we assume \\(X\\) is also “fixed” and not random, the variance of \\(X\\), \\(\\displaystyle\\sum_{i=1}^n (X_i-\\bar{X})\\), in the denominator, is just a constant, and can be brought outside the expectation.\n\\[E[\\hat{\\beta_1}]=\\beta_1+\\frac{E\\left[\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})\\right]  }{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}\\]\nThus, the properties of the equation are primarily driven by the expectation \\(E\\bigg[\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})\\bigg]\\). We now turn to this term.\nUse the property of summation operators to expand the numerator term:\n\\[\\begin{align*}\n    \\hat{\\beta_1}&=\\beta_1+\\frac{\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} \\\\\n        \\hat{\\beta_1}&=\\beta_1+\\frac{\\displaystyle \\sum^n_{i=1} (u_i-\\bar{u})(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} \\\\\n\\end{align*}\\]\nNow divide the numerator and denominator of the second term by \\(\\frac{1}{n}\\). Realize this gives us the covariance between \\(X\\) and \\(u\\) in the numerator and variance of \\(X\\) in the denominator, based on their respective definitions.\n\\[\\begin{align*}\n    \\hat{\\beta_1}&=\\beta_1+\\cfrac{\\frac{1}{n}\\displaystyle \\sum^n_{i=1} (u_i-\\bar{u})(X_i-\\bar{X})}{\\frac{1}{n}\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} \\\\\n    \\hat{\\beta_1}&=\\beta_1+\\cfrac{cov(X,u)}{var(X)} \\\\\n    \\hat{\\beta_1}&=\\beta_1+\\cfrac{s_{X,u}}{s_X^2} \\\\\n\\end{align*}\\]\nBy the Zero Conditional Mean assumption of OLS, \\(s_{X,u}=0\\).\nAlternatively, we can express the bias in terms of correlation instead of covariance:\n\\[E[\\hat{\\beta_1}]=\\beta_1+\\cfrac{cov(X,u)}{var(X)}\\]\nFrom the definition of correlation:\n\\[\\begin{align*}\n     cor(X,u)&=\\frac{cov(X,u)}{s_X s_u}\\\\\n     cor(X,u)s_Xs_u &=cov(X,u)\\\\\n\\end{align*}\\]\nPlugging this in:\n\\[\\begin{align*}\nE[\\hat{\\beta_1}]&=\\beta_1+\\frac{cov(X,u)}{var(X)} \\\\\nE[\\hat{\\beta_1}]&=\\beta_1+\\frac{\\big[cor(X,u)s_xs_u\\big]}{s^2_X} \\\\\nE[\\hat{\\beta_1}]&=\\beta_1+\\frac{cor(X,u)s_u}{s_X} \\\\\nE[\\hat{\\beta_1}]&=\\beta_1+cor(X,u)\\frac{s_u}{s_X} \\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "resources/appendices/2.4-appendix.html#proof-of-the-unbiasedness-of-hatbeta_1",
    "href": "resources/appendices/2.4-appendix.html#proof-of-the-unbiasedness-of-hatbeta_1",
    "title": "2.4 — Goodness of Fit and Bias — Appendix",
    "section": "Proof of the Unbiasedness of \\(\\hat{\\beta_1}\\)",
    "text": "Proof of the Unbiasedness of \\(\\hat{\\beta_1}\\)\nBegin with equation:2\n\\[\\hat{\\beta_1}=\\frac{\\sum Y_iX_i}{\\sum X_i^2}\\]\nSubstitute for \\(Y_i\\):\n\\[\\hat{\\beta_1}=\\frac{\\sum (\\beta_1 X_i+u_i)X_i}{\\sum X_i^2}\\]\nDistribute \\(X_i\\) in the numerator:\n\\[\\hat{\\beta_1}=\\frac{\\sum \\beta_1 X_i^2+u_iX_i}{\\sum X_i^2}\\]\nSeparate the sum into additive pieces:\n\\[\\hat{\\beta_1}=\\frac{\\sum \\beta_1 X_i^2}{\\sum X_i^2}+\\frac{u_i X_i}{\\sum X_i^2}\\]\n\\(\\beta_1\\) is constant, so we can pull it out of the first sum:\n\\[\\hat{\\beta_1}=\\beta_1 \\frac{\\sum X_i^2}{\\sum X_i^2}+\\frac{u_i X_i}{\\sum X_i^2}\\]\nSimplifying the first term, we are left with:\n\\[\\hat{\\beta_1}=\\beta_1 +\\frac{u_i X_i}{\\sum X_i^2}\\]\nNow if we take expectations of both sides:\n\\[E[\\hat{\\beta_1}]=E[\\beta_1] +E\\left[\\frac{u_i X_i}{\\sum X_i^2}\\right]\\]\n\\(\\beta_1\\) is a constant, so the expectation of \\(\\beta_1\\) is itself.\n\\[E[\\hat{\\beta_1}]=\\beta_1 +E\\bigg[\\frac{u_i X_i}{\\sum X_i^2}\\bigg]\\]\nUsing the properties of expectations, we can pull out \\(\\frac{1}{\\sum X_i^2}\\) as a constant:\n\\[E[\\hat{\\beta_1}]=\\beta_1 +\\frac{1}{\\sum X_i^2} E\\bigg[\\sum u_i X_i\\bigg]\\]\nAgain using the properties of expectations, we can put the expectation inside the summation operator (the expectation of a sum is the sum of expectations):\n\\[E[\\hat{\\beta_1}]=\\beta_1 +\\frac{1}{\\sum X_i^2}\\sum E[u_i X_i]\\]\nUnder the exogeneity condition, the correlation between \\(X_i\\) and \\(u_i\\) is 0.\n\\[E[\\hat{\\beta_1}]=\\beta_1\\]"
  },
  {
    "objectID": "resources/appendices/2.6-appendix.html",
    "href": "resources/appendices/2.6-appendix.html",
    "title": "2.6 — Statistical Inference — Appendix",
    "section": "",
    "text": "Footnotes\n\n\nThat is, for each simulation, we randomly selected observations from our existing sample to be in the simulation, and then did not put that observation back in the pool to possibly be selected again.↩︎\nEven when I was in graduate school, 2011–2015↩︎\nIf samples are i.i.d. (independently and identically distributed if they are drawn from the same population randomly and then replaced) we don’t even need to know the population distribution to assume normality↩︎\nInstead of the “standard deviation”. “Standard error” refers to the sampling variability of a sample statistic, and is always talking about a sampling distribution.↩︎\nWhich we need to know! We often do not know it!↩︎\n“Student” was the penname of William Sealy Gosset, who has one of the more interesting stories in statistics. He worked for Guiness in Ireland testing the quality of beer. He found that with small sample sizes, normal distributions did not yield accurate results. He came up with a more accurate distribution, and since Guiness would not let him publish his findings, published it under the pseudonym of “Student.”↩︎\nDegrees of freedom, \\(df\\) are the number of independent values used for the calculation of a statistic minus the number of other statistics used as intermediate steps. For sample standard deviation \\(s\\), we use \\(n\\) deviations \\((x_i-\\bar{x})\\) and 1 parameter \\((\\bar{x})\\), hence \\(df=n-1\\)↩︎\nEquivalently, \\(\\alpha\\) is the probability of a Type I error: a false positive finding where we incorrectly reject a null hypothesis when it the null hypothesis is in fact true.↩︎\nOf course, if we don’t know the population \\(\\sigma\\), we need to use the \\(t\\)-distribution and find critical \\(t\\)-scores instead of \\(Z\\)-scores. See above.↩︎\nNote this is the precise value behind the rule of thumb that 95% of observations fall within 2 standard deviations of the mean!↩︎"
  },
  {
    "objectID": "resources/coding-style.html",
    "href": "resources/coding-style.html",
    "title": "Suggested Code Style Guide",
    "section": "",
    "text": "You will not be graded on the style of your code. But now’s the best time to learn best practices (while you don’t know any alternatives!) to save yourself and your potential colleagues (including your future self) from unnecessary frustration.\n\ncomment above for overall idea\ncomment on side for individual elements of long commands\nname with _\nuse %>% wherever possible\nspaces between all operators: <-, =, +, etc.\n\nException: : and ::\n\nline breaks between multiple arguments to a function\n\n\np<-ggplot(data=data, aes(x=x,y=y,fill=fill))+geom_point()\n\nbecomes\n\np <- ggplot(data = data,\n       aes(x = x,\n           y = y,\n           fill = fill))+\n  geom_point()"
  },
  {
    "objectID": "resources/computing.html",
    "href": "resources/computing.html",
    "title": "A Quick Guide to Using Your Computer",
    "section": "",
    "text": "File Systems\nBack in my day, search was not very good, and so we had to know where all files were located on our computer. We had folders inside of folders, creating a file hierarchy. Today, search is so good that many in gen Z don’t know where files are stored on their computer, or even “what a file is!” Simply type the name of the document you are looking for into search (whether on Google, Siri, Cortana, Finder, etc) and it magically appears from the depths of…somewhere…on your computer or even in the cloud.\nUnfortunately, when computing and coding, computers need precise instructions about what files to work with, meaning you explicitly need to tell them where on your computer you wish to save a file to, or open an existing file. This means you need to specify the path through the hierarchy of folders on your computer.\n\nMac\n\n| Macintosh HD\n  |- Users\n    |- ryansafner\n      |- Applications\n      |- Desktop\n      |- Documents\n      |- Downloads\n      |- Dropbox\n      |- Movies\n      |- Music\n      |- Pictures\n\n\n\nWindows\n\n\n\nDownloading, Saving, and Opening Files\n\n\nCommand Line"
  },
  {
    "objectID": "resources/data.html",
    "href": "resources/data.html",
    "title": "Data Sources and Suggestions",
    "section": "",
    "text": "A near-comprehensive list of all existing data sets built-in to R or R packages1\n\n\n\n\n\nGoogle Database Search\nKaggle\nHarvard Law School: Find a Database\n\n\n\n\nBelow are packages written by and for R users that link up with the API of key data sets for easy use in R. Each link goes to the documentation and description of each package.\nDon’t forget to install3 first and then load it with library().\n\nowidR for importing data from Our World in Data\nwbstats provides access to all the data available on the World Bank API, which is basically everything on their website. The World Bank keeps track of many country-level indicators over time.\ntidycensus gives you access to data from the US Census and the American Community Survey. These are the largest high-quality data sets you’ll find of cross-sectional data on individual people in the US. You’ll need to get a (free) API key from the website (or ask me for mine).\nfredr gets data from the Federal Reserve’s Economic Database (FRED). You’ll need to get a (free) API key from the website (or ask me for mine).\ntidyquant gets data from a number of financial sources (including fredr).\nicpsrdata downloads data from the Inter-university Consortium for Political and Social Research (you’ll need an account and a keycode). ICPSR is a database of datasets from published social science papers for the purposes of reproducibility.\nNHANES uses data from the US National Health and Nutrition Examination Survey.\nipumsr has census data from all around the world, in addition to the US census, American Community Survey, and Current Population Survey. If you’re doing international micro work, look at IPUMS. It’s also the easiest way to get the Current Population Survey (CPS), which is very popular for labor economics. Unfortunately ipumsr won’t get the data from within R; you’ll have to make your own data extract on the IPUMS website and download it. But ipumsr will read that file into R and preserve things like names and labels.\neducation-data-package-r4 is the Urban Institute’s data data on educational institutions in the US, including colleges (in IPEDS) and K-12 schools (in CCD). This package also has data on county-level poverty rates from SAIPE.\npsidR is the Panel Study of Income Dynamics. This study doesn’t just follow people over their lifetimes, it follows their children too, generationally! A great source for studying how things follow families through generations.\natus is th e American Time Use Survey, which is a large cross-sectional data set with information on how people spend their time.\nRilostat uses data from the International Labor Organization. This contains lots of different statistics on labor, like employment, wage gaps, etc., generally aggregated to the national level and changing over time.\ndemocracyData5 is a great “package for accessing and manipulating existing measures of democracy.”\npoliticaldata provides useful functions for obtaining commonly-used data in political analysis and political science, including from sources such as the Comparative Agendas Project (which provides data on politics and policy from 20+ countries), the MIT Election and Data Science Lab, and FiveThirtyEight.\n\nBelow is a list of good data sources depending on the types of topics you might be interested in writing on:6"
  },
  {
    "objectID": "resources/data.html#key-data-sources",
    "href": "resources/data.html#key-data-sources",
    "title": "Data Sources and Suggestions",
    "section": "Key Data Sources",
    "text": "Key Data Sources\n\nCoronavirus Data: John Hopkins CSSE Covid-19 data (definitive), Our World in Data, New York Times Covid data, covdata r package, Tidy Covid data\nOur World in Data\nAmerican Economic Association Data\nIPUMS (Integrated Public Use Microdata Series)\nEconData from UMD\nICPSR (Inter-university Consortium for Political and Social Research)\nNBER’s Public Use Data Archive\nHistorical Macroeconomic Statistics\nUMD’s Interindustry Forecasting\nDB-nomics\nInternet UPC Database\nInternational Trade Data\nOurWorldinData.org (download datasets)\nSciencesPo International Trade Gravity Dataset\nCenter for International Data\nAtlas of Economic Complexity\nU.N. World Development Reports\nObservatory of Economic Complexity\nReddit /r/datasets\nGoogle Cloud Public Datasets\n\nBy Topic\n\nQuality of Government Data has an extremely wide range of data sources pertaining to measures of institutions. The data itself can be found here.\nNational and State Accounts Data: Bureau of Economic Analysis\nLabor Market and Price Data: Bureau of Labor Statistics\nMacroeconomic Data: Federal Reserve Economic Data (FRED), World Development Indicators (World Bank), Penn World Table\nInternational Data: NationMaster.com, Doing Business, CIESIN\nCensus Data: U.S. Census Bureau\nSports Data: Spotrac, Rodney Fort’s Sports Data\nData Clearing House: Stat USA, Fedstats, Statistical Abstract of the United States, Resources for Economists\nPolitical and Social Data: ICPSR, Federal Election Commission, Poole and Rosenthal Roll Call Data (Voting ideology), Archigos Data on Political Leaders, Library of Congress: Thomas (Legislation), Iowa Electronic Markets (Prediction Markets)\nWar and Violence Data: Correlates of War\nState Level Data: Correlates of State Policy\nHealth Data: Centers for Disease Control, CDC Wonder System\nCrime Data: Bureau of Justice Statistics\nEducation Data: National Center for Education Statistics\nEnvironmental Data: EPA\nReligion Data: American Religion Data Archiva (ARDA)\nFinancial Data: Financial Data Finder{Financial Data Finder}\nPhilanthropy Data: The Urban Institute"
  },
  {
    "objectID": "resources/exams/midterm-concepts.html",
    "href": "resources/exams/midterm-concepts.html",
    "title": "Midterm Concepts",
    "section": "",
    "text": "Measures of Fit\n\n\\(R^2\\): fraction of total variation on \\(Y\\) explained by variation in \\(X\\) according to model\n\n\\[\n\\begin{align*}\nR^2 & = \\frac{SSM}{SST} \\\\\nR^2 & = 1 - \\frac{SSR}{SST} \\\\\nR^2 & = r_{X,Y}^2 \\\\\n\\end{align*}\n\\]\n\nWhere\n\n\\(SSM = \\sum (\\hat{Y}_i - \\bar{Y}_i)^2\\)\n\\(SST = \\sum(Y_i - \\bar{Y}_i)^2\\)\n\\(SSR = \\sum u_i^2\\)\n\n\n\n\nStandard error of the regression (or residuals), SER: average size of \\(\\hat{u}_i\\), i.e. average distance between points and the regression line\n\\[\nSER \\, (\\sigma_{\\hat{u}_i})= \\frac{SSR}{n-2} = \\frac{\\sum \\hat{u_i}^2}{n-2}\n\\]\n\n\n\nSampling Distribution of \\(\\hat{\\beta}_1\\)\n\n\\(\\hat{\\beta_1}\\) is a random variable, so it has its own sampling distribution with mean \\(\\mathbb{E}[\\hat{\\beta_1}]\\) and standard error \\(se[\\hat{\\beta_1}]\\)\n\nMean of OLS estimator \\(\\hat{\\beta_1}\\) & Bias: Endogeneity & Exogeneity\n\n\\(X\\) is exogenous if it is not correlated with the error term\n\\[\n\\begin{align*} cor(X,u) &=0 \\\\\n\\mathbb{E}[u|X] &=0 \\\\\n\\end{align*}\n\\]\n\nequivalently, knowing \\(X\\) should tell us nothing about \\(u\\) (zero conditional mean assumption)\nif \\(X\\) is exogenous, OLS estimate of \\(\\beta_1\\) is unbiased\n\\[\nE[\\hat{\\beta}_1]=\\beta_1\n\\]\n\n\\(X\\) is endogenous if it is correlated with the error term\n\\[\ncor(X,u) \\neq 0\n\\]\n\nIf \\(X\\) is endogenous, OLS estimate of \\(\\beta_1\\) is biased:\n\\[\n\\mathbb{E}[\\hat{\\beta}_1] = \\beta_1 + \\underbrace{cor(X,u)\\frac{\\sigma_u}{\\sigma_X}}_{bias}\n\\]\n\nCan measure strength and direction (+ or -) of bias\nNote if unbiased, \\(cor(X,u)=0\\), so \\(E[\\hat{\\beta_1}]=\\beta_1\\)\n\n\nAssumptions about u\n\nThe mean of the errors is 0\n\\[\n\\mathbb{E}[u_i] = 0\n\\]\nThe variance of the errors is constant over all values of \\(X\\) (homoskedasticity)\n\\[\nvar[u_i|X_i]=\\sigma_u^2\n\\]\nErrors are not correlated across observations \\(i\\) and \\(j\\) (no autocorrelation)\n\\[\ncor(u_i,u_j) = 0\n\\]\nThere is no correlation between \\(X\\) and \\(u\\), i.e. the model is exogenous\n\\[\n\\begin{align*} cor(X,u) &=0 \\\\\n\\mathbb{E}[u|X] &=0 \\\\\n\\end{align*}\n\\]\n\n\nPrecision of OLS estimator \\(\\hat{\\beta}_1\\) measures uncertainty/variability of estimate\n\\[\n\\begin{align*}\nvar[\\hat{\\beta}_1]&=\\frac{SER^2}{n\\times var(X)}\\\\\nse[\\hat{\\beta}_1]&=\\sqrt{var[\\hat{\\beta}_1]} \\\\\n\\end{align*}\n\\]\n\nAffected by three factors:\n\nModel fit, (SER)\nSample size, \\(n\\)\nVariation in \\(X\\)\n\nHeteroskedasticity & Homoskedasticity\n\nHomoskedastic errors (\\(\\hat{u}_i\\)) have the same variance over all values of \\(X\\)\nHeteroskedastic errors (\\(\\hat{u}_i\\)) have different variance over values of \\(X\\)\n\nHeteroskedasticity does not bias our estimates, but incorrectly lowers variance & standard errors (inflating $t$-statistics and significance!)\nCan correct for heteroskedasticity by using robust standard errors\n\n\n\n\n\n\nHypothesis Testing of \\(\\beta_1\\)\n\n\\(H_0: \\beta_1=\\beta_{1,0}\\), often \\(H_0: \\beta_1=0\\)\nTwo sided alternative \\(H_1: \\beta_1 \\neq 0\\)\nOne sided alternatives \\(H_1: \\beta_1 > 0\\) or \\(H_2: \\beta_1 < 0\\)\n\\(t\\)-statistic\n\n\\[\nt=\\frac{\\hat{\\beta_1}-\\beta_{1,0}}{se(\\hat{\\beta_1})}\n\\]\n\nCompare \\(t\\) against critical value \\(t\\)*, or compute \\(p\\)-value as usual\nConfidence intervals (95%): \\(\\hat{\\beta_1} \\pm 1.96 \\left(se(\\hat{\\beta_1})\\right)\\)"
  },
  {
    "objectID": "resources/ggplot2.html#fixing-your-scales",
    "href": "resources/ggplot2.html#fixing-your-scales",
    "title": "ggplot2 Extensions",
    "section": "Fixing Your Scales",
    "text": "Fixing Your Scales"
  },
  {
    "objectID": "resources/ggplot2.html#extensions-to-ggplot2",
    "href": "resources/ggplot2.html#extensions-to-ggplot2",
    "title": "ggplot2 Extensions",
    "section": "Extensions to ggplot2",
    "text": "Extensions to ggplot2\nggplot2, being one of the most popular packages, has a lot of user-made extensions that allow you to do lots of neat things with your plots, from plotting networks, Parliaments, dendrograms, and other types of graphs, to formatting and visual tools that help improve your figures.\nFor the following demonstrations, we will use the gapminder data. Let’s start just by making a basic graph and saving it as an object called p. I have decided to map (aes()) each geom_point’s color to continent and size to pop:\n\nlibrary(gapminder)\nlibrary(ggplot2)\n\np<-ggplot(gapminder) +\n  aes(x = gdpPercap,\n      y = lifeExp) +\n  geom_point(aes(size = pop,\n                 color = continent))\np\n\n\n\n\n\nWorking with Scales\nI don’t like the default choices ggplot2 has made for my point sizes for population, or the way it depicts them (in scientific notation) on the legend.\nI will set my own scale by setting the breaks1 manually, according to a vector I define as: c(100000, 1000000, 100000000, 1000000000). So, I will use one point size for populations of 100 thousand, a bigger one for a million, a bigger one for 100 millions, and the biggest for 1 billion.\nI am going to label these (on my legend) as the following vector: c(\"<1 million\",\"1 million\",\"100 million\", \"1 billion\").\nLastly, I don’t think the size of the billion circle is big enough, a billion is a lot of people! So I will set the range of sizes from size 1 point to size 10 point.\nTo do this, I include all of this inside the scale_size command (because I am scaling the size of points):\n\n# let's save this as p2\np2<-p+scale_size(breaks = c(100000, 1000000, 100000000, 1000000000), # cut offs\n             labels=c(\"<1 million\",\"1 million\",\"100 million\", \"1 billion\"), # labels on legend\n             range=c(1,10)) # min & max point size\n\n# let's see what we did\np2\n\n\n\n\nThis is also very hard to see the relationship (because it is nonlinear!). So I will rescale the x_axis logarithmically with base 10:\n\np2+scale_x_log10()\n\n\n\n\nDoing this already gives me a much clearer view of the relationship! But I don’t like the labels, or the breaks it has chosen, so I can customize them again:\n\np2+scale_x_log10(\n    breaks = c(10^3, 10^4, 10^5), # 1,000, 10,000, and 100,000\n    labels = scales::dollar)\n\n\n\n\nThe scales package has a nice command to format labels, in this case I am calling the scales::dollar function to print dollar signs in front of my axes numbers. I could have done it manually instead by setting labels = c(\"$1,000\", \"$10,000\", \"$100,000\")."
  },
  {
    "objectID": "resources/ggplot2.html#subsetting-data",
    "href": "resources/ggplot2.html#subsetting-data",
    "title": "ggplot2 Extensions",
    "section": "Subsetting Data",
    "text": "Subsetting Data\nWe learn more about this in class 1.4 using tidyverse, but let’s only look at one year of data (there’s too much going on in this plot, especially with the large points, some points are covering other points). So let’s only look at the year 2007. I can do this in two ways:\n\nSubset the data, save the subsetted data as an object (I’ll call gap2007), plot with that object as my data:\n\n\nlibrary(tidyverse)\n\n\ngap2007 <- gapminder %>%\n  filter(year == 2007)\n\np2007 <- ggplot(data = gap2007)+\n  aes(x = gdpPercap,\n      y = lifeExp) +\n  geom_point(aes(size = pop,\n                 color = continent))\n\np2007\n\n\n\n\n\nSubset data and pipe it directly into ggplot2’s data argument:\n\n\nlibrary(tidyverse)\n\np2007 <- gapminder %>%\n  filter(year == 2007) %>%\n  ggplot(data = .)+ # . is placeholder\n  aes(x = gdpPercap,\n      y = lifeExp) +\n  geom_point(aes(size = pop,\n                 color = continent))\n\np2007\n\n\n\n\nNow let’s clean up the graph with the same things I did before, hide the legends (set the color and size, my two aesthetic mappings, guides equal to FALSE), add some labels, and change the theme:\n\np3<-p2007+scale_size(breaks = c(100000, 1000000, 100000000, 1000000000), \n             labels=c(\"<1 million\",\"1 million\",\"100 million\", \"1 billion\"), \n             range=c(1,10))+\n  scale_x_log10(\n    breaks = c(10^3, 10^4, 10^5),\n    labels = scales::dollar)+\n  labs(x = \"GDP per Capita (USD)\",\n       y = \"Life Expectancy (years)\")+\n  guides(color = FALSE,\n         size = FALSE)+\n  theme_classic()\n\nWarning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n\"none\")` instead.\n\np3"
  },
  {
    "objectID": "resources/ggplot2.html#ggrepel",
    "href": "resources/ggplot2.html#ggrepel",
    "title": "ggplot2 Extensions",
    "section": "ggrepel",
    "text": "ggrepel\nIf I were to try to label some countries, with either geom_text (just word) or geom_label() (text in a box), setting the label aesthetic to country, see what would happen:\n\np3+geom_label(aes(label = country,\n                  color = continent))\n\n\n\n\nThe labels, which are plotted right on top of each point, cover the points!\nSomeone figured out a clever way to let us do both, and it is a package called ggrepel, which allows you to plot labels that are “repelled” away from the point they are labelling in an intelligent way. This is a separate package, which you must first install and then load with library to use it!\n\n# install.packages(\"ggrepel\") # do this only once\nlibrary(ggrepel)\n\np3+geom_text_repel(aes(label = country,\n                        color = continent,\n                       size = pop),\n                    size = 3)\n\nWarning: ggrepel: 69 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\nThis is much better, but for this particular chart, since a lot of observations are close together, it would be unwise to label everything, perhaps only label a subset of important points."
  },
  {
    "objectID": "resources/ggplot2.html#ggflag",
    "href": "resources/ggplot2.html#ggflag",
    "title": "ggplot2 Extensions",
    "section": "ggflag",
    "text": "ggflag\nOne alternative is instead of points, to use some other marking. Someone created the ggflags package to let you plot flags of countries. This creates a new type of geom, called geom_flag, that requires you to map the country aesthetic to a variable in your data with the country name (incidentally, in gapminder that variable is also called country). Let’s try that out instead (and add my same customizations as above)"
  },
  {
    "objectID": "resources/ggplot2.html#plotly",
    "href": "resources/ggplot2.html#plotly",
    "title": "ggplot2 Extensions",
    "section": "plotly",
    "text": "plotly\nWe can also make our plot a bit more interactive (on web only of course!) using the ggplotly package, which allows ggplot2 to interface with a javascript library called plotly.2\n\n# install.packages(\"plotly\") # do this only once\nlibrary(plotly)\n\n\nggplotly(p3)"
  },
  {
    "objectID": "resources/ggplot2.html#better-barplots",
    "href": "resources/ggplot2.html#better-barplots",
    "title": "ggplot2 Extensions",
    "section": "Better barplots",
    "text": "Better barplots\nAnother major type of plot that we may use often is a barplot. Suppose we want to show the GDP per Capita of the top 20 countries in 2007. If I were to plot country on the x axis and gdpPercap on the y axis with geom_col,3 I get the following mess:\n\nggplot(gap2007)+\n  aes(x = country,\n      y = gdpPercap,\n      fill = continent)+\n  geom_col()\n\n\n\n\nSo let’s filter4 arrange our data in descending order by gdpPercap:\n\ngap2007 %>%\n  arrange(desc(gdpPercap))\n\n# A tibble: 142 × 6\n   country          continent  year lifeExp       pop gdpPercap\n   <fct>            <fct>     <int>   <dbl>     <int>     <dbl>\n 1 Norway           Europe     2007    80.2   4627926    49357.\n 2 Kuwait           Asia       2007    77.6   2505559    47307.\n 3 Singapore        Asia       2007    80.0   4553009    47143.\n 4 United States    Americas   2007    78.2 301139947    42952.\n 5 Ireland          Europe     2007    78.9   4109086    40676.\n 6 Hong Kong, China Asia       2007    82.2   6980412    39725.\n 7 Switzerland      Europe     2007    81.7   7554661    37506.\n 8 Netherlands      Europe     2007    79.8  16570613    36798.\n 9 Canada           Americas   2007    80.7  33390141    36319.\n10 Iceland          Europe     2007    81.8    301931    36181.\n# … with 132 more rows\n\n\nWe only want the top 20 observations, so lets slice to extract just rows 1:20. Then we’ll pipe it into our plot:\n\nbar<-gap2007 %>%\n  arrange(desc(gdpPercap)) %>%\n  slice(1:20) %>%\n  ggplot(data = .)+\n  aes(x = country,\n      y = gdpPercap,\n      fill = continent)+\n  geom_col()\nbar\n\n\n\n\nNow that’s closer to what we wanted! But here are a few more tips and tricks to make it better. First, let’s flip the axes to be able to read the countries better, using coord_flip()\n\nbar+coord_flip()\n\n\n\n\nOne other useful thing to know would be how to display the bars in numerical order (from largest gdpPercap to smalleset gdpPercap) so we can get a clear ranking of countries. To do this, we are going to make use of another tidyverse package called forcats (dealing with factors), specifically the function fct_reorder(), which reorders a factor variable (our country) by the values of some other variable (our gdpPercap). We need to do this to our x variable aesthetic:\n\nbar2<-gap2007 %>%\n  arrange(desc(gdpPercap)) %>%\n  slice(1:20) %>%\n  ggplot(data = .)+\n  aes(x = forcats::fct_reorder(country, gdpPercap), #<<\n      y = gdpPercap,\n      fill = continent)+\n  geom_col()+\n  coord_flip()\nbar2\n\n\n\n\nThis is already looking good. Here’s another creative way to depict the same thing in a more visually-striking way. Instead of using geom_bar(), let’s combine geom_flag (to serve as end points) and geom_segment()5 (line segments) to make the following version:"
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "Look here over the course of the semester for resources, links, and tips on how to succeed in the course, how to write well, and other things of interest related to econometrics, data analysis, managing your workflow, and using R."
  },
  {
    "objectID": "resources/installing-r.html",
    "href": "resources/installing-r.html",
    "title": "Installing R and R Studio",
    "section": "",
    "text": "We will do all of our work in this course with the free & open source programming language R. While you can run everything you need directly in the command line using R, it is a lot more convenient to use an integrated development environment (IDE) like R Studio. Think of R as the engine of a car, and R Studio as the dashboard.\nYou will need to install both, but we will ever only open R Studio."
  },
  {
    "objectID": "resources/installing-r.html#installing-r",
    "href": "resources/installing-r.html#installing-r",
    "title": "Installing R and R Studio",
    "section": "Installing R",
    "text": "Installing R\nFirst you will need to download and install R on your computer.\n\nGo to the Comprehensive R Archive Network (CRAN) that maintains R and its official packages at: https://cran.r-project.org\nClick on “Download R for …” your operating system (Mac or Windows)\n\n\nIf you use a Mac, scroll to the first .pkg file listed on the left and download.\nIf you use Windows, click on base (“This what you want to install R for the first time”)\n\n\nInstall the downloaded package like you would any software application on your computer.\n\n\nTypically, open the file from your Downloads folder (or whever you save downloaded files) and follow the prompts to install on your computer.\n\n\nIf you use a Mac, also download and install XQuartz (https://www.xquartz.org/). You do not need to do this on Windows."
  },
  {
    "objectID": "resources/installing-r.html#install-r-studio",
    "href": "resources/installing-r.html#install-r-studio",
    "title": "Installing R and R Studio",
    "section": "Install R Studio",
    "text": "Install R Studio\n\nGo to RStudio.com and download the free desktop version.\nThe website should automatically detect your operating system and give you a large button to click to download the application.\nInstall the downloaded package like you would any software application on your computer."
  },
  {
    "objectID": "resources/installing-r.html#r-studio-cloud",
    "href": "resources/installing-r.html#r-studio-cloud",
    "title": "Installing R and R Studio",
    "section": "R Studio Cloud",
    "text": "R Studio Cloud\nR is free, but sometimes can be difficult to install and configure on your computer. To make things easier, and to ensure everyone has a consistent experience for class, you can (and should) use the free Rstudio.Cloud service initially. This allows you to run R in your browser (i.e. Chrome, Firefox, Safari, etc), meaning you don’t need to worry about installing things on your computer.\nGo to https://rstudio.cloud and create an account (please use your first and last name). I will send you a link via email to join our class workspace.\nR Studio Cloud is convenient, but is not designed to be as fully customizable and extensive as the main desktop version. I would start with the Cloud version if you have trouble with your own computer or computers on campus running R or R Studio. But ultimately, you will want to eventually do everything on your own computer and not the cloud version."
  },
  {
    "objectID": "resources/pdfs.html",
    "href": "resources/pdfs.html",
    "title": "How to Make a PDF",
    "section": "",
    "text": "There are many good apps out there that will allow you to take photos and convert them to PDFs. This is actually a better method than using your computer (described below), since theses apps optimize your photos for PDFs (using your computer to convert will often result in very large PDF file sizes!). Here are a few apps you can use:\n\nScannable 1\nTurboscan \nImage to PDF Converter Free \nPDF Converter Pro \nSimple Scan \n\nPersonally, I use Scannable — primarily because of its association with Evernote, if you wanted a recommendation. But note it does not exist on Android. I also have successfully used Turboscan in the past.\nAdditionally, as Hood students, you all have Onedrive, you can use the app on your phone to scan documents with photos and convert them to PDFs."
  },
  {
    "objectID": "resources/pdfs.html#using-images-sent-to-your-computer",
    "href": "resources/pdfs.html#using-images-sent-to-your-computer",
    "title": "How to Make a PDF",
    "section": "Using Images Sent to Your Computer",
    "text": "Using Images Sent to Your Computer\nMost modern versions of operating system have a built-in tool in the File Viewer (or Finder) menus, after clicking on one or multiple files, to create a PDF from the files.\nSo first take photos on your smartphone of your written work (one photo per page). Please try to frame your photos properly! Put your paper flat on a solid surface (table, desk, the floor, etc). Get the whole page within the borders of the photo, and not too much background. I don’t need to see half of your desk or bed as you are taking the photo! Take a look at it and make sure it is legible.\nNext, get the photos onto your computer (whether by Airdrop, email to yourself, Dropbox, etc.). Finally, depending on your OS, convert the files to a PDF:\n1. On a Windows PC\nOpen the folder where your photos are currently, in the File Explorer. Select all of the photos, and right click, and select Print. In the dialog box that pops up, select Microsoft Print to PDF in the Printer box, and then click Print. This will save it as a .pdf file in that folder. See more information.\n2. On a Mac\nAs I use a Mac, I will show you how Mac OS has a neat feature built into Finder, which allows converting multiple files into a single PDF file as a Quick Action. I have written two pages in a notebook and taken two separate pictures of them, and airdropped them onto my computer.\n\n\n\nHere is the  example PDF.\n3. On Linux\nIf you use Linux, I assume you know your way around a computer well enough to make a PDF! 🤖"
  },
  {
    "objectID": "resources/reporting-regressions.html",
    "href": "resources/reporting-regressions.html",
    "title": "Reporting Regressions",
    "section": "",
    "text": "Running a regression (or multiple regressions) produces a lot of information — estimated OLS coefficients, hypothesis tests, goodness of fit measures, F-tests, the distribution of residuals, etc. The standard Base R output of the lm() object using summary() looks like this:\nThis contains:"
  },
  {
    "objectID": "resources/reporting-regressions.html#broom",
    "href": "resources/reporting-regressions.html#broom",
    "title": "Reporting Regressions",
    "section": "Broom",
    "text": "Broom"
  },
  {
    "objectID": "resources/reporting-regressions.html#huxtable",
    "href": "resources/reporting-regressions.html#huxtable",
    "title": "Reporting Regressions",
    "section": "Huxtable",
    "text": "Huxtable"
  },
  {
    "objectID": "resources/reporting-regressions.html#modelsummary",
    "href": "resources/reporting-regressions.html#modelsummary",
    "title": "Reporting Regressions",
    "section": "Modelsummary",
    "text": "Modelsummary"
  },
  {
    "objectID": "resources/statistics.html",
    "href": "resources/statistics.html",
    "title": "Statistics Resources",
    "section": "",
    "text": "There are a lot of symbols (often greek letters or ligatures on English letters) used in statistics and econometrics. Luckliy, most of them follow some standard patterns, and are consistent across textbooks and research (note there are exceptions!).\n\n\n\n\n\n\n\n\nStyle\nExamples\nMeaning\n\n\n\n\nGreek letters\n\\(\\beta_0, \\beta_1, \\sigma, u\\)\nTrue parameters of population\n\n\nHats\n\\(\\hat{\\beta_0}, \\hat{\\beta_1}, \\hat{\\sigma}, \\hat{u}\\)\nOur statistical estimates of population parameters, from sample data\n\n\nEnglish capital letters\n\\(X_1, X_2, Y\\)\n(Random) variables in our sample data\n\n\nEnglish lowercase letters\n\\(x_{1i}, x_{2i}, y_i\\)\nIndividual observations of variables in our sample data\n\n\nModified capital letters\n\\(\\bar{X}, \\bar{Y}\\)\nStatistics calculated from our sample data (e.g. sample mean)\n\n\nBold capital letters\n\\(X= \\begin{bmatrix} x_1, x_2, \\cdots , x_n \\\\ \\end{bmatrix}\\) \\(\\mathbf{\\beta} = \\begin{bmatrix} \\beta_1, \\beta_2, \\cdots , \\beta_k \\\\ \\end{bmatrix}\\)\nVector or matrix"
  },
  {
    "objectID": "resources/statistics.html#sample-statistics-vs-population-parameters-formulae",
    "href": "resources/statistics.html#sample-statistics-vs-population-parameters-formulae",
    "title": "Statistics Resources",
    "section": "Sample Statistics vs Population Parameters Formulae",
    "text": "Sample Statistics vs Population Parameters Formulae\n\n\n\n\n\n\n\n\n\nSample\nPopulation\n\n\n\n\nPopulation\n\\(n\\)\n\\(N\\)\n\n\nMean\n\\(\\bar{x} = \\frac{1}{n} \\displaystyle\\sum^n_{i=1} x_i\\)\n\\(\\mu = \\frac{1}{N} \\displaystyle\\sum^N_{i=1} x_i\\)\n\n\nVariance\n\\(s^2=\\frac{1}{n-1} \\displaystyle\\sum^n_{i=1} (x_i-\\bar{x})^2\\)\n\\(\\sigma^2=\\frac{1}{N} \\displaystyle\\sum^N_{i=1} (x_i-\\mu)^2\\)\n\n\nStandard Deviation\n\\(s = \\sqrt{s^2}\\)\n\\(\\sigma = \\sqrt{\\sigma^2}\\)"
  },
  {
    "objectID": "resources/tips.html",
    "href": "resources/tips.html",
    "title": "Tips for Success in This Course",
    "section": "",
    "text": "Footnotes\n\n\nYes, that means I am doing a ton of learning every time I teach!↩︎\nYes, Google is your best friend. But you do not yet know how to ask the right questions, or understand what constitutes good answers.↩︎"
  },
  {
    "objectID": "resources/wrangling.html",
    "href": "resources/wrangling.html",
    "title": "Advanced Data Wrangling Tips & Tricks",
    "section": "",
    "text": "The majority of data wrangling tasks are described in class 1.4. However, depending on the project, there are particular issues that tend to crop up (often depending on the data type and the state of the spreadsheet) that are worth giving you some help with:"
  },
  {
    "objectID": "resources/wrangling.html#importing-data",
    "href": "resources/wrangling.html#importing-data",
    "title": "Advanced Data Wrangling Tips & Tricks",
    "section": "Importing Data",
    "text": "Importing Data\n\nFrom Google Sheets\n\n\nScraping Data from the Web\nThis is a more advanced technique, but in many use cases (and jobs), an essential tool for acquiring relevant data. There are a number of packages out there, but the one that uses tidyverse principles is rvest.\n\n\nCleaning\nThe janitor package\ncleannames()\n\ngenerally makes lowercase names"
  },
  {
    "objectID": "resources/wrangling.html#dealing-with-missing-data",
    "href": "resources/wrangling.html#dealing-with-missing-data",
    "title": "Advanced Data Wrangling Tips & Tricks",
    "section": "Dealing with Missing Data",
    "text": "Dealing with Missing Data\nWhen calculating statistics (e.g. with summarize()), many calculations will give errors if your data contains NAs.\n\nExample: Calculating Mean\n\ndata_missing <- tribble(\n  ~x, ~y,\n  2, 3,\n  1, 4,\n  NA, 2,\n  3, NA,\n  7, 8\n)\n\nNow if we were to get the mean of x:\n\ndata_missing %>% \n  summarize(mean_x = mean(x))\n\n# A tibble: 1 × 1\n  mean_x\n   <dbl>\n1     NA\n\n\nIt gives us NA.\nOne way to combat this is to ignore all observations that contain NA values. Most statistics functions (like mean()) have an optional argument na.rm, which if set to TRUE, will ignore NAs when performing the calculation:\n\ndata_missing %>%\n  summarize(mean_x = mean(x, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  mean_x\n   <dbl>\n1   3.25"
  },
  {
    "objectID": "resources/wrangling.html#working-with-strings",
    "href": "resources/wrangling.html#working-with-strings",
    "title": "Advanced Data Wrangling Tips & Tricks",
    "section": "Working with Strings",
    "text": "Working with Strings"
  },
  {
    "objectID": "resources/wrangling.html#working-with-dates-time",
    "href": "resources/wrangling.html#working-with-dates-time",
    "title": "Advanced Data Wrangling Tips & Tricks",
    "section": "Working with Dates & Time",
    "text": "Working with Dates & Time"
  },
  {
    "objectID": "resources/zipping.html",
    "href": "resources/zipping.html",
    "title": "Zipping & Unzipping Files",
    "section": "",
    "text": "Since R projects typically consist of multiple files (R scripts, datasets, images, etc.) the easiest way to distribute and send them is to combine all the different files in to a single compressed .zip file. When you unzip a .zip file, your computer extracts all the files contained inside to a new folder on your computer.\nUnzipping files on macOS is simple, but unzipping files on Windows can cause problems if you don’t pay careful attention."
  },
  {
    "objectID": "resources/zipping.html#unzipping-files-on-macos",
    "href": "resources/zipping.html#unzipping-files-on-macos",
    "title": "Zipping & Unzipping Files",
    "section": "Unzipping files on macOS",
    "text": "Unzipping files on macOS\nDouble click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file’s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started."
  },
  {
    "objectID": "resources/zipping.html#unzipping-files-on-windows",
    "href": "resources/zipping.html#unzipping-files-on-windows",
    "title": "Zipping & Unzipping Files",
    "section": "Unzipping files on Windows",
    "text": "Unzipping files on Windows\nA long story short: right click on the .zip file, select “Extract All…”, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what’s inside, but it will do so without actually extracting anything. This is quite annoying. Here’s what it looks like—the only clues that this folder is really a .zip file are that there’s a “Compressed Folder Tools” tab at the top with “Extract” in red.\n\nIt is tempting to just open files from this view, but this causes problems. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\n\nInstead, you need to right click on the .zip file and select “Extract All…”:\n\nThen choose where you want to unzip all the files and click on “Extract”\n\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Content materials contains suggested readings, more details about assignments, math appendices, and other helpful resources. I suggest you view these before each class.\n\n Slides are “Quarto” html presentations that can be opened in any browser (You can find a downloadable PDF on content pages)\n\n\n R materials contain extra tutorials, videos, practice exercises for using R\n\n\n Assignments are listed with due dates\n\n\nPlease note that the lesson numbers, topics, and titles (e.g. 1.1) are my design, and do not match up with the textbook!\nRelevant materials, if applicable will be posted before class meets and become colored links.\n\n\n\n\n\n\n\n\n\n\n\n\n\nI. Data Analysis in R\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nSlides\n\n\nR\n\n\nAssignment\n\n\n\n\n\n\n\n\nPreliminary Survey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMon Aug 22\n\n\n1.1 — Introduction to Econometrics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWed Aug 24\n\n\n1.2 — Meet R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWed Aug 31\n\n\n1.3 — Data Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWed Sep 07\n\n\n1.4 — Data Wrangling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMon Sep 12\n\n\n1.5 — Optimize Workflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem Set 1 due Fri Sep 23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nII. Linear Regression\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nSlides\n\n\nR\n\n\nAssignment\n\n\n\n\n\n\nWed Sep 14\n\n\n2.1 — Data and Descriptive Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMon Sep 19\n\n\n2.2 — Random Variables and Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem Set 2 due Fri Sep 30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWed Sep 21\n\n\n2.3 — Simple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMon Sep 26\n\n\n2.4 — Goodness of Fit and Bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWed Sep 28\n\n\n2.5 — Precision and Diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMon Oct 03\n\n\n2.6 — Inference for Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWed Oct 05\n\n\n2.7 — Hypothesis Testing for Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem Set 3 due Fri Oct 14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMon Oct 17\n\n\nMidterm Exam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIII. Causal Inference\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nSlides\n\n\nR\n\n\nAssignment\n\n\n\n\n\n\nWed Oct 19\n\n\n3.1 — The Fundamental Problem of Causal Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMon Oct 24\n\n\n3.2 — DAGs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWed Oct 26\n\n\n3.3 — Omitted Variable Bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIV. Multivariate Linear Regression\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nSlides\n\n\nR\n\n\nAssignment\n\n\n\n\n\n\nMon Oct 31\n\n\n4.1 — Multivariate OLS Estimators: Bias, Precision, and Fit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem Set 4 due Fri Nov 11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWed Nov 02\n\n\n4.2 — Writing an Empirical Paper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMon Nov 07\n\n\n4.3 — Regression with Categorical Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWed Nov 09\n\n\n4.4 — Regression with Interaction Effects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMon Nov 14\n\n\n4.5 — Transforming Variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem Set 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nV. Panel Data and Advanced Models\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nSlides\n\n\nR\n\n\nAssignment\n\n\n\n\n\n\nWed Nov 16\n\n\n5.1 — Panel Data and Fixed Effects Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMon Nov 21\n\n\n5.2 — Difference-in-Difference Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMon Nov 28\n\n\n5.3 — Instrumental Variables Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFri Dec 30\n\n\n5.4 — Logistic Regression Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Paper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Exam"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Dr. Ryan Safner\n   114 Rosenstock\n   Office Hours: MW 1:30-2:30PM\n   safner@hood.edu\n\n\n\n\n\n   MW 11:30 AM—12:55 PM\n   Rosenstock Trading Room\n   Aug 22—Dec 13, 2022\n   Slack\nDownload PDF\nEconometrics is the application of statistical tools to quantify and measure economic relationships in the real world. It uses real data to test economic hypotheses, quantitatively estimate causal relationships between economic variables, and to make forecasts of future events. The primary tool that economists use for empirical analysis is ordinary least squares (OLS) linear regression, so the majority of this course will focus on understanding, applying, and extending OLS regressions.\nI assume you have some working knowledge of economics at the intermediate level and some basic statistical tools.The formal prerequisites for this course are ECON 205 and ECON 206; ECMG 212 or MATH 112; and ECON 305 or ECON 306. We will do some basic review of some necessary statistics and probability at the beginning until everyone is comfortable, before jumping right into regressions."
  },
  {
    "objectID": "syllabus.html#books",
    "href": "syllabus.html#books",
    "title": "Syllabus",
    "section": "Books",
    "text": "Books\nThe following book is required and will be available from the campus bookstore. (You are not obligated to buy it, I just strongly recommend it in the sense that you will still have access to all data and assignments without possessing the book. But this is a course where you really will want to understand the derivations or get additional context beyond just my slides…)\n\nBailey, Michael A, 2019, Real Econometrics, New York: Oxford University Press, 2nd ed.\n\nYou are welcome to purchase the book by other means (e.g. Amazon, half.com, etc). I have no financial stake in requiring you to purchase this book. The (cheaper) 1st edition is sufficient, but makes significantly less use of R (in favor of STATA).\nThe following two books are recommended, and are free online. (You can purchase a hard copy of the first one if you really want.):\n\nGrolemund, Garrett and Hadley Wickham, R For Data Science\nIsmay, Chester and Albert Y Kim, Modern Dive: Statistical Inference Via Data Science\n\nThe first book is the number one resource for using R and tidyverse, and is written for beginners. I still look at it frequently. The second is another great reference for using tidyverse in the context of basic statistics."
  },
  {
    "objectID": "syllabus.html#articles",
    "href": "syllabus.html#articles",
    "title": "Syllabus",
    "section": "Articles",
    "text": "Articles\nThroughout the course, I will post both required and supplemental (non-required) readings that enrich your understanding for each topic."
  },
  {
    "objectID": "syllabus.html#software",
    "href": "syllabus.html#software",
    "title": "Syllabus",
    "section": "Software",
    "text": "Software\nYou are strongly recommended to download copies of R and R Studio on your own computers. These software packages are available on all computers in the trading room, and you will have access to them during the week to work on assignments.\nWe will also have a shared class workspace in RStudio.cloud that runs a full instance of R Studio in your web browser (so no need to install anything!) will let you access files and assignments."
  },
  {
    "objectID": "syllabus.html#attendence",
    "href": "syllabus.html#attendence",
    "title": "Syllabus",
    "section": "Attendence",
    "text": "Attendence\nYour day-to-day classroom attendance is not graded. My philosophy is that you are all adults and must take ownership of your own learning or else you will not succeed. Some assignments may require in-class participation for credit, and an (unexcused) absence may be detrimental to your grade. Attending class is one of the strongest predictors of success.\nHowever, as required under Hood College’s “Promise of Fall Plan,” (Ch. 2-C) your classroom attendance will be recorded at every class meeting. This is primarily to facilitate contact tracing.\nIf you know you will be absent, you are not required to let me know, but it is polite to give notice (Note if I do not reply to an email of yours letting me know, I am probably busy but will still see it and appreciate your email). Your absence will be noted and recorded for the purposes stated above. If, however, we have an assignment due in class, you must notify me ahead of time in order to make alternate arrangements to still receive credit. Hasty ex-post attempts to notify me will generate little sympathy."
  },
  {
    "objectID": "syllabus.html#late-assignments",
    "href": "syllabus.html#late-assignments",
    "title": "Syllabus",
    "section": "Late Assignments",
    "text": "Late Assignments\nI will accept late assignments, but will subtract a specified amount of points as a penalty. Even if it is the last week of the semester, I encourage you to turn in late work: some points are better than no points!\nHomeworks: If you turn in a homework after it is due but before it is graded or the answer key posted, I generally will not take off any points. However, if you turn in a homework after the answer key is posted, I will automatically deduct 15 points (so the maximum grade you can earn on it is an 80).\nExams: If you know that you will be unable to complete an in-class exam as scheduled for a legitimate reason, please notify me at least one week in advance, and we will schedule a make-up exam date. Failure to do so, including desperate attempts to make arrangements only after the exam will result in a grade of 0 and little sympathy.\nPapers: Starting at the deadline, I will take off 1 point for every hour that your assignment is late.\nI reserve the right to re-weight assignments for students whom I believe are legitimately unable to complete a particular assignment."
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nI will try my best to post grades on Blackboard’s Grading Center and return graded assignments to you within about one week of you turning them in. There will be exceptions. Where applicable, I will post answer keys once I know most homeworks are turned in (see Late Assignments above for penalties). Blackboard’s Grading Center is the place to look for your most up-to-date grades. See also my  Grade Calculator app where you can calculate your overall grade using existing assignment grades and forecast “what if” scenarios."
  },
  {
    "objectID": "syllabus.html#communication-email-slack-and-virtual-office-hours",
    "href": "syllabus.html#communication-email-slack-and-virtual-office-hours",
    "title": "Syllabus",
    "section": "Communication: Email, Slack, and Virtual Office Hours",
    "text": "Communication: Email, Slack, and Virtual Office Hours\nStudents must regularly monitor their Hood email accounts to receive important college information, including messages related to this class. Email through the Blackboard system is my main method of communicating announcements and deadlines regarding your assignments. Please do not reply to any automated Blackboard emails - I may not recieve it!. My Hood email (safner@hood.edu) is the best means of contacting me. I will do my best to respond within 24 hours. If I do not reply within 48 hours, do not take it personally, and feel free to send a follow up email in the very likely event that I genuinely did not see your original message.\nOur slack channel is available to all students and faculty in Economics and Business. I have invited all of my classes and advisees. It will not be extended to non-Business/Economics students or faculty. All users must use their hood emails and true first and last names. Each course has its own channel, exclusive for verified students in the course, and myself, by my invite only. As a third party platform, you agree to its Terms of Service. I have created this space as a way to stay connected, to help one another, and to foster community. Behaviors such as posting inappropriate content, harassing others, or engaging in academic dishonesty, to be determined solely at my discretion, will result in one warning, the content will be deleted, and subsequent behavior will result in a ban.\nIn addition to in-person office hours, you can also make an appointment for “office hours” on Zoom. You can join in with video, audio, and/or chat, whichever you feel comfortable with. Of course, if you are not available during those times, we can schedule our own time if you prefer this method over email or Slack. If you want to go over material from class, please have specific questions you want help with. I am not in the business of giving private lectures (particularly if you missed class without a valid excuse).\nWatch this excellent and accurate video explaining office hours:"
  },
  {
    "objectID": "syllabus.html#netiquette",
    "href": "syllabus.html#netiquette",
    "title": "Syllabus",
    "section": "Netiquette",
    "text": "Netiquette\nWhen using Zoom and Slack, please follow appropriate internet etiquette (“Netiquette”). Written communications, like blog posts or use of the Zoom chat, lacks important nonverbal cues (such as body language, tone of voice, sarcasm, etc).\nAbove all else, please respect one another and think/reread carefully about how others may see your post before you submit a comment. You are expected to disagree and have different opinions, this is inherently valuable in a discussion. Please be civil and constructive in responding to others’ comments: writing “have you considered ‘X’?” is a lot more helpful to all involved than just writing “well you’re just wrong.”\nPosting content that is wilfully incindiary, illegal, or that constitutes academic dishonesty (such as plagarism) will automatically earn a grade of 0 and may be elevated to other authorities on campus.\nWhen using the chat function on Zoom or public Slack channels, please treat it as official course communications, even though I may not be grading it. It may be a quick and informal tool - don’t feel you need to worry about spelling or perfect grammar - but please try to avoid too informal “text-speak” (i.e. say “That’s good for you” instead of “thas good 4 u”)."
  },
  {
    "objectID": "syllabus.html#privacy",
    "href": "syllabus.html#privacy",
    "title": "Syllabus",
    "section": "Privacy",
    "text": "Privacy\nMaryland law requires all parties consent for a conversation or meeting to be recorded. If you join in, and certainly if you participate, you are consenting to be recorded. However, as described below, videos are not accessible beyond our class.\nLive lectures are recorded on Zoom and posted to Blackboard via Panopto, a secure course management system for video. Among other nice features (such as multiple video screens, close captioning, and time-stamped search functions!), Panopto is authenticated via your Blackboard credentials, ensuring that our course videos are not accessible to the open internet.\n\nFor the privacy of your peers, and to foster an environment of trust and academic freedom to explore ideas, do not record our course lectures or discussions. You are already getting my official copies.\nThe Family Educational Rights and Privacy Act prevents me from disclosing or discussing any student information, including grades and records about student performance. If the student is at least 18 years of age, parents (or spouses) do not have a right to obtain this information, except with consent by the student.\nMany of you may be tuning in remotely, living with parents, and may have occasional interruptions due to sharing a space. This is normal and fine, but know that I will protect your privacy and not discuss your performance when parents (or anyone other than you, for that matter) are present, without your explicit consent."
  },
  {
    "objectID": "syllabus.html#enrollment",
    "href": "syllabus.html#enrollment",
    "title": "Syllabus",
    "section": "Enrollment",
    "text": "Enrollment\nStudents are responsible for verifying their enrollment in this class. The last day to add or drop this class with no penalty is Wednesday, September 1. Be aware of important dates."
  },
  {
    "objectID": "syllabus.html#honor-code",
    "href": "syllabus.html#honor-code",
    "title": "Syllabus",
    "section": "Honor Code",
    "text": "Honor Code\nHood College has an Academic Honor Code which requires all members of this community to maintain the highest standards of academic honesty and integrity. Cheating, plagiarism, lying, and stealing are all prohibited. All violations of the Honor Code are taken seriously, will be reported to appropriate authority, and may result in severe penalties, including expulsion from the college. See here for more detailed information."
  },
  {
    "objectID": "syllabus.html#van-halen-and-mms",
    "href": "syllabus.html#van-halen-and-mms",
    "title": "Syllabus",
    "section": "Van Halen and M&Ms",
    "text": "Van Halen and M&Ms\nWhen you have completed reading the syllabus, email me a picture of the band Van Halen and a picture of a bowl of M&Ms. If you do this before the date of the first exam, you will get bonus points on the exam. If 75-100% of the class does this, you each get 2 points. If 50-75% of the class does this, you each get 4 points. If 25-50% of the class does this, you each get 6 points. If 0-25% of the class does this, you each get 8 points. Yes, you read this correctly."
  },
  {
    "objectID": "syllabus.html#accessibility-equity-and-accommodations",
    "href": "syllabus.html#accessibility-equity-and-accommodations",
    "title": "Syllabus",
    "section": "Accessibility, Equity, and Accommodations",
    "text": "Accessibility, Equity, and Accommodations\nCollege courses can, and should, be challenging and bring you out of your comfort zone in a safe and equitable environment. If, however, you feel at any point in the semester that certain assignments or aspects of the course will be disproportionately uncomfortable or burdensome for you due to any factor beyond your control, please come see me or email me. I am a very understanding person and am happy to work out a solution together. I reserve the right to modify and reweight assignments at my sole discretion for students that I belive would legitimately be at a disadvantage, through no fault of their own, to complete them as described.\nIf you are unable to afford required textbooks or other resources for any reason, come see me and we can find a solution that works for you.\nThis course is intended to be accessible for all students, including those with mental, physical, or cognitive disabilities, illness, injuries, impairments, or any other condition that tends to negatively affect one’s equal access to education. If at any point in the term, you find yourself not able to fully access the space, content, and experience of this course, you are welcome to contact me to discuss your specific needs. I also encourage you to contact the Office of Accessibility Services (301-696-3421). If you have a diagnosis or history of accommodations in high school or previous postsecondary institutions, Accessibility Services can help you document your needs and create an accommodation plan. By making a plan through Accessibility Services, you can ensure appropriate accommodations without disclosing your condition or diagnosis to course instructors."
  }
]