---
format:
  revealjs:
    theme: [default, custom.scss]
    logo: "../images/metrics_hex.png"
    footer: "[ECON 480 — Econometrics](https://metricsF22.classes.ryansafner.com)"
    height: 900
    width: 1600
    df-print: paged
    slide-number: c
    chalkboard: true
overview: true
execute:
  echo: false
  warning: false
  freeze: auto
---

##  {data-menu-title="Title Slide" background-image="images/metrics_title_slide.png"}

[2.6 --- Statistical Inference]{.custom-title}

[ECON 480 • Econometrics • Fall 2022]{.custom-subtitle}

[Dr. Ryan Safner <br> Associate Professor of Economics]{.custom-author}

[<a href="mailto:safner@hood.edu"><i class="fa fa-paper-plane fa-fw"></i>safner\@hood.edu</a> <br> <a href="https://github.com/ryansafner/metricsF22"><i class="fa fa-github fa-fw"></i>ryansafner/metricsF22</a><br> <a href="https://metricsF22.classes.ryansafner.com"> <i class="fa fa-globe fa-fw"></i>metricsF22.classes.ryansafner.com</a><br>]{.custom-institution}

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(broom)
library(haven)
library(kableExtra)
library(patchwork)
library(fontawesome)
library(gapminder)
library(ggthemes)
library(scales)
knitr::opts_chunk$set(echo=F,
                      message=F,
                      warning=F)
update_geom_defaults("label", list(family = "Fira Sans Condensed"))
update_geom_defaults("text", list(family = "Fira Sans Condensed"))

```

```{r}
ca_school <- read_dta("../files/data/CAschool.dta")
scatter <- ggplot(data = ca_school)+
  aes(x = str,
      y = testscr)+
  geom_point(color = "blue")+
  labs(x = "Student to Teacher Ratio",
       y = "Test Score")+
  theme_light(base_family = "Fira Sans Condensed",
           base_size = 20)
school_reg <- lm(testscr ~ str, data = ca_school)
aug_reg <- school_reg %>% augment()
school_reg_tidy <- tidy(school_reg)
```

## Contents {background-color="#314f4f"}

[Variation in $\hat{\beta}_1$](#variation-in-hatbeta_1)

[Presenting Regression Results](#presenting-regression-results)

[Diagnostics About Regression](#diagnostics-about-regression)

[Heteroskedasticity](#heteroskedasticity)

[Outliers](#outliers)

# Why Uncertainty Matters {.centered background-color="#314f4f"}

## Recall: Two Big Problems with Data

::: columns
::: {.column width="60%"}
- We use econometrics to [identify]{.hi} causal relationships & make [inferences]{.hi-purple} about them:

1. Problem for [identification]{.hi}: [endogeneity]{.hi}
  - $X$ is **exogenous** if $cor(x, u) = 0$
  - $X$ is **endogenous** if $cor(x, u) \neq 0$
    
2. Problem for [inference]{.hi-purple}: [randomness]{.hi-purple}
    - Data is random due to **natural sampling variation**
    - Taking one sample of a population will yield slightly different information than another sample of the same population

:::

::: {.column width="40%"}

![](images/causality.jpg){width="450" fig-align="center"}

![](images/randomimage.jpg){width="450" fig-align="center"}
:::
:::


## Distributions of the OLS Estimators

$$Y_i = \beta_0+\beta_1 X_i+u_i$$

- OLS estimators $(\hat{\beta_0}$ and $\hat{\beta_1})$ are computed from a finite (specific) sample of data

- Our OLS model contains **2 sources of randomness**:

. . .

- [*Modeled* randomness]{.hi}: population $u_i$ includes all factors affecting $Y$ *other* than $X$
    - different samples will have different values of those other factors $(u_i)$

. . .

- [*Sampling* randomness]{.hi-purple}: different samples will generate different OLS estimators
    - Thus, $\hat{\beta_0}, \hat{\beta_1}$ are *also* **random variables**, with their own [sampling distribution]{.hi-purple}

## The Two Problems: Where We're Heading...Ultimately

[[Sample]{.b} $\color{#6A5ACD}{\xrightarrow{\text{statistical inference}}}$ [Population]{.b} $\color{#e64173}{\xrightarrow{\text{causal indentification}}}$ [Unobserved Parameters]{.b}]{.center}

<br>

. . .

- We want to [identify]{.hi} causal relationships between **population** variables
  - Logically first thing to consider
  - [Endogeneity problem]{.hi-purple}

. . .

- We'll use **sample** *statistics* to [infer]{.hi-purple} something about population *parameters*
  - In practice, we'll only ever have a finite *sample distribution* of data
  - We *don't* know the *population distribution* of data
  - [Randomness problem]{.hi-purple}

## Why Sample vs. Population Matters

::: columns
::: {.column width="50%"}
```{R, gen-dataset, include = F, cache = T}
library(parallel)

# Set population and sample sizes
n_p <- 100
n_s <- 30

# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 5, sd = 1.5),
  e = rnorm(n_p, mean = 0, sd = 1),
  y = i + 0.5 * x + e,
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)

# Regressions
lm0 <- lm(y ~ x, data = pop_df)
lm1 <- lm(y ~ x, data = filter(pop_df, s1 == T))
lm2 <- lm(y ~ x, data = filter(pop_df, s2 == T))
lm3 <- lm(y ~ x, data = filter(pop_df, s3 == T))

# Simulation
sim_df <- mclapply(mc.cores = 10, X = 1:1e4, FUN = function(x, size = n_s) {
  lm(y ~ x, data = pop_df %>% sample_n(size = size)) %>% tidy()
  }) %>% 
  do.call(rbind, .) %>%
  as_tibble()
```

```{r}
ggplot(data = pop_df, aes(x = row, y = col)) +
geom_point(color = "darkslategray", size = 10) +
theme_void()
```

**Population**
:::
::: {.column width="50%"}
```{r}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = "#e64173", size = 3
) +
geom_point(color = "darkslategray", size = 6) +
theme_void()

```

**Population relationship**
<br>
$Y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` X_i + u_i$

$Y_i = \beta_0 + \beta_1 X_i + u_i$

:::
:::

## Why Sample vs. Population Matters

::: columns
::: {.column width="50%"}
```{r}
ggplot(data = pop_df, aes(x = row, y = col, shape = s1)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_void()+
  theme(legend.position = "FALSE")
```

**Sample 1:** 30 random individuals
:::
::: {.column width="50%"}
```{r}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = "#e64173", size = 3, alpha = 0.3
) +
geom_point(aes(shape = s1), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_void()+
  theme(legend.position = "FALSE")

```

**Population relationship**
<br>
$Y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` X_i + u_i$

**Sample relationship**
<br>
$\hat{Y}_i = `r round(lm1$coefficients[1], 2)` + `r round(lm1$coefficients[2], 2)` X_i$

:::
:::

## Why Sample vs. Population Matters

::: columns
::: {.column width="50%"}
```{r}
ggplot(data = pop_df, aes(x = row, y = col, shape = s2)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_void()+
  theme(legend.position = "FALSE")
```

**Sample 2:** 30 random individuals
:::
::: {.column width="50%"}
```{r}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = "#e64173", size = 3, alpha = 0.3
) +
geom_point(aes(shape = s2), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_void()+
  theme(legend.position = "FALSE")

```

**Population relationship**
<br>
$Y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` X_i + u_i$

**Sample relationship**
<br>
$\hat{Y}_i = `r round(lm2$coefficients[1], 2)` + `r round(lm2$coefficients[2], 2)` X_i$

:::
:::

## Why Sample vs. Population Matters

::: columns
::: {.column width="50%"}
```{r}
ggplot(data = pop_df, aes(x = row, y = col, shape = s3)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_void()+
  theme(legend.position = "FALSE")
```

**Sample 3:** 30 random individuals
:::
::: {.column width="50%"}
```{r}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = "#e64173", size = 3, alpha = 0.3
) +
geom_point(aes(shape = s3), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm3$coefficients[1], slope = lm3$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_void()+
  theme(legend.position = "FALSE")

```

**Population relationship**
<br>
$Y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` X_i + u_i$

**Sample relationship**
<br>
$\hat{Y}_i = `r round(lm3$coefficients[1], 2)` + `r round(lm3$coefficients[2], 2)` X_i$

:::
:::

## Why Sample vs. Population Matters

::: columns
::: {.column width="50%"}
- Let's repeat this process **10,000 times**!

- This exercise is called a [(Monte Carlo) simulation]{.hi-purple}
  - I'll show you how to do this next class with the `infer` package

:::
::: {.column width="50%"}
```{r}
#| output-location: fragment
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
sim_scatter <- ggplot() +
geom_abline(data = line_df, aes(intercept = intercept, slope = slope), alpha = 0.01) +
geom_point(data = pop_df, aes(x = x, y = y), size = 6, color = "darkslategray") +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = "#e64173", size = 3
) +
theme_void()

sim_scatter
```
:::
:::

## Why Sample vs. Population Matters

::: columns
::: {.column width="50%"}
- **_On average_**, estimated regression lines from (hypothetical) samples provide an unbiased estimate of true population regression line
$$\mathbb{E}[\hat{\beta}_1] =  \beta_1$$

- But, any *individual* estimate can miss the mark

- This leads to [uncertainty]{.hi-purple} about our estimated regression line
  - We only have *1* sample in reality!
  - This is why we care about the [standard error]{.hi} of our line: $se(\hat{\beta_1})$!
:::
::: {.column width="50%"}
```{r}

sim_scatter
```
:::
:::

# Confidence Intervals {.centered background-color="#314f4f"}

## Statistical Inference

[[Sample]{.b} $\color{#6A5ACD}{\xrightarrow{\text{statistical inference}}}$ [Population]{.b} $\color{#e64173}{\xrightarrow{\text{causal indentification}}}$ [Unobserved Parameters]{.b}]{.center}

. . .

- We want to start [inferring]{.hi-purple} what the true population regression model is, using our estimated regression model from our sample

. . .

$$\hat{Y_i}=\hat{\beta_0}+\hat{\beta_1}X \color{#6A5ACD}{\xrightarrow{\text{🤞 hopefully 🤞}}} Y_i=\beta_0+\beta_1X+u_i$$ 

- We can’t yet make [*causal* inferences]{.hi} about whether/how $X$ *causes* $Y$
  - coming after the midterm!

## Estimation and Statistical Inference

::: columns
::: {.column width="50%"}
- Our problem with [uncertainty]{.hi-purple} is we don’t know whether our sample estimate is *close* or *far* from the unknown population parameter

- But we can use our errors to learn how well our model statistics likely estimate the true parameters

- Use $\hat{\beta_1}$ and its standard error, $se(\hat{\beta_1})$ for statistical inference about true $\beta_1$

- We have two options...

:::
::: {.column width="50%"}
![](images/randomimage.jpg){width="450" fig-align="center"}
:::
:::

## Estimation and Statistical Inference


::: columns
::: {.column width="50%"}
![**Point estimate**](images/spearfishing.png){width="600" fig-align="center"}

- Use our $\hat{\beta_1}$ & $se(\hat{\beta_1})$ to determine if statistically significant evidence to reject a hypothesized $\beta_1$

:::
::: {.column width="50%"}

![**Confidence Interval**](images/castnet2.jpg){width="600" fig-align="center"}

- Use our $\hat{\beta_1}$ & $se(\hat{\beta_1})$ to create a *range* of values that gives us a good chance of capturing the true $\beta_1$

:::
:::

## Accuracy vs. Precision

![](images/garfieldweather.png){width="1000" fig-align="center"}


## Generating Confidence Intervals

::: columns
::: {.column width="50%"}
- We can generate our confidence interval by generating a [“bootstrap”]{.hi-purple} sampling distribution:
    - Take our sample data and resample it many times by selecting random observations and then replacing them

- This allows us to approximate the sampling distribution of $\hat{\beta_1}$ by simulation!

![](images/bootstraps.png){fig-align="center"}
:::
::: {.column width="50%"}
```{r}
beta_dist <- ggplot(data = tibble(x=-4:4))+
  aes(x = x)+
  stat_function(fun = dnorm, geom="area", fill = "gray", alpha = 0.5)+
  geom_segment(aes(x=0,xend=0, y=0, yend=0.4), linetype="dashed")+
  scale_y_continuous(limits = c(0,0.4),
                     expand = c(0,0))+
  scale_x_continuous(breaks = c(0),
                     labels = expression(E(hat(beta[1]))))+
  labs(x = expression(hat(beta[1])),
       y = "Probability")+
  theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)+
  theme(axis.title.x = element_text(hjust=0.9))
beta_dist

```
:::
:::

# Confidence Intervals Using the `infer` Package {.centered background-color="#314f4f"}

## Confidence Intervals Using the `infer` Package I

::: columns
::: {.column width="30%"}
![](images/infer_gnome.png)
:::
::: {.column width="70%"}

- The `infer` package allows you to do statistical inference in a *tidy* way, following the philosophy of the `tidyverse`

```{r}
#| echo: true
# install.packages("infer")

# load
library(infer)
```
:::
:::

## Confidence Intervals Using the `infer` Package II

::: columns
::: {.column width="30%"}
![](images/infer_gnome.png)
:::
::: {.column width="70%"}

- `infer` allows you to run through these steps manually to understand the process:

1. `specify()` a model

2. `generate()` a bootstrap distribution 

3. `calculate()` the confidence interval

4. `visualize()` with a histogram (optional)
:::
:::

## Confidence Intervals Using the `infer` Package III

![](images/infer0jpeg.jpeg){fig-align="center"}

## Confidence Intervals Using the `infer` Package III

![](images/infer1jpeg.jpeg){fig-align="center"}

## Confidence Intervals Using the `infer` Package III

![](images/infer2nohyp.jpeg){fig-align="center"}

## Confidence Intervals Using the `infer` Package III

![](images/infer3nohyp.jpeg){fig-align="center"}

## Confidence Intervals Using the `infer` Package III

![](images/infer4nohyp.jpeg){fig-align="center"}

## Bootstrapping

::: columns
::: {.column width="50%"}
### Our Sample

```{r}
school_reg %>% tidy()
```
:::
::: {.column width="50%"}
### Another “Sample”

```{r}
ca_school %>%
  specify(testscr ~ str) %>%
  generate(reps = 1,
           type = "bootstrap") %>%
  lm(testscr ~ str, data = .) %>%
  tidy()

```

👆 Bootstrapped from Our Sample

:::
:::

. . .

- Now we want to do this 1,000 times to simulate the (unknown) sampling distribution of $\hat{\beta_1}$

## The `infer` Pipeline: `specify()`

![](images/infer1jpeg.jpeg){fig-align="center"}

## The `infer` Pipeline: `specify()`

::: columns
::: {.column width="30%"}
### Specify

`data %>%`

`  specify(y ~ x)`

:::
::: {.column width="70%"}
- Take our data and pipe it into the `specify()` function, which is essentially a `lm()` function for regression (for our purposes)

```{r}
#| echo: true
#| eval: false
ca_school %>%
  specify(testscr ~ str)
```

```{r}
ca_school %>%
  specify(testscr ~ str) %>%
  rmarkdown::paged_table(., options = list(rows.print = 5))
```

:::
:::

## The `infer` Pipeline: `generate()`

![](images/infer2nohyp.jpeg){fig-align="center"}

## The `infer` Pipeline: `generate()`

::: columns
::: {.column width="30%"}
### Specify

### Generate

`%>%`
` generate(reps = n,`
`          type = "bootstrap")`

:::
::: {.column width="70%"}
- Now the magic starts, as we run a number of simulated samples

- Set the number of `reps` and set `type` to `"bootstrap"`

```{r}
#| echo: true
#| eval: false
ca_school %>%
  specify(testscr ~ str) %>%
  generate(reps = 1000, #<<
           type = "bootstrap") #<<
```
:::
:::

## The `infer` Pipeline: `generate()`

::: columns
::: {.column width="30%"}
### Specify

### Generate

`%>%`
` generate(reps = n,`
`          type = "bootstrap")`

:::
::: {.column width="70%"}
- Now the magic starts, as we run a number of simulated samples

- Set the number of `reps` and set `type` to `"bootstrap"`

```{r}
ca_school %>%
  specify(testscr ~ str) %>%
  generate(reps = 1000, #<<
           type = "bootstrap") %>%
  rmarkdown::paged_table(., options = list(rows.print = 5))
```

- `replicate`: the “sample” number (1-1000)

- creates `x` and `y` values (data points)

:::
:::

## The `infer` Pipeline: `calculate()`

![](images/infer3nohyp.jpeg){fig-align="center"}

## The `infer` Pipeline: `calculate()`

::: columns
::: {.column width="30%"}
### Specify

### Generate

### Calculate
`%>%`
` calculate(stat = "slope")`

:::
::: {.column width="70%"}
```{r}
#| eval: false
#| echo: true
ca_school %>%
  specify(testscr ~ str) %>%
  generate(reps = 1000,
           type = "bootstrap") %>%
  calculate(stat = "slope") #<<
```

- For each of the 1,000 replicates, calculate `slope` in `lm(testscr ~ str)`

- Calls it the `stat`

:::
:::

## The `infer` Pipeline: `calculate()`

::: columns
::: {.column width="30%"}
### Specify

### Generate

### Calculate
`%>%`
` calculate(stat = "slope")`

:::
::: {.column width="70%"}
```{r}
ca_school %>%
  specify(testscr ~ str) %>%
  generate(reps = 1000,
           type = "bootstrap") %>%
  calculate(stat = "slope") #<<
```

:::
:::

## The `infer` Pipeline: `calculate()`

::: columns
::: {.column width="30%"}
### Specify

### Generate

### Calculate
`%>%`
` calculate(stat = "slope")`

:::
::: {.column width="70%"}
```{r}
#| echo: true
boot <- ca_school %>%
  specify(testscr ~ str) %>%
  generate(reps = 1000,
           type = "bootstrap") %>%
  calculate(stat = "slope")
```

- `boot` is (our simulated) sampling distribution of $\hat{\beta_1}$!

- We can now use this to estimate the confidence interval from *our* $\hat{\beta_1}=-2.28$

- And visualize it

:::
:::

## Confidence Interval

::: columns
::: {.column width="50%"}
- A 95% confidence interval is the middle 95% of the sampling distribution

```{r}
#| echo: true
ci <- boot %>%
  summarize(lower = quantile(stat, 0.025),
            upper = quantile(stat, 0.975))
ci

```
:::
::: {.column width="50%"}
```{r}
#| echo: true
sampling_dist <- ggplot(data = boot)+
  aes(x = stat)+
  geom_histogram(color="white", fill = "#e64173")+
  labs(x = expression(hat(beta[1])))+
  theme_pander(base_family = "Fira Sans Condensed",
           base_size=20)

sampling_dist

```
:::
:::

## Confidence Interval

::: columns
::: {.column width="50%"}
- A 95% confidence interval is the middle 95% of the sampling distribution

```{r}
#| echo: true
ci <- boot %>%
  summarize(lower = quantile(stat, 0.025),
            upper = quantile(stat, 0.975))
ci

```
:::
::: {.column width="50%"}
```{r}
#| echo: true
sampling_dist+
  geom_vline(data = ci, aes(xintercept = lower), size = 1, linetype="dashed")+ #<<
  geom_vline(data = ci, aes(xintercept = upper), size = 1, linetype="dashed") #<<

```
:::
:::

## The `infer` Pipeline: `get_confidence_interval()`

::: columns
::: {.column width="30%"}
### Specify

### Generate

### Calculate

### Get Confidence Interval

`%>%`
` get_confidence_interval()`

:::
::: {.column width="70%"}
```{r}
#| echo: true
ca_school %>% #<< # save this
  specify(testscr ~ str) %>%
  generate(reps = 1000,
           type = "bootstrap") %>%
  calculate(stat = "slope") %>%
  get_confidence_interval(level = 0.95, #<<
                          type = "se", #<<
                          point_estimate = -2.28) #<<
```

:::
:::

## Broom Can Estimate a Confidence Interval

```{r}
#| echo: true
#| output-location: fragment
school_reg %>% 
  tidy(conf.int = T)
```

<br>

. . .

```{r}
#| echo: true
#| output-location: fragment
our_CI <- school_reg %>% 
  tidy(conf.int = T) %>%
  filter(term == "str") %>%
  select(conf.low, conf.high)

our_CI
```

::: footer
:::

## The `infer` Pipeline: `visualize()`

::: columns
::: {.column width="30%"}
### Specify

### Generate

### Calculate

### Visualize

`%>%`
` visualize()`

:::
::: {.column width="70%"}
```{r}
#| echo: true
ca_school %>%
  specify(testscr ~ str) %>%
  generate(reps = 1000,
           type = "bootstrap") %>%
  calculate(stat = "slope") %>%
  visualize() #<<
```

:::
:::


## The `infer` Pipeline: `visualize()`

::: columns
::: {.column width="30%"}
### Specify

### Generate

### Calculate

### Visualize

`%>%`
` visualize()`

:::
::: {.column width="70%"}

- If we have our confidence levels saved (`our_CI`) we can `shade_ci()` in `infer`'s `visualize()` function


```{r}
#| echo: true
ca_school %>%
  specify(testscr ~ str) %>%
  generate(reps = 1000,
           type = "bootstrap") %>%
  calculate(stat = "slope") %>%
  visualize()+
  shade_ci(endpoints = our_CI) 
```

:::
:::

::: footer
:::


. . .

- `visualize()` is just a wrapper for `ggplot()`

# Confidence Intervals, Theory

## Confidence Intervals, Theory

::: columns
::: {.column width="50%"}
- In general, a [confidence interval (CI)]{.hi} takes a point estimate and extrapolates it within some [margin of error (MOE)]{.hi-purple}:

$\bigg( \big[$ estimate - MOE $\big]$, $\big[$ estimate + MOE $\big] \bigg)$

- The main question is, [how confident do we want to be]{.hi-turquoise} that our interval contains the true parameter?
  - Larger confidence level, larger margin of error (and thus larger interval)

:::
::: {.column width="50%"}
![](images/castnet2.jpg)
:::
:::

## Confidence Intervals, Theory


::: columns
::: {.column width="50%"}
- $\color{#6A5ACD}{(1- \alpha)}$ is the .hi-purple[confidence level] of our confidence interval
  - $\color{#6A5ACD}{\alpha}$ is the .hi-purple[“significance level”] that we use in hypothesis testing
  - $\color{#6A5ACD}{\alpha}=$ probability that the true parameter is *not* contained within our interval

- Typical levels: 90%, 95%, 99%
  - 95% is especially common, $\alpha=0.05$

:::
::: {.column width="50%"}
![](images/castnet2.jpg)
:::
:::

## Confidence Levels

::: columns
::: {.column width="50%"}
- Depending on our confidence level, we are essentially looking for the middle $(1-\alpha)$% of the sampling distribution

- This puts $\alpha$ in the tails; $\frac{\alpha}{2}$ in each tail 

:::
::: {.column width="50%"}
```{r}
ggplot(data = tibble(x=-4:4))+
  aes(x = x)+
  stat_function(fun = dt, args=list(1000), size=2, geom="area", fill="gray", alpha=0.5)+
  stat_function(fun = dt, args=list(1000), size=2, xlim=c(-1.96,1.96), geom="area", fill="#e64173", alpha=1)+
  #stat_function(fun = dt, args=list(1000), size=2, xlim=c(-4,-1.96), geom="area", fill="#e64173")+
  #stat_function(fun = dt, args=list(1000), size=2, xlim=c(1.96, 4), geom="area", fill="#e64173")+
  labs(x = expression(paste("Sampling Distribution of "),hat(beta[1])),
       y = "Probability")+
  annotate("segment", x = -1.96, xend = 1.96, y = 0.10, yend = 0.10, colour = "#000000", size=0.5, linetype = 2, alpha=1, arrow=arrow(length=unit(0.25,"cm"), ends="both", type="closed"))+
  annotate("text", x = 0, y = 0.11, label = expression(1-alpha), color="#000000")+
  
  annotate("segment", x = -4, xend = -1.96, y = 0.05, yend = 0.05, colour = "#000000", size=0.5, linetype = 2, alpha=1, arrow=arrow(length=unit(0.25,"cm"), ends="both", type="closed"))+
  annotate("text", x = -3, y = 0.075, label = expression(frac(alpha,2)), color="#000000")+
  
  annotate("segment", x = 1.96, xend = 4, y = 0.05, yend = 0.05, colour = "#000000", size=0.5, linetype = 2, alpha=1, arrow=arrow(length=unit(0.25,"cm"), ends="both", type="closed"))+
  annotate("text", x = 3, y = 0.075, label = expression(frac(alpha,2)), color="#000000")+
  scale_x_continuous(breaks=NULL)+
  scale_y_continuous(breaks=NULL,
                     limits=c(0,0.45),
                     expand=c(0,0))+
  theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)
```

:::
:::

## Confidence Levels and the Empirical Rule

::: columns
::: {.column width="50%"}
- Recall the .hi-purple[68-95-99.7% empirical rule] for (standard) normal distributions!^[I’m playing fast and loose here, we can’t actually use the normal distribution, we use the Student’s t-distribution with n-k-1 degrees of freedom. But there’s no need to complicate things you don’t need to know about. Look at today’s [appendix](/resources/appendices/2.6-appendix) for more.]

- 95% of data falls within 2 standard deviations of the mean

- Thus, in 95% of samples, the true parameter is likely to fall within *about* 2 standard deviations of the sample estimate

:::
::: {.column width="50%"}
```{r}
ggplot(data = tibble(x=-4:4))+
  aes(x = x)+
  stat_function(fun = dnorm, size=2, geom="area", fill="gray", alpha=0.5)+
  stat_function(fun = dnorm, size=2, xlim=c(-2,2), geom="area", fill="#e64173")+
  labs(x = "Z (Standard Deviations from mean)",
       y = "Probability")+
  annotate("segment", x = -2, xend = 2, y = 0.10, yend = 0.10, colour = "#000000", size=0.5, linetype = 2, alpha=1, arrow=arrow(length=unit(0.25,"cm"), ends="both", type="closed"))+
  annotate("text", x = 0, y = 0.11, label = "95%", color="#000000")+
  annotate("segment", x = -4, xend = -1.96, y = 0.05, yend = 0.05, colour = "#000000", size=0.5, linetype = 2, alpha=1, arrow=arrow(length=unit(0.25,"cm"), ends="both", type="closed"))+
  annotate("text", x = -3, y = 0.075, label = "2.5%", color="#000000")+
  
  annotate("segment", x = 1.96, xend = 4, y = 0.05, yend = 0.05, colour = "#000000", size=0.5, linetype = 2, alpha=1, arrow=arrow(length=unit(0.25,"cm"), ends="both", type="closed"))+
  annotate("text", x = 3, y = 0.075, label = "2.5%", color="#000000")+
  scale_x_continuous(breaks=seq(-4,4,1))+
  scale_y_continuous(breaks=NULL,
                     limits=c(0,0.45),
                     expand=c(0,0))+
  theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)
```
:::
:::

## Interpretting Confidence Intervals

::: columns
::: {.column width="50%"}
:::
::: {.column width="50%"}
:::
:::