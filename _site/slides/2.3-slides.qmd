---
format:
  revealjs:
    theme: [default, custom.scss]
    logo: "../images/metrics_hex.png"
    footer: "[ECON 480 — Econometrics](https://metricsF22.classes.ryansafner.com)"
    height: 900
    width: 1600
    #df-print: paged
    slide-number: c
    chalkboard: true
overview: true
execute:
  echo: false
  warning: false
  freeze: auto
---

##  {data-menu-title="Title Slide" background-image="images/metrics_title_slide.png"}

[2.3 --- Simple Linear Regression]{.custom-title}

[ECON 480 • Econometrics • Fall 2022]{.custom-subtitle}

[Dr. Ryan Safner <br> Associate Professor of Economics]{.custom-author}

[<a href="mailto:safner@hood.edu"><i class="fa fa-paper-plane fa-fw"></i>safner\@hood.edu</a> <br> <a href="https://github.com/ryansafner/metricsF22"><i class="fa fa-github fa-fw"></i>ryansafner/metricsF22</a><br> <a href="https://metricsF22.classes.ryansafner.com"> <i class="fa fa-globe fa-fw"></i>metricsF22.classes.ryansafner.com</a><br>]{.custom-institution}

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(kableExtra)
library(patchwork)
library(fontawesome)
library(gapminder)
library(ggthemes)
library(scales)
knitr::opts_chunk$set(echo=F,
                      message=F,
                      warning=F)
update_geom_defaults("label", list(family = "Fira Sans Condensed"))
update_geom_defaults("text", list(family = "Fira Sans Condensed"))

```

## Contents {background-color="#314f4f"}

[Exploring Relationships](#exploring-relationships)

[Quantifying Relationships](#quantifying-relationships)

[Linear Regression](#linear-regression)

[Deriving OLS Estimators](#deriving-ols-estimators)

[Our Class Size Example in R](#our-class-size-example-in-r)

# Exploring Relationships {.centered background-color="#314f4f"}

## Bivariate Data and Relationships I

::: columns
::: {.column width="50%"}
- We looked at single variables for descriptive statistics
- Most uses of statistics in economics and business investigate relationships *between* variables

::: callout-tip
## Examples

  - \# of police & crime rates
  - healthcare spending & life expectancy
  - government spending & GDP growth
  - carbon dioxide emissions & temperatures
:::
:::

::: {.column width="50%"}
![](images/statsgraphs.jpg)
:::
:::

## Bivariate Data and Relationships II

::: columns
::: {.column width="50%"}
- We will begin with [bivariate]{.hi-purple} data for relationships between $X$ and $Y$

- Immediate aim is to explore [associations]{.hi} between variables, quantified with [correlation]{.hi} and [linear regression]{.hi}

- Later we want to develop more sophisticated tools to argue for [causation]{.hi}
:::

::: {.column width="50%"}
![](images/statsgraphs.jpg)
:::
:::

## Bivariate Data: Spreadsheets I {.smaller}

```{r}
#| echo: false
econfreedom <- read_csv("../files/data/econfreedom.csv")

econfreedom %>%
  rmarkdown::paged_table()
```

- **Rows** are individual observations (countries)
- **Columns** are variables on all individuals


## Bivariate Data: Spreadsheets II

```{r}
#| echo: true
econfreedom %>%
  glimpse()
```

## Bivariate Data: Spreadsheets III

```{r}
#| echo: true
#| eval: false

source("summaries.R")
econfreedom %>%
  summary_table(ef, gdp)
```

```{r}
#| echo: false
#| eval: true

source("../files/summaries.R")
econfreedom %>%
  summary_table(ef, gdp) %>%
  rmarkdown::paged_table()
```

## Bivariate Data: Scatterplots I

::: {.panel-tabset}
## Plot

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 14
scatter <- ggplot(data = econfreedom)+
  aes(x = ef,
      y = gdp)+
  geom_point(aes(color = continent),
             size = 2)+
  labs(x = "Economic Freedom Index (2014)",
       y = "GDP per Capita (2014 USD)",
       color = "")+
  scale_y_continuous(labels = scales::dollar)+
  theme_pander(base_family = "Fira Sans Condensed",
           base_size=20)+
  theme(legend.position = "bottom")
scatter
```

## Code

```{r}
#| echo: true
#| eval: false

ggplot(data = econfreedom)+
  aes(x = ef,
      y = gdp)+
  geom_point(aes(color = continent),
             size = 2)+
  labs(x = "Economic Freedom Index (2014)",
       y = "GDP per Capita (2014 USD)",
       color = "")+
  scale_y_continuous(labels = scales::dollar)+
  theme_pander(base_family = "Fira Sans Condensed",
           base_size=20)+
  theme(legend.position = "bottom")
```
:::

## Bivariate Data: Scatterplots II

::: columns
::: {.column width="50%"}
- Look for [association]{.hi-purple} between independent and dependent variables

1. [Direction]{.hi-turquoise}: is the trend positive or negative?

2. [Form]{.hi-turquoise}: is the trend linear, quadratic, something else, or no pattern?

3. [Strength]{.hi-turquoise}: is the association strong or weak? 

4. [Outliers]{.hi-turquoise}: do any observations deviate from the trends above? 

:::
::: {.column width="50%"}
```{r}
scatter
```
:::
:::

# Quantifying Relationships {.centered background-color="#314f4f"}

## Covariance

- For any two variables, we can measure their [sample covariance, $cov(X,Y)$]{.hi} or [$s_{X,Y}$]{.hi} to quantify how they vary *together*^[Henceforth we limit all measures to *samples*, for convenience. Population covariance is denoted $\sigma_{X,Y}$]

. . .

$$s_{X,Y}=E\big[(X-\bar{X})(Y-\bar{Y}) \big]$$

. . .

- Intuition: if $x_i$ is above the mean of $X$, would we expect the associated $y_i$:
    - to be **above** the mean of $Y$ also $(X$ and $Y$ covary **positively**)
    - to be **below** the mean of $Y$ $(X$ and $Y$ covary **negatively**)

. . .

- Covariance is a common measure, but the units are meaningless, thus we rarely need to use it so **don't worry about learning the formula**

## Covariance, in R

```{r}
#| echo: true
econfreedom %>%
  summarize(covariance = cov(ef, gdp))
```

. . .

8923 what, exactly?

## Correlation {.smaller}

- Better to *standardize* covariance into a more intuitive concept: [correlation, $r_{X,Y}$]{.hi} $\in [-1, 1]$

. . .

$$r_{X,Y}=\frac{s_{X,Y}}{s_X s_Y}=\frac{cov(X,Y)}{sd(X)sd(Y)}$$

. . .

- Simply weight covariance by the product of the standard deviations of $X$ and $Y$

. . .

- Alternatively, take the average^[Over n-1, a *sample* statistic!] of the product of standardized $(Z$-scores for) each $(x_i,y_i)$ pair:^[See today's [appendix page](/resources/appendices/2.3-appendix) for example code to calculate correlation "by hand" in R using the second method.]

. . .

\begin{align*}
r&=\frac{1}{n-1}\sum^n_{i=1}\bigg(\frac{x_i-\bar{X}}{s_X}\bigg)\bigg(\frac{y_i-\bar{Y}}{s_Y}\bigg)\\
r&=\frac{1}{n-1}\sum^n_{i=1}Z_XZ_Y\\
\end{align*}

## Correlation: Interpretation

::: columns
::: {.column width="50%"}
- Correlation is standardized to 
$$-1 \leq r \leq 1$$

- Negative values $\implies$ negative association

- Positive values $\implies$ positive association

- Correlation of 0 $\implies$ no association

- As $|r| \rightarrow 1 \implies$ the stronger the association

- Correlation of $|r|=1 \implies$ perfectly linear

:::
::: {.column width="50%"}
```{r}
library("MASS")
library("gridExtra")
library("ggthemes")

# generated data for specified correlation: https://stackoverflow.com/questions/13291308/generate-numbers-with-specific-correlation
# colors chosen from http://colorbrewer2.org/#type=diverging&scheme=RdBu&n=6

# corr of -.75 

dfn75 <- mvrnorm(50, mu = c(0,0), Sigma = matrix(c(1,-0.75,-0.75,1), ncol = 2),
               empirical = TRUE)
dfn75<-data.frame(dfn75)
dfn75 <- dfn75 %>%
  rename(y=X1,
         x=X2)

corrn75<-ggplot(dfn75, aes(x=x,
                   y=y))+
  geom_point(color="#b2182b")+
  theme_void()+
  labs(title="Correlation: -0.75")

# corr of -.5 

dfn50 <- mvrnorm(50, mu = c(0,0), Sigma = matrix(c(1,-0.5,-0.5,1), ncol = 2),
               empirical = TRUE)
dfn50<-data.frame(dfn50)
dfn50 <- dfn50 %>%
  rename(y=X1,
         x=X2)

corrn50<-ggplot(dfn50, aes(x=x,
                   y=y))+
  geom_point(color="#ef8a62")+
  theme_void()+
  labs(title="Correlation: -0.50")

# corr of -.25 

dfn25 <- mvrnorm(50, mu = c(0,0), Sigma = matrix(c(1,-0.25,-0.25,1), ncol = 2),
               empirical = TRUE)
dfn25<-data.frame(dfn25)
dfn25 <- dfn25 %>%
  rename(y=X1,
         x=X2)

corrn25<-ggplot(dfn25, aes(x=x,
                   y=y))+
  geom_point(color="#fddbc7")+
  theme_void()+
  labs(title="Correlation: -0.25")

# corr of 0.25

df25 <- mvrnorm(50, mu = c(0,0), Sigma = matrix(c(1,0.25,0.25,1), ncol = 2),
               empirical = TRUE)
df25<-data.frame(df25)
df25 <- df25 %>%
  rename(y=X1,
         x=X2)

corr25<-ggplot(df25, aes(x=x,
                   y=y))+
  geom_point(color="#d1e5f0")+
  theme_void()+
  labs(title="Correlation: 0.25")

# corr of .50 

df50 <- mvrnorm(50, mu = c(0,0), Sigma = matrix(c(1,0.50,0.50,1), ncol = 2),
               empirical = TRUE)
df50<-data.frame(df50)
df50 <- df50 %>%
  rename(y=X1,
         x=X2)

corr50<-ggplot(df50, aes(x=x,
                   y=y))+
  geom_point(color="#67a9cf")+
  theme_void()+
  labs(title="Correlation: 0.50")

# corr of 0.75

df75 <- mvrnorm(50, mu = c(0,0), Sigma = matrix(c(1,0.75,0.75,1), ncol = 2),
               empirical = TRUE)
df75<-data.frame(df75)
df75 <- df75 %>%
  rename(y=X1,
         x=X2)

corr75<-ggplot(df75, aes(x=x,
                   y=y))+
  geom_point(color="#2166ac")+
  theme_void()+
  labs(title="Correlation: 0.75")

# arrange all on grid
grid.arrange(corrn25, corrn50, corrn75, corr25, corr50, corr75, ncol=3)

```
:::
:::

## Guess the Correlation!

![](images/guessthecorrelation.png){fig-align="center"}

[Guess the Correlation Game](http://guessthecorrelation.com/index.html)

## Correlation and Covariance in R

```{r}
#| echo: true

econfreedom %>%
  summarize(covariance = cov(ef, gdp),
            correlation = cor(ef, gdp))
```

## Correlation and Endogeneity

::: columns
::: {.column width="50%"}
- Your Occasional Reminder: [Correlation does not imply causation!]{.hi}
    - I'll show you the difference in a few weeks (when we can actually talk about causation)

- If $X$ and $Y$ are strongly correlated, $X$ can still be [endogenous]{.hi-purple}!

- See [today's appendix page](/resources/appendix/2.3-appendix) for more on Covariance and Correlation

:::
::: {.column width="50%"}
![](images/causality.jpg)
:::
:::

## Always Plot Your Data!

![](images/DinoSequentialSmaller.gif){fig-align="center"}

# Linear Regression {.centered background-color="#314f4f"}

## Fitting a Line to Data

::: columns
::: {.column width="50%"}
- If an association appears linear, we can estimate the equation of a line that would “fit” the data

$$Y = a + bX$$

- A linear equation describing a line has two parameters:
  - $a$: vertical intercept
  - $b$: slope

:::
::: {.column width="50%"}
```{r}
x <- runif(100,1,9)
e <- rnorm(100,0,2.5)
y <- 12-0.5*x+e
df <- tibble(x,y)

p <- ggplot(df, aes(x=x,y=y))+
  geom_point()+
  scale_x_continuous(breaks = seq(0,20,1),
                     limits = c(0,10),
                     expand = c(0,0))+
  scale_y_continuous(breaks = seq(0,20,2),
                     limits = c(0,20),
                     expand = c(0,0))+
  theme_bw(base_family = "Fira Sans Condensed",
           base_size=20)

p + geom_abline(slope = -1, intercept = 14, color="red", size=1)
  #geom_smooth(method="lm", se=F)
```
:::
:::

## Fitting a Line to Data

::: columns
::: {.column width="50%"}
- If an association appears linear, we can estimate the equation of a line that would “fit” the data

$$Y = a + bX$$

- A linear equation describing a line has two parameters:
  - $a$: vertical intercept
  - $b$: slope

- How do we choose the equation that **best** fits the data? 

:::
::: {.column width="50%"}
```{r}
p+geom_abline(slope=-1, intercept=14, color="red", size=1)+
  geom_abline(slope=-0.125, intercept=10, color="orange", size=1)
```
:::
:::

## Fitting a Line to Data

::: columns
::: {.column width="50%"}
- If an association appears linear, we can estimate the equation of a line that would “fit” the data

$$Y = a + bX$$

- A linear equation describing a line has two parameters:
  - $a$: vertical intercept
  - $b$: slope

- How do we choose the equation that **best** fits the data? 

- This process is called [linear regression]{.hi}

:::
::: {.column width="50%"}
```{r}
p+geom_abline(slope=-1, intercept=14, color="red", size=1)+
  geom_abline(slope=-0.125, intercept=10, color="orange", size=1)+
  geom_smooth(method="lm", se=F, color="green", size=2, fullrange = T)
```
:::
:::

## Population Linear Regression Model

- Linear regression lets us **estimate** the slope of the [population]{.hi-purple} regression line between $X$ and $Y$ using [sample]{.hi-purple} data

- We can make [statistical inferences]{.hi-purple} about what the true population slope coefficient is
  - eventually & hopefully: a [*causal* inference]{.hi-purple}

- $\text{slope}=\frac{\Delta Y}{\Delta X}$: for a 1-unit change in $X$, how many units will this *cause* $Y$ to change?

## Class Size Example

::: columns
::: {.column width="50%"}
::: callout-tip
## Example
What is the relationship between class size and educational performance?
:::

:::
::: {.column width="50%"}
![](images/smallclass.jpg)
:::
:::

## Class Size Example: Data Import

```{r}
#| echo: true


# Load the Data

# install.packages("haven") # install for first use

# Packages
library("haven") # load for importing .dta files

# Import and save as ca_school

ca_school <- read_dta("../files/data/caschool.dta")
```

Data are student-teacher-ratio and average test scores on Stanford 9 Achievement Test for 5th grade students for 420 K-6 and K-8 school districts in California in 1999, (Stock and Watson, 2015: p. 141)

## Class Size Example: Data

```{r}
#| echo: true
ca_school %>%
  glimpse()
```

## Class Size Example: Data {.smaller}

```{r}
#| echo: false
ca_school %>%
  rmarkdown::paged_table()
```

## Class Size Example: Scatterplot

::: {.panel-tabset}
## Plot

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 14
scatter <- ggplot(data = ca_school)+
  aes(x = str,
      y = testscr)+
  geom_point(color = "blue")+
  labs(x = "Student to Teacher Ratio",
       y = "Test Score")+
  theme_pander(base_family = "Fira Sans Condensed",
           base_size = 20)
scatter
```

## Code

```{r}
#| echo: true
#| eval: false

ggplot(data = ca_school)+
  aes(x = str,
      y = testscr)+
  geom_point(color = "blue")+
  labs(x = "Student to Teacher Ratio",
       y = "Test Score")+
  theme_bw(base_family = "Fira Sans Condensed",
           base_size = 20)
```
:::

## Class Size Example: Slope I

::: columns
::: {.column width="50%"}
- If we *change* $(\Delta)$ the class size by an amount, what would we expect the *change* in test scores to be?

$$\beta = \frac{\text{change in test score}}{\text{change in class size}} = \frac{\Delta \text{test score}}{\Delta \text{class size}}$$

- If we knew $\beta$, we could say that changing class size by 1 student will change test scores by $\beta$

:::
::: {.column width="50%"}
![](images/smallclass.jpg)
:::
:::

## Class Size Example: Slope II

::: columns
::: {.column width="50%"}
- Rearranging: 

$$\Delta \text{test score} = \beta \times \Delta \text{class size}$$

:::
::: {.column width="50%"}
![](images/smallclass.jpg)
:::
:::

## Class Size Example: Slope III

::: columns
::: {.column width="50%"}
- Rearranging: 

$$\Delta \text{test score} = \beta \times \Delta \text{class size}$$

- Suppose $\beta=-0.6$. If we shrank class size by 2 students, our model predicts:

\begin{align*}
	\Delta \text{test score} &= -2 \times \beta\\
	\Delta \text{test score} &= -2 \times -0.6\\
	\Delta \text{test score}&= 1.2	\\
\end{align*}

Test scores would improve by 1.2 points, *on average*.
:::
::: {.column width="50%"}
![](images/smallclass.jpg)
:::
:::

## Class Size Example: Slope and Average Effect

::: columns
::: {.column width="50%"}
$$\text{test score} = \beta_0 + \beta_{1} \times \text{class size}$$	

- The line relating class size and test scores has the above equation

- $\beta_0$ is the [vertical-intercept]{.hi}, test score where class size is 0

- $\beta_{1}$ is the [slope]{.hi} of the regression line

- This relationship only holds [on average]{.hi-purple} for all districts in the population, *individual* districts are also affected by other factors
:::
::: {.column width="50%"}
![](images/smallclass.jpg)
:::
:::

## Class Size Example: Marginal Effect

::: columns
::: {.column width="50%"}
- To get an equation that holds for *each* district, we need to include other factors

$$\text{test score} = \beta_0 + \beta_1 \text{class size}+\text{other factors}$$

- For now, we will ignore these until Unit III

- Thus, $\beta_0 + \beta_1 \text{class size}$ gives the [average effect]{.hi-purple} of class sizes on scores

- Later, we will want to estimate the [marginal effect]{.hi} ([causal effect]{.hi}) of each factor on an individual district's test score, holding all other factors constant
:::
::: {.column width="50%"}
![](images/smallclass.jpg)
:::
:::

## Econometric Models: Overview I

$$Y = \beta_0 + \beta_1 X + u$$

. . .

- $Y$ is the [dependent variable]{.hi} of interest
    - AKA “response variable,” “regressand,” “Left-hand side (LHS) variable”

. . .

- $X_1$ is an [independent variable]{.hi}
    - AKA “explanatory variable”, “regressor,” “Right-hand side (RHS) variable”, “covariate”

. . . 

- Our data consists of a spreadsheet of observed values of $(X_{1i}, X_{2i}, Y_i)$

## Econometric Models: Overview II

$$Y = \beta_0 + \beta_1 X + u$$

- To model, we [“regress Y on $X_1$”]{.hi-turquoise}

. . .

- $\beta_0$ and $\beta_1$ are [parameters]{.hi-purple} that describe the population relationships between the variables
    - unknown! to be estimated

. . .

- $u$ is a random [error term]{.hi}
    - **‘U’nobservable**, we can't measure it, and must model with assumptions about it

## The Population Regression Model

::: columns
::: {.column width="50%"}
- How do we draw a line through the scatterplot? We do not know the **“true”** $\beta_0$ or $\beta_1$

- We do have data from a [sample]{.hi-turquoise} of class sizes and test scores

- So the real question is, [how can we estimate $\beta_0$ and $\beta_1$?]{.hi-purple}

:::
::: {.column width="50%"}
```{r}
scatter
```
:::
:::

# Deriving OLS Estimators {.centered background-color="#314f4f"}

## Actual, Predicted, and Residual Values

::: columns
::: {.column width="50%"}

- With a simple linear regression model, for each associated $X$ value, we have

1. The [**observed**]{.blue} (or [**actual**]{.blue}) [**values**]{.blue} of $\color{#0047AB}{Y_i}$

:::
::: {.column width="50%"}
```{r}
library(broom)
school_reg <- lm(testscr ~ str, data = ca_school)
aug_reg <- school_reg %>% augment()

scatter
```
:::
:::

## Actual, Predicted, and Residual Values

::: columns
::: {.column width="50%"}

- With a simple linear regression model, for each associated $X$ value, we have

1. The [**observed**]{.blue} (or [**actual**]{.blue}) [**values**]{.blue} of $\color{#0047AB}{Y_i}$
2. [**Predicted**]{.green} (or [**fitted**]{.green}) [**values**]{.green}, $\color{#047806}{\hat{Y}_i}$

:::
::: {.column width="50%"}
```{r}
scatter +
  geom_point(data = aug_reg, aes(x = str, y = .fitted), color = "#047806")
```
:::
:::

## Actual, Predicted, and Residual Values

::: columns
::: {.column width="50%"}

- With a simple linear regression model, for each associated $X$ value, we have

1. The [**observed**]{.blue} (or [**actual**]{.blue}) [**values**]{.blue} of $\color{#0047AB}{Y_i}$
2. [**Predicted**]{.green} (or [**fitted**]{.green}) [**values**]{.green}, $\color{#047806}{\hat{Y}_i}$
3. The [**residual**]{.red} (or [**error**]{.red}), $\color{#D7250E}{\hat{u}_i}=\color{#0047AB}{Y_i}-\color{#047806}{\hat{Y}_i}$ ... the difference between predicted and observed values

\begin{align*}
\color{#0047AB}{Y_i} &= \color{#047806}{\hat{Y}_i} + \color{#D7250E}{\hat{u}_i} \\
\color{#0047AB}{\text{Observed}_i} &= \color{#047806}{\text{Model}_i} + \color{#D7250E}{\text{Error}_i} \\
\end{align*}

:::
::: {.column width="50%"}
```{r}
scatter +
  geom_point(data = aug_reg, aes(x = str, y = .fitted), color = "#047806")+
  geom_segment(data = aug_reg, aes(x = str, xend = str, y = testscr, yend = .fitted), linetype = "dashed", color = "#D7250E")
```
:::
:::

## Deriving OLS Estimators

::: columns
::: {.column width="50%"}

- Take the residuals $\color{#D7250E}{\hat{u}_i}$ and square them (why)?
:::
::: {.column width="50%"}
```{r}
scatter +
  geom_point(data = aug_reg, aes(x = str, y = .fitted), color = "#047806")+
  geom_segment(data = aug_reg, aes(x = str, xend = str, y = testscr, yend = .fitted), linetype = "dashed", color = "#D7250E", alpha=0.05)+
  geom_rect(data = aug_reg, aes(xmin = str, xmax = str + .resid, ymin = testscr, ymax = .fitted),alpha=0.01,color="red", fill="red", linetype="dashed")
```
:::
:::

## Deriving OLS Estimators

::: columns
::: {.column width="50%"}

- Take the residuals $\color{#D7250E}{\hat{u}_i}$ and square them (why)?

- [The regression line *minimizes* the sum of the squared residuals (SSR)]{.hi-purple}

$$SSR = \sum^n_{i=1} \color{#D7250E}{\hat{u}_i}^2$$

:::
::: {.column width="50%"}
```{r}
scatter +
  geom_point(data = aug_reg, aes(x = str, y = .fitted), color = "#047806")+
  geom_segment(data = aug_reg, aes(x = str, xend = str, y = testscr, yend = .fitted), linetype = "dashed", color = "#D7250E", alpha=0.05)+
  geom_rect(data = aug_reg, aes(xmin = str, xmax = str + .resid, ymin = testscr, ymax = .fitted),alpha=0.01,color="red", fill="red", linetype="dashed")
```
:::
:::

## *O*-rdinary *L*-east *S*-quares Estimators

- The [Ordinary Least Squares (OLS) estimators]{.hi} of the unknown population parameters $\beta_0$ and $\beta_1$, solve the calculus problem:

$$\min_{\beta_0, \beta_1} \sum^n_{i=1}[\underbrace{Y_i-(\underbrace{\beta_0+\beta_1 X_i}_{\hat{Y_i}})}_{\hat{u_i}}]^2$$

. . .

- Intuitively, OLS estimators [minimize the sum of the squared residuals (distance between the actual values $Y_i$ and the predicted values $\hat{Y_i}$) along the estimated regression line]{.hi-purple}

## The OLS Regression Line

- The [OLS regression line]{.hi} or [sample regression line]{.hi} is the linear function constructed using the OLS estimators:

$$\hat{Y_i}=\hat{\beta_0}+\hat{\beta_1}X_i$$

. . . 

- $\hat{\beta_0}$ and $\hat{\beta_1}$ (“beta 0 hat” & “beta 1 hat”) are the [OLS estimators]{.hi} of population parameters $\beta_0$ and $\beta_1$ using sample data

. . . 

- The [predicted value]{.hi} of Y given X, based on the regression, is $E(Y_i|X_i)=\hat{Y_i}$ 

. . .

- The [residual]{.hi-purple} or [prediction error]{.hi-purple} for the $i^{th}$ observation is the difference between observed $Y_i$ and its predicted value, $\hat{u_i}=Y_i-\hat{Y_i}$

## The OLS Regression Estimators

- The solution to the SSE minimization problem yields:^[See [next class’ appendix page](/resources/appendices/2.4-appendix) for proofs.]

. . .

$$\hat{\beta}_0=\bar{Y}-\hat{\beta}_1\bar{X}$$

. . .

$$\hat{\beta}_1=\frac{\displaystyle\sum^n_{i=1}(X_i-\bar{X})(Y_i-\bar{Y})}{\displaystyle\sum^n_{i=1}(X_i-\bar{X})^2}=\frac{s_{XY}}{s^2_X}= \frac{cov(X,Y)}{var(X)}$$

. . .

## (Some) Properties of OLS

1. The regression line goes through the “center of mass” point $(\bar{X},\bar{Y})$
  - Again, $\hat{\beta}_0= \bar{Y}-\hat{\beta}_1 \bar{X}$

2. The slope, $\hat{\beta}_1$ has the same sign as the correlation coefficient $r_{X,Y}$, and is related
  $$\hat{\beta}_1=r\frac{s_Y}{s_X}$$

3. The residuals sum and average to zero
$$\begin{align*}
\sum^n_{i=1} \hat{u}_i &= 0\\
\mathbb{E}[\hat{u}] &= 0 \\
\end{align*}$$

4. The residuals and $X$ are uncorrelated

# Our Class Size Example in R {.centered background-color="#314f4f"}

## Class Size Scatterplot (Again)

::: columns
::: {.column width="50%"}
- There is some true (unknown) population relationship: 
$$\text{test score}_i=\beta_0+\beta_1 str_i$$

- $\beta_1=\frac{\Delta \text{test score}}{\Delta \text{str}}= ??$

:::
::: {.column width="50%"}
```{r}
scatter
```
:::
:::

## Class Size Scatterplot with Regression Line

```{r}
#| code-fold: true
#| fig-width: 14
#| fig-align: center
scatter+
  geom_smooth(method = "lm", color = "red")
```

## Linear Regression in R I

::: columns
::: {.column width="40%"}
```{r}
#| echo: true

# run regression of testscr on str
school_reg <- lm(testscr ~ str, 
                 data = ca_school)
```
:::
::: {.column width="60%"}
Format for regression is `lm(y ~ x, data = df)`

- `y` is dependent variable (listed first!)

- `~` means “is modeled by” or “is explained by”

- `x` is the independent variable

- `df` is name of dataframe where data is stored

This is `base R` (there's no good `tidyverse` way to do this yet...ish^[[`tidymodels`](https://www.tidymodels.org/) appears to be the new contender. It is used primarily for machine learning, but standardizes modeling, including OLS, in a tidy way. I think it's a bit unnecessary for us to use, for now.])

:::
:::

## Linear Regression in R II

::: columns
::: {.column width="50%"}
```{r}
#| echo: true

# look at reg object
school_reg 
```

:::
::: {.column width="50%"}
- Stored as an `lm` object called `school_reg`, a type of `list` object

:::
:::


## Linear Regression in R II

::: columns
::: {.column width="50%"}
```{r}
#| echo: true

# get full summary
school_reg %>% summary()
```

:::
::: {.column width="50%"}
- Looking at the `summary`, there's a lot of information here!

- These objects are cumbersome, come from a much older, pre-`tidyverse` era of `base R`

- Luckily, we now have some more `tidy` ways of working with regression *output*!

:::
:::

## Tidy Regression with `broom`

::: columns
::: {.column width="30%"}
![[broom.tidyverse.org](https://broom.tidyverse.org/)](images/rbroom.png)

:::
::: {.column width="70%"}
- The `broom` package allows us to work with regression objects as tidier `tibbles`

- Several useful commands:

| Command | Does |
|---------|------|
| `tidy()` | Create tibble of regression coefficients & stats |
| `glance()` | Create tibble of regression fit statistics |
| `augment()` | Create tibble of data with regression-based variables |

:::
:::

## Tidy Regression with `broom`: `tidy()`

- The `tidy()` function creates a *tidy* `tibble` of regression output

```{r}
#| echo: true
#| eval: true
#| output-location: fragment
# load packages
library(broom)

# tidy regression output
school_reg %>% 
  tidy()
```


## Tidy Regression with `broom`: `tidy()`

- The `tidy()` function creates a *tidy* `tibble` of regression output...**with confidence intervals**

```{.r code-line-numbers="6"}
# load packages
library(broom)

# tidy regression output
school_reg %>%
  tidy(conf.int = TRUE)
```

```{r}
#| output-location: fragment
school_reg %>% tidy(conf.int = T)
```

## Tidy Regression with `broom`: `glance()`

- `glance()` shows us a lot of overall regression statistics and diagnostics
    - We'll interpret these in next class and beyond

```{r}
#| echo: true
#| output-location: fragment

# look at regression statistics and diagnostics
school_reg %>% 
  glance()
```

## Tidy Regression with `broom`: `augment()`

- `augment()` creates a new tibble with the data $(X,Y)$ and regression-based variables, including:
    - `.fitted` are fitted (predicted) values from model, i.e. $\hat{Y}_i$
    - `.resid` are residuals (errors) from model, i.e. $\hat{u}_i$

```{r}
#| echo: true
#| output-location: fragment
# add regression-based values to data
school_reg %>% 
  augment()
```

## Class Size Regression Result

- Using OLS, we find:
$$\widehat{\text{test score}_i}=689.93-2.28 \, str_i$$ 
. . .

- $\hat{\beta_0} = 689.93$: test score for $str=0$

. . .

- $\hat{\beta_1} = -2.28$: for every 1 unit change in $str$, $\hat{\text{test_score}}$ changes by -2.28 points

. . .

$$\text{test score}_i = 689.93 - 2.28 \, str_i + \hat{u}_i$$

## Class Size Regression Residuals

`.resid = testscr - .fitted`

. . .

$$\hat{u}_i = \text{test score}_i - \widehat{\text{test score}}_i$$
. . .

$$\hat{u}_i = \text{test score}_i - (689.93-2.28 \, str_i)$$

## Class Size Regression: Fitted and Residual Values

```{r}
#| echo: true

aug_reg <- school_reg %>% 
  augment()

aug_reg %>% 
  dplyr::select(testscr, str, .fitted, .resid)
```

`testscr = .fitted + .resid`

## Class Size Regression: An Example Data Point I

- One district in our sample is Richmond Elementary

. . .

```{r}
ca_school %>%
  filter(district == "Richmond Elementary") %>%
  dplyr::select(observat, district, testscr, str, everything()) %>%
  rmarkdown::paged_table()
```

. . .

```{r}
#| echo: true
#| eval: false
aug_reg %>%
  slice(355) #

```

```{r}
aug_reg %>%
  slice(355) %>%
  rmarkdown::paged_table()
```

## Class Size Regression: An Example Data Point II

- `.fitted` value:
$$\widehat{\text{Test Score}}_{\text{Richmond}}=698-2.28(22) \approx 648$$

. . .

- `.resid` value: 
$$\hat{u}_{Richmond}=672-648 \approx 24$$

## Class Size Regression: An Example Data Point III

::: {.panel-tabset}
## Plot
```{r}
richmond <- ca_school %>%
  filter(district=="Richmond Elementary")

ggplot(data = ca_school,
       aes(x = str,
           y = testscr))+
  geom_point(color="blue")+
  geom_smooth(method="lm", color="red")+
  geom_point(data = richmond, color="magenta")+
  geom_text(data = richmond, color="magenta", label="Richmond", vjust=-1)+
  geom_segment(x=22,y=672.4, xend=22,yend=648,linetype=2, color="magenta")+ #connect Richmond to regression line to show residual
  labs(x = "Student to Teacher Ratio",
       y = "Test Score")+
  theme_pander(base_family = "Fira Sans Condensed",
           base_size = 20)
```

## Code

```{r}
richmond <- aug_reg %>%
  slice(355)

ggplot(data = ca_school,
       aes(x = str,
           y = testscr))+
  geom_point(color = "blue", alpha = 0.25)+
  geom_smooth(method = "lm",
              color = "red")+
  # Actual value
  geom_point(data = richmond,
             aes(x = str,
                 y = testscr),
             color = "magenta")+
  # Fitted value
  geom_point(data = richmond,
             aes(x = str,
                 y = .fitted),
             color = "green")+
  geom_text(data = richmond,
            aes(x = str,
                y = testscr),
            color = "magenta",
            label = "Richmond",
            vjust = -1)+
  geom_segment(data = richmond,
               aes(x = str,
               y = testscr,
               xend = str,
               yend = .fitted),
               linetype = 2,
               color = "magenta")+ # connect Richmond to regression line to show residual
  labs(x = "Student to Teacher Ratio",
       y = "Test Score")+
  theme_pander(base_family = "Fira Sans Condensed",
           base_size = 20)

```
:::

## Making Predictions

- We can use the regression model to make a prediction for a particular $x_i$

::: callout-tip
## Example

Suppose we have a school district with a student/teacher ratio of 18. What is the predicted average district test score?
:::

. . .

\begin{align*}
\widehat{\text{test score}_i} &= \hat{\beta_0}+\hat{\beta_1} \, \text{str}_i \\
&= 698.93 - 2.28 (18)\\
&= 657.89\\
\end{align*}

## Making Predictions In `R`

- We can do this in `R` with the `predict()`^[See more options [here](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/predict.lm.html)] function, which requires (at least) two inputs:
  1. An `lm` object (saved regression)
  2. `newdata` with $X$ value(s) to predict $\hat{Y}$ for, as a `data.frame` (or `tibble`)

. . .

```{r}
#| echo: true
#| output-location: fragment
some_district <- tibble(str = 18) # make a dataframe of "new data"'

some_district # look at it just to see
```

<br>

. . . 

```{r}
#| echo: true
#| output-location: fragment
predict(school_reg, # regression lm object
        newdata = some_district) # a dataframe of new data)
```

## Making Predictions In `R`, Manually I

- Of course we could do it ourselves...

```{r}
#| echo: true
# save tidied regression

tidy_reg <- tidy(school_reg)

```

. . .

<br>

```{r}
#| echo: true
#| output-location: fragment
# look at it, again
tidy_reg

```

## Making Predictions In `R`, Manually II

- Of course we could do it ourselves...

```{r}
#| echo: true
# extract and save beta_0
beta_0 <- tidy_reg %>%
  filter(term == "(Intercept)") %>%
  pull(estimate)
```

<br>

. . .

```{r}
#| echo: true
#| output-location: fragment
# check it 
beta_0
```

## Making Predictions In `R`, Manually II

- Of course we could do it ourselves...

```{r}
#| echo: true
#| output-location: fragment
# extract and save beta_1
beta_1 <- tidy_reg %>%
  filter(term == "str") %>%
  pull(estimate)
# check it
beta_1
```

. . .

<br>

```{r}
#| echo: true
#| output-location: fragment
# predict for str = 18
beta_0 + beta_1 * 18
```
