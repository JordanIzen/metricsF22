---
format:
  revealjs:
    theme: [default, custom.scss]
    logo: "../images/metrics_hex.png"
    footer: "[ECON 480 — Econometrics](https://metricsF22.classes.ryansafner.com)"
    height: 900
    width: 1600
    #df-print: paged
    slide-number: c
    chalkboard: true
overview: true
execute:
  echo: false
  warning: false
  freeze: auto
---

##  {data-menu-title="Title Slide" background-image="images/metrics_title_slide.png"}

[2.4 --- Goodness of Fit and Bias]{.custom-title}

[ECON 480 • Econometrics • Fall 2022]{.custom-subtitle}

[Dr. Ryan Safner <br> Associate Professor of Economics]{.custom-author}

[<a href="mailto:safner@hood.edu"><i class="fa fa-paper-plane fa-fw"></i>safner\@hood.edu</a> <br> <a href="https://github.com/ryansafner/metricsF22"><i class="fa fa-github fa-fw"></i>ryansafner/metricsF22</a><br> <a href="https://metricsF22.classes.ryansafner.com"> <i class="fa fa-globe fa-fw"></i>metricsF22.classes.ryansafner.com</a><br>]{.custom-institution}

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(broom)
library(haven)
library(kableExtra)
library(patchwork)
library(fontawesome)
library(gapminder)
library(ggthemes)
library(scales)
knitr::opts_chunk$set(echo=F,
                      message=F,
                      warning=F)
update_geom_defaults("label", list(family = "Fira Sans Condensed"))
update_geom_defaults("text", list(family = "Fira Sans Condensed"))

```

```{r}
ca_school <- read_dta("../files/data/caschool.dta")
scatter <- ggplot(data = ca_school)+
  aes(x = str,
      y = testscr)+
  geom_point(color = "blue")+
  labs(x = "Student to Teacher Ratio",
       y = "Test Score")+
  theme_pander(base_family = "Fira Sans Condensed",
           base_size = 20)
school_reg <- lm(testscr ~ str, data = ca_school)
aug_reg <- school_reg %>% augment()
school_reg_tidy <- tidy(school_reg)
```

## Contents {background-color="#314f4f"}

[Exploring Relationships](#exploring-relationships)

[Quantifying Relationships](#quantifying-relationships)

[Linear Regression](#linear-regression)

[Deriving OLS Estimators](#deriving-ols-estimators)

[Our Class Size Example in R](#our-class-size-example-in-r)



## Actual, Predicted, and Residual Values

::: columns
::: {.column width="50%"}

- With a simple linear regression model, for each associated $X$ value, we have

1. The [**observed**]{.blue} (or [**actual**]{.blue}) [**values**]{.blue} of $\color{#0047AB}{Y_i}$
2. [**Predicted**]{.green} (or [**fitted**]{.green}) [**values**]{.green}, $\color{#047806}{\hat{Y}_i}$
3. The [**residual**]{.red} (or [**error**]{.red}), $\color{#D7250E}{\hat{u}_i}=\color{#0047AB}{Y_i}-\color{#047806}{\hat{Y}_i}$ ... the difference between predicted and observed values

\begin{align*}
\color{#0047AB}{Y_i} &= \color{#047806}{\hat{Y}_i} + \color{#D7250E}{\hat{u}_i} \\
\color{#0047AB}{\text{Observed}_i} &= \color{#047806}{\text{Model}_i} + \color{#D7250E}{\text{Error}_i} \\
\end{align*}

> “All models are wrong. But some are useful. — George Box”

:::
::: {.column width="50%"}
```{r}
scatter +
  geom_point(data = aug_reg, aes(x = str, y = .fitted), color = "#047806")+
  geom_segment(data = aug_reg, aes(x = str, xend = str, y = testscr, yend = .fitted), linetype = "dashed", color = "#D7250E")
```
:::
:::

# Goodness of Fit {.centered background-color="#314f4f"}

## Goodness of Fit

- How well does a line fit data? How tightly clustered around the line are the data points?

- Quantify **how much variation in $\color{#0047AB}{Y_i}$ is "explained" by the model, $\color{#047806}{\hat{Y}_i}$**

. . .

$$\underbrace{\color{#0047AB}{Y_i}}_{\color{#0047AB}{Observed}}=\underbrace{\color{#047806}{\hat{Y}_i}}_{\color{#047806}{Model}}+\underbrace{\color{#D7250E}{\hat{u}_i}}_{\color{#D7250E}{Error}}$$

- Recall OLS estimators chosen to **minimize** [**Sum of Squared Residuals (SSR)**]{.red}: $\left(\displaystyle \sum^n_{i=1}\hat{u_i}^2\right)$

## $R^2$

- Primary measure^[Sometimes called the ["coefficient of determination"]{.hi}] is [R-squared]{.hi}, the fraction of variation in $\color{#0047AB}{Y_i}$ explained by variation in predicted values $\color{#047806}{\hat{Y}_i}$

$$R^2 = \frac{var(\color{#047806}{\hat{Y}_i})}{var(\color{#0047AB}{Y_i})}$$
## $R^2$ Formula I

$$R^2 = \frac{\color{#047806}{SSM}}{\color{#0047AB}{SST}}$$

. . .

- [**Model Sum of Squares (SSM)**]{.green}:^[Sometimes called Explained Sum of Squares (ESS) or Regression Sum of Squares (RSS) in other textbooks.] sum of squared deviations of *predicted* values from their mean^[It can be shown that $\color{#047806}{\bar{\hat{Y_i}}}=\color{#0047AB}{\bar{Y_i}}$]

$$\color{#047806}{SSM} = \sum^n_{i=1}(\hat{Y_i}-\bar{Y})^2$$

. . .

- [**Total Sum of Squares (TSS)**]{.blue}: sum of squared deviations of *observed* values from their mean

$$\color{#0047AB}{SST} = \sum^n_{i=1}(Y_i-\bar{Y})^2$$

## $R^2$ Formula II

- Equivalently, $R^2$ is the complement of the fraction of **unexplained** variation in $Y_i$ 

$$R^2=1-\left(\frac{\color{#D7250E}{SSR}}{\color{#0047AB}{SST}}\right)$$
. . .

- Equivalently, the square of the correlation coefficient between $X$ and $Y$^[The correlation coefficient between $X$ and $Y$ is the same as the correlation coefficient between $Y$ and $\hat{Y}$: $r_{X,Y}=r_{Y,\hat{Y}}$, so squaring either would also result in $R^2$.]: 

$$R^2=(r_{X,Y})^2$$
## Visualizing $R^2$ I

::: columns
::: {.column width="50%"}

- **Total Variation in Y**: Areas [**A**]{.red} + [C]{.hi-purple}

$$\color{#0047AB}{SST} = \sum^n_{i=1}(Y_i-\bar{Y})^2$$

- [Variation in Y explained by X: Area C]{.hi-purple}

$$\color{#047806}{SSM} = \sum^n_{i=1}(\hat{Y_i}-\bar{Y})^2$$

- [**Unexplained variation in Y: Area A**]{.red}
$$\color{#D7250E}{SSR} = \sum^n_{i=1}(\hat{u_i})^2$$

:::
::: {.column width="50%"}

$$R^2 = \frac{SSM}{SST} = \frac{\color{purple}{C}}{\color{red}{A}+\color{purple}{C}}$$

```{r}
library(ggforce)
venn_gen_colors <-c("blue", "red")
venn_gen_df <-tibble(
  x = c(0, 1.5),
  y = c(0, 0),
  r = c(1,1),
  l = c("Y", "X"),
  xl = c(0, 1.5),
  yl = c(0, 0),
)

ggplot(data = venn_gen_df)+
  aes(x0 = x,
      y0 = y,
      r = r,
      fill = l,
      color = l)+
  geom_circle(alpha = 0.3, size = 0.75)+
  geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans Book", parse = F)+
  theme_void()+
  theme(legend.position = "none")+
  scale_fill_manual(values = venn_gen_colors)+
  scale_color_manual(values = venn_gen_colors)+
  annotate(x = 0, y = 0.2, label = "A", geom = "text", size = 7, family = "Fira Sans Book")+
  annotate(x = 1.5, y = 0.2, label = "B", geom = "text", size = 7, family = "Fira Sans Book")+
  annotate(x = 0.75, y = 0.2, label = "C", geom = "text", size = 7, family = "Fira Sans Book")+
  coord_equal()
```

:::
:::

## Visualizing $R^2$ II

::: columns
::: {.column width="50%"}
```{r, echo = T}
# make a function to calc. sum of sq. devs
sum_sq <- function(x){sum((x - mean(x))^2)}

# find total sum of squares
SST <- school_reg %>%
  augment() %>%
  summarize(SST = sum_sq(testscr))

# find explained sum of squares
SSM <- school_reg %>%
  augment() %>%
  summarize(SST = sum_sq(.fitted))

# look at them and divide to get R^2
tribble(
  ~SSM, ~SST, ~R_sq,
  SSM, SST, SSM/SST
  ) %>%
  knitr::kable()
```

:::
::: {.column width="50%"}

$$R^2 = \frac{SSM}{SST} = \frac{\color{purple}{C}}{\color{red}{A}+\color{purple}{C}}=0.05$$

```{r}
library(ggforce)
venn_colors <-c("blue", "red")
venn_df <-tibble(
  x = c(0, 1.2),
  y = c(0, 0),
  r = c(1,0.3),
  l = c("Test Score", "STR"),
  xl = c(0, 1.2),
  yl = c(0, 0),
)

areas <- tribble(
  ~x, ~y, ~ll,
  0, 1, "A",
  1.15, 0.2, "C",
  1.2, 0.2, "B"
)

ggplot(data = venn_df)+
  aes(x0 = x,
      y0 = y,
      r = r,
      fill = l,
      color = l)+
  geom_circle(alpha = 0.3, size = 0.75)+
  geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans Book", parse = F)+
  theme_void()+
  theme(legend.position = "none")+
  scale_fill_manual(values = venn_colors)+
  scale_color_manual(values = venn_colors)+
  annotate(x = 0, y = 0.2, label = "A", geom = "text", size = 7, family = "Fira Sans Book")+
  annotate(x = 1.2, y = 0.2, label = "B", geom = "text", size = 7, family = "Fira Sans Book")+
  annotate(x = 0.95, y = 0.0, label = "C", geom = "text", size = 7, family = "Fira Sans Book")+
 # geom_label(data = areas,
  #          aes(x = x,
   #             y = y,
   #             label = ll)
 #           )+
 # annotate(
  #  x = -5.5, y = 3.3,
  #  geom = "text", label = "Multiple regression", size = 9, family = "Fira Sans Book",
  #  hjust = 0) +
  #xlim(-5.5, 4.5) +
  #ylim(-4.2, 3.4) +
  
  coord_equal()
```

:::
:::

## Calculating $R^2$ in `R` I

- Recall `broom`'s `augment()` command makes a lot of new regression-based values like:
    - `.fitted`: predicted values $(\hat{Y_i})$
    - `.resid`: residuals $(\hat{u_i})$
    
```{r, echo=T}
library(broom)
school_reg %>%
  augment()
```

## Calculating $R^2$ in `R` I

- Or, simpler, can calculate $R^2$ in `R` as the ratio of variances in model vs. actual 

```{r, echo = T}
# as ratio of variances
school_reg %>%
  augment() %>%
  summarize(r_sq = var(.fitted)/var(testscr)) # var. of *predicted* testscr over var. of *actual* testscr
```


$$R^2 = \frac{var(\hat{Y})}{var(Y)} = \frac{\color{red}{\frac{1}{n-1}}\sum^n_{i=1}(\hat{Y_i}-\bar{Y})^2}{\color{red}{\frac{1}{n-1}}\sum^n_{i=1}(Y_i-\bar{Y})^2} \rightarrow \frac{SSM}{SST}$$

- SSM and SST are simply the numerators of the variance of $\hat{Y}$ and $Y$, respectively (i.e. before dividing by $n-1$, which will cancel out).


## Standard Error of the Regression

- [Standard Error of the Regression]{.hi}^[In machine learning and other contexts, focus on [Root Mean Square Error (RMSE)]{.hi}:$$RMSE = \sqrt{\frac{SSR}{n}} = \sqrt{\frac{\sum \hat{u}_i^2}{n}}$$ i.e. the positive root of the average squared residuals...so, average residual.], $\color{#e64173}{\hat{\sigma}_u}$ is an estimator of the standard deviation of $u_i$

. . . 

$$\hat{\sigma_u}=\sqrt{\frac{SSR}{n-2}}=\sqrt{\frac{\sum \hat{u}_i^2}{n-2}}$$

. . .

- Measures $\approx$ [average residual]{.hi-purple} (distance between data points & regression line)
  - [Degrees of Freedom correction]{.hi-turquoise} of $n-2$: we use up 2 df to first calculate $\hat{\beta_0}$ and $\hat{\beta_1}$!

## Calculating SER in `R` 

```{r}
#| echo: true
#| output-location: fragment
school_reg %>%
  augment() %>%
  summarize(SSR = sum(.resid^2),
            df = n()-2,
            SER = sqrt(SSR/df))
```


. . .

In large samples (where $n-2 \approx n)$, SER $\rightarrow$ standard deviation of the residuals

```{r}
#| echo: true
#| output-location: fragment
school_reg %>%
  augment() %>%
  summarize(sd_resid = sd(.resid))
```

## Goodness of Fit: Looking at R I


::: columns
::: {.column width="50%"}
- `summary()` command in `Base R` gives:
  - `Multiple R-squared`
  - `Residual standard error` (SER)
    - Calculated with a df of $n-2$
:::
::: {.column width="50%"}
```{r}
#| echo: true
school_reg %>% summary()
```
:::
:::

## Goodness of Fit: Looking at R I


::: columns
::: {.column width="50%"}
- `r.squared` is `0.05` $\implies$ about 5% of variation in `testscr` is explained by our model
- `sigma` (SER) is `18.6` $\implies$ average test score is about 18.6 points above/below our model's prediction
:::
::: {.column width="50%"}
```{r}
#| echo: true
# using broom
library(broom)
school_reg %>% glance()
```

<br>

```{r}
#| echo: true
#| 
# extract it if you want with pull
school_r_sq <- glance(school_reg) %>% pull(r.squared)
school_r_sq
```
:::
:::

# The Sampling Distributions of the OLS Estimators {background-color="#314f4f"} 

## Recall: Two Big Problems with Data

::: columns
::: {.column width="50%"}
- We want to use econometrics to [identify]{.hi} causal relationships and make [inferences]{.hi} about them

1. Problem for [identification]{.hi}: [endogeneity]{.hi-purple}
    - $X$ is **exogenous** if its variation is *unrelated* to other factors $(u)$ that affect $Y$
    - $X$ is **endogenous** if its variation is *related* to other factors $(u)$ that affect $Y$
    
2. Problem for [inference]{.hi}: [randomness]{.hi-purple}
    - Data is random due to **natural sampling variation**
    - Taking one sample of a population will yield slightly different information than another sample of the same population

:::

::: {.column width="50%"}

![](images/causality.jpg){width="750" fig-align="center"}

![](images/randomimage.jpg){width="750" fig-align="center"}
:::
:::

## Distributions of the OLS Estimators

- OLS estimators $(\hat{\beta_0}$ and $\hat{\beta_1})$ are computed from a finite (specific) sample of data

- Our OLS model contains **2 sources of randomness**:

. . .

- [*Modeled* randomness]{.hi-purple}: population $u$ includes all factors affecting $Y$ *other* than $X$
    - different samples will have different values of those other factors $(u_i)$

. . .

- [*Sampling* randomness]{.hi-purple}: different samples will generate different OLS estimators
    - Thus, $\hat{\beta_0}, \hat{\beta_1}$ are *also* **random variables**, with their own [sampling distribution]{.hi}

## Inferential Statistics and Sampling Distributions

::: columns
::: {.column width="50%"}
- [Inferential statistics]{.hi} analyzes a **sample** to make inferences about a much larger (unobservable) **population**

- [Population]{.hi-purple}: all possible individuals that match some well-defined criterion of interest
  - Characteristics about (relationships between variables describing) populations are called [“parameters”]{.hi-turquoise} 

- [Sample]{.hi-purple}: some portion of the population of interest to *represent the whole*
  - Samples examine part of a population to generate .hi-turquoise[statistics] used to [estimate]{.hi} population [parameters]{.hi-turquoise}

:::
::: {.column width="50%"}
![](images/citymodel.jpg)
:::
:::

## Sampling Basics

::: {.callout-tip}
## Example
Suppose you randomly select 100 people and ask how many hours they spend on the internet each day. You take the mean of your sample, and it comes out to 5.4 hours. 
:::

. . .

- 5.4 hours is a [sample statistic]{.hi-purple} describing the sample; we are more interested in the corresponding [parameter]{.hi-turquoise} of the relevant population (e.g. all Americans)

. . .

- If we take another sample of 100 people, would we get the same number?

. . .

- *Roughly*, but probably not exactly

- [Sampling variability]{.hi} describes the effect of a statistic varying somewhat from sample to sample
  - This is *normal*, not the result of any error or bias!

## i.i.d. Samples

::: columns
::: {.column width="50%"}
- If we collect many samples, and each sample is randomly drawn from the population (and then replaced), then the distribution of samples is said to be [independently and identically distributed (i.i.d.)]{.hi-purple}

- Each sample is **independent** of each other sample (due to replacement)

- Each sample comes from the **identical** underlying population distribution

:::
::: {.column width="50%"}
![](images/sampling.jpg)
:::
:::

## The Sampling Distribution of OLS Estimators

::: columns
::: {.column width="50%"}
- Calculating OLS estimators for a sample makes the OLS estimators *themselves* random variables:

- Draw of $i$ is random $\implies$ value of each $(X_i,Y_i)$ is random $\implies$ $\hat{\beta_0},\hat{\beta_1}$ are random

- Taking different samples will create different values of $\hat{\beta_0},\hat{\beta_1}$

- Therefore, $\hat{\beta_0},\hat{\beta_1}$ each have a [sampling distribution]{.hi} across different samples 

:::
::: {.column width="50%"}
![](images/randomimage.jpg)
:::
:::

## The Central Limit Theorem

::: {.callout-note}
## The Central Limit Theorem
If we collect samples of size $n$ from the same population and generate a sample statistic (e.g. OLS estimator), then with large enough $n$, the distribution of the sample statistic is approximately normal if:

1. $n \geq 30$
2. Samples come from a *known* normal distribution $\sim N(\mu,\sigma)$
:::

. . .

- If neither of these are true, we have other methods (coming shortly!) 

- One of the most fundamental principles in all of statistics

- Allows for virtually all testing of statistical hypotheses $\rightarrow$ estimating probabilities of values on a normal distribution

## The Sampling Distribution of $\hat{\beta_1}$ I

::: columns
::: {.column width="50%"}
- The CLT allows us to approximate the sampling distributions of $\hat{\beta_0}$ and $\hat{\beta_1}$ as normal

- We care about $\hat{\beta_1}$ (slope) since it has economic meaning, rarely about $\hat{\beta_0}$ (intercept)

$$\hat{\beta_1} \sim N(E[\hat{\beta_1}], \sigma_{\hat{\beta_1}})$$

:::
::: {.column width="50%"}
```{r, fig.retina=3}
beta_dist <- ggplot(data = tibble(x=-4:4))+
  aes(x = x)+
  stat_function(fun = dnorm, geom="area", fill = "gray", alpha = 0.5)+
  geom_segment(aes(x=0,xend=0, y=0, yend=0.4), linetype="dashed")+
  scale_x_continuous(breaks = 0,
                     labels = expression(E(hat(beta[1]))))+
  scale_y_continuous(limits = c(0,0.4),
                     expand = c(0,0))+
  labs(x = expression(hat(beta[1])),
       y = "Probability")+
  theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)
beta_dist
```

:::
:::

## The Sampling Distribution of $\hat{\beta_1}$ II

::: columns
::: {.column width="50%"}
$$\hat{\beta_1} \sim N(E[\hat{\beta_1}], \sigma_{\hat{\beta_1}})$$

- We want to know: 

1. $\mathbb{E}[\hat{\beta_1}]$; what is the [center]{.hi-purple} of the distribution? (today)

2. $\sigma_{\hat{\beta_1}}$; how [precise]{.hi-purple} is our estimate? (next class)

:::
::: {.column width="50%"}
```{r, fig.retina=3}
beta_dist
```

:::
:::

# Bias and Exogeneity {background-color="#314f4f"} 

## Assumptions about Errors I
::: columns
::: {.column width="50%"}
- In order to talk about $E[\hat{\beta_1}]$, we need to talk about population $u$

- Recall: $u$ is a random variable, and we can never measure the error term

:::
::: {.column width="50%"}
![](images/error.png)
:::
:::

## Assumptions about Errors II {.smaller}
::: columns
::: {.column width="50%"}
- We make [4 critical **assumptions** about $u$]{.hi}:

1. The expected value of the errors is 0
$$E[u]=0$$

:::
::: {.column width="50%"}
![](images/error.png)
:::
:::

## Assumptions about Errors II  {.smaller}
::: columns
::: {.column width="50%"}
- We make [4 critical **assumptions** about $u$]{.hi}:

1. The expected value of the errors is 0
$$E[u]=0$$

2. The variance of the errors over $X$ is constant:
$$var(u|X)=\sigma^2_{u}$$

:::
::: {.column width="50%"}
![](images/error.png)
:::
:::

## Assumptions about Errors II  {.smaller}
::: columns
::: {.column width="50%"}
- We make [4 critical **assumptions** about $u$]{.hi}:

1. The expected value of the errors is 0
$$E[u]=0$$

2. The variance of the errors over $X$ is constant:
$$var(u|X)=\sigma^2_{u}$$

3. Errors are not correlated across observations: 
$$cor(u_i,u_j)=0 \quad \forall i \neq j$$

:::
::: {.column width="50%"}
![](images/error.png)
:::
:::

## Assumptions about Errors II  {.smaller}
::: columns
::: {.column width="50%"}
- We make [4 critical **assumptions** about $u$]{.hi}:

1. The expected value of the errors is 0
$$E[u]=0$$

2. The variance of the errors over $X$ is constant:
$$var(u|X)=\sigma^2_{u}$$

3. Errors are not correlated across observations: 
$$cor(u_i,u_j)=0 \quad \forall i \neq j$$

4. There is no correlation between $X$ and the error term: 
$$cor(X, u)=0 \text{ or } E[u|X]=0$$

:::
::: {.column width="50%"}
![](images/error.png)
:::
:::

## Assumptions about Errors III
::: columns
::: {.column width="50%"}
1. The expected value of the errors is 0
$$E[u]=0$$

2. The variance of the errors over $X$ is constant:
$$var(u|X)=\sigma^2_{u}$$

- The first two assumptions $\implies$ errors are **i.i.d.**, drawn from the same distribution with mean 0 and variance $\sigma^2_{u}$

:::
::: {.column width="50%"}
```{r}
ggplot(data = tibble(x=-4:4))+
  aes(x = x)+
  stat_function(fun = dnorm, geom="area", fill = "gray", alpha = 0.5)+
  geom_segment(aes(x=0,xend=0, y=0, yend=0.4), linetype="dashed")+
  scale_x_continuous(breaks = c(0),
                     labels = c(0))+
  scale_y_continuous(limits = c(0,0.4),
                     expand = c(0,0))+
  labs(x = expression("u"),
       y = "Probability")+
  theme_classic(base_family = "Fira Sans Condensed",
           base_size=20)
```
:::
:::

## Assumptions about Errors: Homoskedasticity
::: columns
::: {.column width="50%"}

2. The variance of the errors over $X$ is constant:
$$var(u|X)=\sigma^2_{u}$$

- Assumption 2 implies that errors are [“homoskedastic”]{.hi}:
 they have the same variance across $X$

- Often this assumption is violated: errors may be [“heteroskedastic”]{.hi-purple}:
 they do not have the same variance across $X$

- This *is* a problem for **inference**, but we have a simple fix for this (next class)

:::
::: {.column width="50%"}
```{r}
library(ggridges)
df <- tibble(
  x = runif(1000,0,10),
  y = 1+2*x + rnorm(1000,0,1)
  ) %>%
  filter(y > 0,
         x > 0)
# Means

cef_df <- df %>% 
  filter(y > 0,
         x > 0) %>%
  mutate(x = as.factor(round(x))) %>%
  filter(x != 0)

#means_df <- cef_df %>% group_by(x) %>% summarize(y = mean(y))
# The CEF in ggplot
gg_cef <- ggplot(data = df)+
  aes(x = y,
      y = x) +
  geom_density_ridges(
    data = cef_df,
    aes(x = y, y = x),
    #rel_min_height = 0.003,
  #  color = "white",
  #  scale = 2.5,
  #  size = 0.3
  ) +
  labs(x = "X",
       y = "Y")+
  geom_smooth(method = "lm", color = "red")+
  geom_point(alpha = 0.4)+
  #scale_fill_viridis(option = "magma") +
  theme_pander(base_family = "Fira Sans Condensed", base_size = 18) +
  theme(
    legend.position = "none"
  )+ coord_flip()
gg_cef
```
:::
:::

## Assumption 3: No Serial Correlation

::: columns
::: {.column width="50%"}
- Errors are not correlated across observations: 
$$cor(u_i,u_j)=0 \quad \forall i \neq j$$

- For simple cross-sectional data, this is rarely an issue

- Time-series & panel data nearly always contain [serial correlation]{.hi} or [autocorrelation]{.hi} between errors

- e.g. "this week's sales look a lot like last week's sales, which look like...etc"

- There are fixes to deal with autocorrelation (coming much later)

:::
::: {.column width="50%"}
:::
:::

## Assumption 4: The Zero Conditional Mean Assumption

::: columns
::: {.column width="50%"}
- No correlation between $X$ and the error term: 
$$cor(X, u)=0$$

- [This is the absolute killer assumption, because it assumes **exogeneity**]{.hi-purple}

- Often called the [Zero Conditional Mean]{.hi-purple} assumption: 
$$E[u|X]=0$$

> "Does knowing $X$ give me any useful information about $u$?"

  - If yes: model is **endogenous**, **biased** and **not-causal**! 

:::
::: {.column width="50%"}
![](images/error.png)
:::
:::

## Exogeneity and Unbiasedness

- $\hat{\beta_1}$ is [unbiased]{.hi-purple} iff there is no systematic difference, on average, between sample values of $\hat{\beta_1}$ and [true population parameter]{.hi-turquoise} $\beta_1$, i.e.

$$E[\hat{\beta_1}]=\beta_1$$

. . .

- Does *not* mean any sample gives us $\hat{\beta_1}=\beta_1$, only the [estimation procedure]{.hi-purple} will, *on average*, yield the correct value

- Random errors above and below the true value cancel out (so that on average, $E[\hat{u}|X]=0)$

## Sidenote: Statistical Estimators I

- In statistics, an [estimator]{.hi} is a rule for calculating a statistic (about a population parameter)

. . .

::: {.callout-tip}
## Example
We want to estimate the average height (H) of U.S. adults (population) and have a random sample of 100 adults.
:::

. . .

- Calculate the mean height of our sample $(\bar{H})$ to estimate the true mean height of the population $(\mu_H)$

- $\bar{H}$ is an **estimator** of $\mu_H$

. . .

- There are many estimators we *could* use to estimate $\mu_H$
  - How about using the first value in our sample: $H_1$ ?

## Sidenote: Statistical Estimators II

::: columns
::: {.column width="50%"}
- What makes one estimator (e.g. $\bar{H}$) better than another (e.g. $H_1$)?^[Technically, we also care about [consistency]{.hi-purple}: minimizing uncertainty about the correct value. The Law of Large Numbers, similar to CLT, permits this. We don't need to get too advanced about probability in this class.]

1. [Unbiasedness]{.hi-purple}: does the estimator give us the true parameter *on average*?  

2. [Efficiency]{.hi-purple}: an estimator with a smaller variance is better

:::
::: {.column width="50%"}
![](images/biasvariability.png)

:::
:::

## Exogeneity and Unbiasedness II

::: columns
::: {.column width="50%"}
- $\mathbf{\hat{\beta_1}}$ is the [Best Linear Unbiased Estimator (BLUE)]{.hi} estimator of  $\mathbf{\beta_1}$ **when $X$ is exogenous**^[The proof for this is known as the famous [Gauss-Markov Theorem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem). See today's [appendix](/resources/appendices/2.4-appendix) for a simplified proof.]

- No systematic difference, on average, between sample values of $\hat{\beta_1}$ and the true population $\beta_1$:

$$E[\hat{\beta_1}]=\beta_1$$

- Does *not* mean that each sample gives us $\hat{\beta_1}=\beta_1$, only the estimation **procedure** will, **on average**, yield the correct value

:::
::: {.column width="50%"}
![](images/biasvariability.png)

:::
:::

## Exogeneity and Unbiasedness III

- Recall, an [exogenous]{.hi} variable $(X)$ is unrelated to other factors affecting $Y$, i.e.:

$$cor(X,u)=0$$

. . .

- Again, this is called the **Zero Conditional Mean Assumption**

$$E(u|X)=0$$

. . .

- For any known value of $X$, the expected value of $u$ is 0

- Knowing the value of $X$ must tell us *nothing* about the value of $u$ (anything else relevant to $Y$ other than $X$)

- We can then confidently assert causation: $X \rightarrow Y$

## Endogeneity and Bias

- Nearly all independent variables are [endogenous]{.hi}, they **are** related to the error term $u$
$$cor(X,u)\neq 0$$

. . .

::: {.callout-tip}
## Example
Suppose we estimate the following relationship:

$$\text{Violent crimes}_t=\beta_0+\beta_1\text{Ice cream sales}_t+u_t$$
:::

- We find $\hat{\beta_1}>0$

- Does this mean Ice cream sales $\rightarrow$ Violent crimes?

## Endogeneity and Bias: Takeaways

- The true expected value of $\hat{\beta_1}$ is actually:^[See today's [appendix](/resources/appendices/2.4-appendix) for proof.]

$$E[\hat{\beta_1}]=\beta_1+cor(X,u)\frac{\sigma_u}{\sigma_X}$$

. . .

1. If $X$ is exogenous: $cor(X,u)=0$, we're just left with $\beta_1$

. . .

2. The larger $cor(X,u)$ is, larger [bias]{.hi-purple}: $\left(E[\hat{\beta_1}]-\beta_1 \right)$

. . .

3. We can [“sign”]{.hi-turquoise} the direction of the bias based on $cor(X,u)$
  - [Positive]{.hi-turquoise} $cor(X,u)$ overestimates the true $\beta_1$ $(\hat{\beta_1}$ is too high)
  - [Negative]{.hi-turquoise} $cor(X,u)$ underestimates the true $\beta_1$ $(\hat{\beta_1}$ is too low)

## Endogeneity and Bias: Example I

::: {.callout-tip}
## Example
$$wages_i=\beta_0+\beta_1 education_i+u$$
:::

. . .

- Is this an accurate reflection of $education \rightarrow wages$?

- Does $E[u|education]=0$?

- What would $E[u|education]>0$ mean? 

## Endogeneity and Bias: Example II

::: {.callout-tip}
## Example
$$\text{per capita cigarette consumption}=\beta_0+\beta_1 \text{State cig tax rate}+u$$
:::

. . .

- Is this an accurate reflection of $taxes \rightarrow consumption$?

- Does $E[u|tax]=0$?

- What would $E[u|tax]>0$ mean? 
