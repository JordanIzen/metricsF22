<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academia 4.3.1">
  <meta name="generator" content="Hugo 0.79.1" />

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Ryan Safner">

  
  
  
    
  
  <meta name="description" content="Overview  Readings  Slides  Assignments  Problem Set 2  Math Appendix  Deriving the OLS Estimators Finding \(\hat{\beta_0}\) Finding \(\hat{\beta_1}\) Algebraic Properties of OLS Estimators Bias in \(\hat{\beta_1}\) Proof of the Unbiasedness of \(\hat{\beta_1}\)     Tuesday, September 21, 2021    Problem Set 2 is due by the end of the day today.    Overview Today we continue looking at basic OLS regression.">

  
  <link rel="alternate" hreflang="en-us" href="https://metricsf22.classes.ryansafner.com/content/2.4-content/">

  


  

  
  
  
  <meta name="theme-color" content="#314f4f">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Sans+Condensed:400,400i,700,700i|Fira+Mono:400,700)&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academia.min.0c527bc9fd0ba6033034eb84176a3cc2.css">

  

  
  
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://metricsf22.classes.ryansafner.com/content/2.4-content/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="ECON 480 — Econometrics">
  <meta property="og:url" content="https://metricsf22.classes.ryansafner.com/content/2.4-content/">
  <meta property="og:title" content="2.4 — OLS: Goodness of Fit and Bias — Class Content | ECON 480 — Econometrics">
  <meta property="og:description" content="Overview  Readings  Slides  Assignments  Problem Set 2  Math Appendix  Deriving the OLS Estimators Finding \(\hat{\beta_0}\) Finding \(\hat{\beta_1}\) Algebraic Properties of OLS Estimators Bias in \(\hat{\beta_1}\) Proof of the Unbiasedness of \(\hat{\beta_1}\)     Tuesday, September 21, 2021    Problem Set 2 is due by the end of the day today.    Overview Today we continue looking at basic OLS regression."><meta property="og:image" content="https://metricsf22.classes.ryansafner.com/img/icon-192.png">
  <meta property="twitter:image" content="https://metricsf22.classes.ryansafner.com/img/icon-192.png"><meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2020-06-08T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2022-07-12T18:11:35-04:00">
  

  


  





  <title>2.4 — OLS: Goodness of Fit and Bias — Class Content | ECON 480 — Econometrics</title>

</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">ECON 480 — Econometrics</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation"><span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">
      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/syllabus/"><span>Syllabus</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/schedule/"><span>Schedule</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/content/"><span>Content</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/assignments/"><span>Assignments</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/resources/"><span>Resources</span></a>
        </li>

        
        

        

        
        
        
          
            
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="https://rstudio.cloud/" target="_blank" rel="noopener"><span><i class="fas fa-cloud"></i></span></a>
        </li>

        
        

        

        
        
        
          
            
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="https://hoodcollegeeconomics.slack.com" target="_blank" rel="noopener"><span><i class="fab fa-slack"></i></span></a>
        </li>

        
        

      

        

        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>
    </div>
  </div>
</nav>


  

<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      





<form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-3" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <span><i class="fas fa-bars"></i></span>
  </button>

  
</form>

<nav class="collapse docs-links" id="docs-nav">
  
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/content/">Course content</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/content/">Overview</a>
      </li>
      
      <li >
        <a href="/content/1.1-content/">1.1 — Introduction</a>
      </li>
      
      <li >
        <a href="/content/1.2-content/">1.2 — Meet R</a>
      </li>
      
      <li >
        <a href="/content/1.3-content/">1.3 — Data Visualization with ggplot2</a>
      </li>
      
      <li >
        <a href="/content/1.4-content/">1.4 — Data Wrangling in the tidyverse</a>
      </li>
      
      <li >
        <a href="/content/1.5-content/">1.5 — Optimize Your Workflow</a>
      </li>
      
      <li >
        <a href="/content/2.1-content/">2.1 — Data 101 &amp; Descriptive Statistics</a>
      </li>
      
      <li >
        <a href="/content/2.2-content/">2.2 — Random Variables &amp; Distributions</a>
      </li>
      
      <li >
        <a href="/content/2.3-content/">2.3 — OLS Linear Regression</a>
      </li>
      
      <li class="active">
        <a href="/content/2.4-content/">2.4 — OLS: Goodness of Fit &amp; Bias</a>
      </li>
      
      <li >
        <a href="/content/2.5-content/">2.5 — OLS: Precision &amp; Diagnostics</a>
      </li>
      
      <li >
        <a href="/content/2.6-content/">2.6 — Statistical Inference</a>
      </li>
      
      <li >
        <a href="/content/2.7-content/">2.7 — Inference for Regression</a>
      </li>
      
      <li >
        <a href="/content/3.1-content/">3.1 — The Fundamental Problem of Causal Inference &amp; Potential Outcomes</a>
      </li>
      
      <li >
        <a href="/content/3.2-content/">3.2 — Causal Inference &amp; DAGs</a>
      </li>
      
      <li >
        <a href="/content/3.3-content/">3.3 — Omitted Variable Bias</a>
      </li>
      
      <li >
        <a href="/content/3.4-content/">3.4 — Multivariate OLS Estimators: Bias, Precision, &amp; Fit</a>
      </li>
      
      <li >
        <a href="/content/3.5-content/">3.5 — Reading and Writing Empirical Papers</a>
      </li>
      
      <li >
        <a href="/content/3.6-content/">3.6 — Regression with Categorical Data</a>
      </li>
      
      <li >
        <a href="/content/3.7-content/">3.7 — Regression with Interaction Effects</a>
      </li>
      
      <li >
        <a href="/content/3.8-content/">3.8 — Polynomial Regression</a>
      </li>
      
      <li >
        <a href="/content/3.9-content/">3.9 — Logarithmic Regression</a>
      </li>
      
      <li >
        <a href="/content/4.1-content/">4.1 — Panel Data and Fixed Effects</a>
      </li>
      
      <li >
        <a href="/content/4.2-content/">4.2 — Difference-in-Difference Models</a>
      </li>
      
    </ul>
    

  </div>
  
  
</nav>

    </div>

    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article" itemscope itemtype="http://schema.org/Article">

        <div class="docs-article-container">
          <h1 itemprop="name">2.4 — OLS: Goodness of Fit and Bias — Class Content</h1>

          <div class="article-style" itemprop="articleBody">
            
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#overview"><i class="fas fa-info-circle fa-lg"></i> Overview</a></li>
<li><a href="#readings"><i class="fas fa-book-reader fa-lg"></i> Readings</a></li>
<li><a href="#slides"><i class="fas fa-chalkboard-teacher"></i> Slides</a></li>
<li><a href="#assignments"><i class="fas fa-laptop-code"></i> Assignments</a>
<ul>
<li><a href="#problem-set-2">Problem Set 2</a></li>
</ul></li>
<li><a href="#math-appendix">Math Appendix</a>
<ul>
<li><a href="#deriving-the-ols-estimators">Deriving the OLS Estimators</a></li>
<li><a href="#finding-hatbeta_0">Finding <span class="math inline">\(\hat{\beta_0}\)</span></a></li>
<li><a href="#finding-hatbeta_1">Finding <span class="math inline">\(\hat{\beta_1}\)</span></a></li>
<li><a href="#algebraic-properties-of-ols-estimators">Algebraic Properties of OLS Estimators</a></li>
<li><a href="#bias-in-hatbeta_1">Bias in <span class="math inline">\(\hat{\beta_1}\)</span></a></li>
<li><a href="#proof-of-the-unbiasedness-of-hatbeta_1">Proof of the Unbiasedness of <span class="math inline">\(\hat{\beta_1}\)</span></a></li>
</ul></li>
</ul>
</div>

<p><div class="alert alert-note">
  <div>
    <em>Tuesday, September 21, 2021</em>
  </div>
</div>
</p>
<p><div class="alert alert-warning">
  <div>
    <a href="/assignments/02-problem-set">Problem Set 2</a> is due by the end of the day today.
  </div>
</div>
</p>
<div id="overview" class="section level2">
<h2><i class="fas fa-info-circle fa-lg"></i> Overview</h2>
<p>Today we continue looking at basic OLS regression. We will cover how to measure if a regression line is a good fit (using <span class="math inline">\(R^2\)</span> and <span class="math inline">\(\sigma_u\)</span> or SER), and whether OLS estimators are <em>biased</em>. These will depend on four critical <em>assumptions about <span class="math inline">\(u\)</span>.</em></p>
<p>In doing so, we begin an ongoing exploration into inferential statistics, which will finally become clear in another week. The most confusing part is recognizing that there is a <em>sampling distribution of each OLS estimator</em>. We want to measure the center of that sampling distribution, to see if the estimator is <em>biased</em>. Next class we will measure the spread of that distribution.</p>
<p>We continue the extended example about class sizes and test scores, which comes from a (Stata) dataset from an old textbook that I used to use, <a href="https://www.pearson.com/us/higher-education/product/Stock-Introduction-to-Econometrics-Update-3rd-Edition/9780133486872.html?tab=resources">Stock and Watson, 2007</a>. Download and follow along with the data from today’s example:<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<ul>
<li><a href="http://metricsf21.classes.ryansafner.com/data/caschool.dta"><i class="fas fa-table"></i> <code>caschool.dta</code></a></li>
</ul>
<p>I have also made a RStudio Cloud project documenting all of the things we have been doing with this data that may help you when you start working with regressions (next class):</p>
<ul>
<li><a href="https://rstudio.cloud/spaces/163934/project/2940189"><i class="fas fa-cloud"></i> Class Size Regression Analysis</a></li>
</ul>
</div>
<div id="readings" class="section level2">
<h2><i class="fas fa-book-reader fa-lg"></i> Readings</h2>
<ul>
<li><i class="fas fa-book"></i> Ch. 3.2-3.4, 3.7-3.8 in Bailey, <em>Real Econometrics</em></li>
</ul>
</div>
<div id="slides" class="section level2">
<h2><i class="fas fa-chalkboard-teacher"></i> Slides</h2>
<p>Below, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. Note while in going through the slides, you can type <kbd>h</kbd> to see a special list of viewing options, and type <kbd>o</kbd> for an outline view of all the slides.</p>
<p>The lower button will allow you to download a PDF version of the slides. I suggest printing the slides beforehand and using them to take additional notes in class (<em>not everything</em> is in the slides)!</p>
<p>
<p style="text-align:center;"><a target="_blank" href="/slides/2.4-slides.html"><img src ="/slides/2.4-slides.png" alt = "2.4-slides"></a></p>
<p style="text-align:center;"><a class="btn btn-primary btn-lg" target="_blank" href="/slides/2.4-slides.pdf"><i class="fas fa-file-pdf"></i> Download as PDF</a></p>
</p>
</div>
<div id="assignments" class="section level2">
<h2><i class="fas fa-laptop-code"></i> Assignments</h2>
<div id="problem-set-2" class="section level3">
<h3>Problem Set 2</h3>
<p><a href="/assignments/02-problem-set">Problem Set 2</a> is due by Tuesday September 21. Please see the <a href="/assignments/problem-sets">instructions</a> for more information on how to submit your assignment (there are multiple ways!).</p>
</div>
</div>
<div id="math-appendix" class="section level2">
<h2>Math Appendix</h2>
<div id="deriving-the-ols-estimators" class="section level3">
<h3>Deriving the OLS Estimators</h3>
<p>The population linear regression model is:</p>
<p><span class="math display">\[Y_i=\beta_0+\beta_1 X_i + u _i\]</span></p>
<p>The errors <span class="math inline">\((u_i)\)</span> are unobserved, but for candidate values of <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span>, we can obtain an estimate of the residual. Algebraically, the error is:</p>
<p><span class="math display">\[\hat{u_i}=    Y_i-\hat{\beta_0}-\hat{\beta_1}X_i\]</span></p>
<p>Recall our goal is to find <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> that <em>minimizes</em> the sum of squared errors (SSE):</p>
<p><span class="math display">\[SSE= \sum^n_{i=1} \hat{u_i}^2\]</span></p>
<p>So our minimization problem is:</p>
<p><span class="math display">\[\min_{\hat{\beta_0}, \hat{\beta_1}} \sum^n_{i=1} (Y_i-\hat{\beta_0}-\hat{\beta_1}X_i)^2\]</span></p>
<p>Using calculus, we take the partial derivatives and set it equal to 0 to find a minimum. The first order conditions are:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial SSE}{\partial \hat{\beta_0}}&amp;=-2\displaystyle\sum^n_{i=1} (Y_i-\hat{\beta_0}-\hat{\beta_1} X_i)=0\\
\frac{\partial SSE}{\partial \hat{\beta_1}}&amp;=-2\displaystyle\sum^n_{i=1} (Y_i-\hat{\beta_0}-\hat{\beta_1} X_i)X_i=0\\
\end{align*}\]</span></p>
</div>
<div id="finding-hatbeta_0" class="section level3">
<h3>Finding <span class="math inline">\(\hat{\beta_0}\)</span></h3>
<p>Working with the first FOC, divide both sides by <span class="math inline">\(-2\)</span>:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} (Y_i-\hat{\beta_0}-\hat{\beta_1} X_i)=0\]</span></p>
<p>Then expand the summation across all terms and divide by <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[\underbrace{\frac{1}{n}\sum^n_{i=1} Y_i}_{\bar{Y}}-\underbrace{\frac{1}{n}\sum^n_{i=1}\hat{\beta_0}}_{\hat{\beta_0}}-\underbrace{\frac{1}{n}\sum^n_{i=1} \hat{\beta_1} X_i}_{\hat{\beta_1}\bar{X}}=0\]</span></p>
<p>Note the first term is <span class="math inline">\(\bar{Y}\)</span>, the second is <span class="math inline">\(\hat{\beta_0}\)</span>, the third is <span class="math inline">\(\hat{\beta_1}\bar{X}\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>So we can rewrite as:
<span class="math display">\[\bar{Y}-\hat{\beta_0}-\beta_1=0\]</span></p>
<p>Rearranging:</p>
<p><span class="math display">\[\hat{\beta_0}=\bar{Y}-\bar{X}\beta_1\]</span></p>
</div>
<div id="finding-hatbeta_1" class="section level3">
<h3>Finding <span class="math inline">\(\hat{\beta_1}\)</span></h3>
<p>To find <span class="math inline">\(\hat{\beta_1}\)</span>, take the second FOC and divide by <span class="math inline">\(-2\)</span>:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} (Y_i-\hat{\beta_0}-\hat{\beta_1} X_i)X_i=0\]</span></p>
<p>From the formula for <span class="math inline">\(\hat{\beta_0}\)</span>, substitute in for <span class="math inline">\(\hat{\beta_0}\)</span>:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} \bigg(Y_i-[\bar{Y}-\hat{\beta_1}\bar{X}]-\hat{\beta_1} X_i\bigg)X_i=0\]</span></p>
<p>Combining similar terms:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} \bigg([Y_i-\bar{Y}]-[X_i-\bar{X}]\hat{\beta_1}\bigg)X_i=0\]</span></p>
<p>Distribute <span class="math inline">\(X_i\)</span> and expand terms into the subtraction of two sums (and pull out <span class="math inline">\(\hat{\beta_1}\)</span> as a constant in the second sum:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} [Y_i-\bar{Y}]X_i-\hat{\beta_1}\displaystyle\sum^n_{i=1}[X_i-\bar{X}]X_i=0\]</span></p>
<p>Move the second term to the righthand side:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} [Y_i-\bar{Y}]X_i=\hat{\beta_1}\displaystyle\sum^n_{i=1}[X_i-\bar{X}]X_i\]</span></p>
<p>Divide to keep just <span class="math inline">\(\hat{\beta_1}\)</span> on the right:</p>
<p><span class="math display">\[\frac{\displaystyle\sum^n_{i=1} [Y_i-\bar{Y}]X_i}{\displaystyle\sum^n_{i=1}[X_i-\bar{X}]X_i}=\hat{\beta_1}\]</span></p>
<p>Note that from the <a href="https://metricsf19.classes.ryansafner.com/class/07-class/#the-summation-operatorproperties">rules about summation operators</a>:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} [Y_i-\bar{Y}]X_i=\displaystyle\sum^n_{i=1} (Y_i-\bar{Y})(X_i-\bar{X})\]</span></p>
<p>and:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} [X_i-\bar{X}]X_i=\displaystyle\sum^n_{i=1} (X_i-\bar{X})(X_i-\bar{X})=\displaystyle\sum^n_{i=1}(X_i-\bar{X})^2\]</span></p>
<p>Plug in these two facts:</p>
<p><span class="math display">\[\frac{\displaystyle\sum^n_{i=1} (Y_i-\bar{Y})(X_i-\bar{X})}{\displaystyle\sum^n_{i=1}(X_i-\bar{X})^2}=\hat{\beta_1}\]</span></p>
</div>
<div id="algebraic-properties-of-ols-estimators" class="section level3">
<h3>Algebraic Properties of OLS Estimators</h3>
<p>The OLS residuals <span class="math inline">\(\hat{u}\)</span> and predicted values <span class="math inline">\(\hat{Y}\)</span> are chosen by the minimization problem to satisfy:</p>
<ol style="list-style-type: decimal">
<li><p>The expected value (average) error is 0:
<span class="math display">\[E(u_i)=\frac{1}{n}\displaystyle \sum_{i=1}^n \hat{u_i}=0\]</span></p></li>
<li><p>The covariance between <span class="math inline">\(X\)</span> and the errors is 0:
<span class="math display">\[\hat{\sigma}_{X,u}=0\]</span></p></li>
</ol>
<p>Note the first two properties imply strict <strong>exogeneity</strong>. That is, this is only a valid model if <span class="math inline">\(X\)</span> and <span class="math inline">\(u\)</span> are not correlated.</p>
<ol start="3" style="list-style-type: decimal">
<li><p>The expected predicted value of <span class="math inline">\(Y\)</span> is equal to the expected value of <span class="math inline">\(Y\)</span>:
<span class="math display">\[\bar{\hat{Y}}=\frac{1}{n} \displaystyle\sum_{i=1}^n \hat{Y_i} = \bar{Y}\]</span></p></li>
<li><p>Total sum of squares is equal to the explained sum of squares plus sum of squared errors:
<span class="math display">\[\begin{align*}TSS&amp;=ESS+SSE\\
\sum_{i=1}^n (Y_i-\bar{Y})^2&amp;=\sum_{i=1}^n (\hat{Y_i}-\bar{Y})^2 + \sum_{i=1}^n {u}^2\\
\end{align*}\]</span></p></li>
</ol>
<p>Recall <span class="math inline">\(R^2\)</span> is <span class="math inline">\(\frac{ESS}{TSS}\)</span> or <span class="math inline">\(1-SSE\)</span></p>
<ol start="5" style="list-style-type: decimal">
<li>The regression line passes through the point <span class="math inline">\((\bar{X},\bar{Y})\)</span>, i.e. the mean of <span class="math inline">\(X\)</span> and the mean of <span class="math inline">\(Y\)</span>.</li>
</ol>
</div>
<div id="bias-in-hatbeta_1" class="section level3">
<h3>Bias in <span class="math inline">\(\hat{\beta_1}\)</span></h3>
<p>Begin with the formula we derived for <span class="math inline">\(\hat{\beta_1}\)</span>:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\displaystyle \sum^n_{i=1} (Y_i-\bar{Y})(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span></p>
<p>Recall from <strong>Rule 6</strong> of summations, we can rewrite the numerator as</p>
<p><span class="math display">\[\begin{align*}
    =&amp;\displaystyle \sum^n_{i=1} (Y_i-\bar{Y})(X_i-\bar{X})\\
    =&amp; \displaystyle \sum^n_{i=1} Y_i(X_i-\bar{X})\\
\end{align*}\]</span></p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\displaystyle \sum^n_{i=1} Y_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span></p>
<p>We know the true population relationship is expressed as:</p>
<p><span class="math display">\[Y_i=\beta_0+\beta_1 X_i+u_i\]</span></p>
<p>Substituting this in for <span class="math inline">\(Y_i\)</span> in equation 2:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\displaystyle \sum^n_{i=1} (\beta_0+\beta_1X_i+u_i)(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span>
Breaking apart the sums in the numerator:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\displaystyle \sum^n_{i=1} \beta_0(X_i-\bar{X})+\displaystyle \sum^n_{i=1} \beta_1X_i(X_i-\bar{X})+\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span></p>
<p>We can simplify equation 4 using <strong>Rules 4 and 5</strong> of summations</p>
<ol style="list-style-type: decimal">
<li>The first term in the numerator <span class="math inline">\(\left[\displaystyle \sum^n_{i=1} \beta_0(X_i-\bar{X})\right]\)</span> has the constant <span class="math inline">\(\beta_0\)</span>, which can be pulled out of the summation. This gives us the summation of deviations, which add up to 0 as per <strong>Rule 4</strong>:</li>
</ol>
<p><span class="math display">\[\begin{align*}
\displaystyle \sum^n_{i=1} \beta_0(X_i-\bar{X})&amp;= \beta_0 \displaystyle \sum^n_{i=1} (X_i-\bar{X})\\
&amp;=\beta_0 (0)\\
&amp;=0\\
\end{align*}\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>The second term in the numerator <span class="math inline">\(\left[\displaystyle \sum^n_{i=1} \beta_1X_i(X_i-\bar{X})\right]\)</span> has the constant <span class="math inline">\(\beta_1\)</span>, which can be pulled out of the summation. Additionally, <strong>Rule 5</strong> tells us <span class="math inline">\(\displaystyle \sum^n_{i=1} X_i(X_i-\bar{X})=\displaystyle \sum^n_{i=1}(X_i-\bar{X})^2\)</span>:</li>
</ol>
<p><span class="math display">\[\begin{align*}
\displaystyle \sum^n_{i=1} \beta_1X_1(X_i-\bar{X})&amp;= \beta_1 \displaystyle \sum^n_{i=1} X_i(X_i-\bar{X})\\
&amp;=\beta_1\displaystyle \sum^n_{i=1}(X_i-\bar{X})^2\\
\end{align*}\]</span></p>
<p>When placed back in the context of being the numerator of a fraction, we can see this term simplifies to just <span class="math inline">\(\beta_1\)</span>:</p>
<p><span class="math display">\[\begin{align*}
    \frac{\beta_1\displaystyle \sum^n_{i=1}(X_i-\bar{X})^2}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} &amp;=\frac{\beta_1}{1} \times \frac{\displaystyle \sum^n_{i=1}(X_i-\bar{X})^2}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\\
    &amp;=\beta_1   \\
\end{align*}\]</span></p>
<p>Thus, we are left with:</p>
<p><span class="math display">\[\hat{\beta_1}=\beta_1+\frac{\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span></p>
<p>Now, take the expectation of both sides:</p>
<p><span class="math display">\[E[\hat{\beta_1}]=E\left[\beta_1+\frac{\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} \right]\]</span></p>
<p>We can break this up, using properties of expectations. First, recall <span class="math inline">\(E[a+b]=E[a]+E[b]\)</span>, so we can break apart the two terms.</p>
<p><span class="math display">\[E[\hat{\beta_1}]=E[\beta_1]+E\left[\frac{\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} \right]\]</span></p>
<p>Second, the true population value of <span class="math inline">\(\beta_1\)</span> is a constant, so <span class="math inline">\(E[\beta_1]=\beta_1\)</span>.</p>
<p>Third, since we assume <span class="math inline">\(X\)</span> is also “fixed” and not random, the variance of <span class="math inline">\(X\)</span>, <span class="math inline">\(\displaystyle\sum_{i=1}^n (X_i-\bar{X})\)</span>, in the denominator, is just a constant, and can be brought outside the expectation.</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1+\frac{E\left[\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})\right]  }{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span></p>
<p>Thus, the properties of the equation are primarily driven by the expectation <span class="math inline">\(E\bigg[\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})\bigg]\)</span>. We now turn to this term.</p>
<p>Use the property of summation operators to expand the numerator term:</p>
<p><span class="math display">\[\begin{align*}
    \hat{\beta_1}&amp;=\beta_1+\frac{\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} \\
        \hat{\beta_1}&amp;=\beta_1+\frac{\displaystyle \sum^n_{i=1} (u_i-\bar{u})(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} \\
\end{align*}\]</span></p>
<p>Now divide the numerator and denominator of the second term by <span class="math inline">\(\frac{1}{n}\)</span>. Realize this gives us the covariance between <span class="math inline">\(X\)</span> and <span class="math inline">\(u\)</span> in the numerator and variance of <span class="math inline">\(X\)</span> in the denominator, based on their respective definitions.</p>
<p><span class="math display">\[\begin{align*}
    \hat{\beta_1}&amp;=\beta_1+\cfrac{\frac{1}{n}\displaystyle \sum^n_{i=1} (u_i-\bar{u})(X_i-\bar{X})}{\frac{1}{n}\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} \\
    \hat{\beta_1}&amp;=\beta_1+\cfrac{cov(X,u)}{var(X)} \\
    \hat{\beta_1}&amp;=\beta_1+\cfrac{s_{X,u}}{s_X^2} \\
\end{align*}\]</span></p>
<p>By the Zero Conditional Mean assumption of OLS, <span class="math inline">\(s_{X,u}=0\)</span>.</p>
<p>Alternatively, we can express the bias in terms of correlation instead of covariance:</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1+\cfrac{cov(X,u)}{var(X)}\]</span></p>
<p>From the definition of correlation:</p>
<p><span class="math display">\[\begin{align*}
     cor(X,u)&amp;=\frac{cov(X,u)}{s_X s_u}\\
     cor(X,u)s_Xs_u &amp;=cov(X,u)\\
\end{align*}\]</span></p>
<p>Plugging this in:</p>
<p><span class="math display">\[\begin{align*}
E[\hat{\beta_1}]&amp;=\beta_1+\frac{cov(X,u)}{var(X)} \\
E[\hat{\beta_1}]&amp;=\beta_1+\frac{\big[cor(X,u)s_xs_u\big]}{s^2_X} \\
E[\hat{\beta_1}]&amp;=\beta_1+\frac{cor(X,u)s_u}{s_X} \\
E[\hat{\beta_1}]&amp;=\beta_1+cor(X,u)\frac{s_u}{s_X} \\
\end{align*}\]</span></p>
</div>
<div id="proof-of-the-unbiasedness-of-hatbeta_1" class="section level3">
<h3>Proof of the Unbiasedness of <span class="math inline">\(\hat{\beta_1}\)</span></h3>
<p>Begin with equation:<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\sum Y_iX_i}{\sum X_i^2}\]</span></p>
<p>Substitute for <span class="math inline">\(Y_i\)</span>:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\sum (\beta_1 X_i+u_i)X_i}{\sum X_i^2}\]</span></p>
<p>Distribute <span class="math inline">\(X_i\)</span> in the numerator:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\sum \beta_1 X_i^2+u_iX_i}{\sum X_i^2}\]</span></p>
<p>Separate the sum into additive pieces:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\sum \beta_1 X_i^2}{\sum X_i^2}+\frac{u_i X_i}{\sum X_i^2}\]</span></p>
<p><span class="math inline">\(\beta_1\)</span> is constant, so we can pull it out of the first sum:</p>
<p><span class="math display">\[\hat{\beta_1}=\beta_1 \frac{\sum X_i^2}{\sum X_i^2}+\frac{u_i X_i}{\sum X_i^2}\]</span></p>
<p>Simplifying the first term, we are left with:</p>
<p><span class="math display">\[\hat{\beta_1}=\beta_1 +\frac{u_i X_i}{\sum X_i^2}\]</span></p>
<p>Now if we take expectations of both sides:</p>
<p><span class="math display">\[E[\hat{\beta_1}]=E[\beta_1] +E\left[\frac{u_i X_i}{\sum X_i^2}\right]\]</span></p>
<p><span class="math inline">\(\beta_1\)</span> is a constant, so the expectation of <span class="math inline">\(\beta_1\)</span> is itself.</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1 +E\bigg[\frac{u_i X_i}{\sum X_i^2}\bigg]\]</span></p>
<p>Using the properties of expectations, we can pull out <span class="math inline">\(\frac{1}{\sum X_i^2}\)</span> as a constant:</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1 +\frac{1}{\sum X_i^2} E\bigg[\sum u_i X_i\bigg]\]</span></p>
<p>Again using the properties of expectations, we can put the expectation inside the summation operator (the expectation of a sum is the sum of expectations):</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1 +\frac{1}{\sum X_i^2}\sum E[u_i X_i]\]</span></p>
<p>Under the exogeneity condition, the correlation between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(u_i\)</span> is 0.</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1\]</span></p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Note this is a <code>.dta</code> Stata file. You will need to (install and) load the package <code>haven</code> to <code>read_dta()</code> Stata files into a dataframe.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>From the <a href="https://metricsf19.classes.ryansafner.com/class/07-class/#the-summation-operator">rules about summation operators</a>, we define the mean of a random variable <span class="math inline">\(X\)</span> as <span class="math inline">\(\bar{X}=\frac{1}{n}\displaystyle\sum_{i=1}^n X_i\)</span>. The mean of a constant, like <span class="math inline">\(\beta_0\)</span> or <span class="math inline">\(\beta_1\)</span> is itself.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Admittedly, this is a simplified version where <span class="math inline">\(\hat{\beta_0}=0\)</span>, but there is no loss of generality in the results.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

          </div>

          



          
          <div class="article-widget">
            
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/content/2.3-content/" rel="next">2.3 — OLS Linear Regression — Class Content</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/content/2.5-content/" rel="prev">2.5 — OLS: Precision and Diagnostics — Class Content</a>
  </div>
  
</div>

          </div>
          
        </div>

        <div class="body-footer">
          Last updated on Jul 12, 2022
        </div>

      </article>

      <footer class="site-footer">
  <div class="container">
    <div class="row">
      <div class="col-md-6">
        
        <p>
          Content <i class="fab fa-creative-commons"></i> 2022-2023 <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a></a> <a href="https://ryansafner.com">Ryan Safner</a> &middot; <a href="Credits">Credits</a> &middot; Powered by <a href="https://bookdown.org/yihui/blogdown/">blogdown</a> and <a href="https://gohugo.io/">Hugo</a>
        </p>
      </div>
      <div class="col-md-6">
        <ul class="list-inline network-icon text-right">
          
          
        </ul>
      </div>
    </div>
  </div>
</footer>

    </main>
  </div>
</div>


      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js" integrity="sha256-pB/deHc9CGfFpJRjC43imB29Rse8tak+5eXqntO94ck=" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academia.min.7276a6a3624de715a5c7f54c7c07696d.js"></script>

    

<script src="/js/math-code.js"></script>


  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
